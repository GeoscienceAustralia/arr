<?xml version="1.0" encoding="UTF-8"?>
<chapter version="5.0" xmlns="http://docbook.org/ns/docbook"
         xmlns:xlink="http://www.w3.org/1999/xlink"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:svg="http://www.w3.org/2000/svg"
         xmlns:m="http://www.w3.org/1998/Math/MathML"
         xmlns:html="http://www.w3.org/1999/xhtml"
         xmlns:db="http://docbook.org/ns/docbook">
  <title>AT-SITE FLOOD FREQUENCY ANALYSIS</title>

  <section>
    <title>Introduction</title>

    <para>George Kuczera and Stewart Franks</para>

    <para>School of Engineering, University of Newcastle</para>

    <para>Flood frequency analysis refers to procedures that use recorded and
    related flood data to select and fit a probability model of flood peaks at
    a particular location in the catchment. The flood probability model can
    then be used to perform risk-based design and flood risk assessment and to
    provide input to regional flood estimation methods.</para>

    <para>This chapter considers the issues and techniques useful when
    undertaking a flood frequency analysis using flood data at a single site.
    It represents an evolutionary update of Chapter 10 in the
    3<superscript>rd</superscript> Edition of Australian Rainfall and Runoff
    (Pilgrim and Doran, 1987). Nonetheless, where appropriate the original
    contribution by Pilgrim and Doran has been retained. The major changes
    include introduction of non-homogeneous probability models to account for
    long-term climate variability, abandonment of product log-moments with the
    log-Pearson III distribution, use of Bayesian methods to make better use
    of available flood information (such as censored flow data, rating error
    and regional information) and reduced prescription about the choice of
    flood probability model.</para>

    <para>The primary purpose of this chapter is to present guidelines on
    performing flood frequency analyses. Often judgment will need to be
    exercised. To inform such judgments, the key conceptual foundations that
    underpin flood frequency analysis are described; a reader, however, will
    need an understanding of elementary probability theory and statistics to
    get maximum benefit from this description. In addition a number of worked
    examples are provided to provide deeper insight with the implied caveat
    that the examples are not exhaustive in their scope. It is expected that
    users of at site flood frequency analysis will encounter problems
    different to those explored in the examples. While it is expected that
    most users will use software written by others to implement the methods
    described in this chapter, sufficient information is provided to enable
    users to develop their own software for their applications.</para>

    <section/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <para>
          <emphasis role="bold"/>

          <phrase id="WPGeneratedID_TOC_1_1">CONCEPTUAL FRAMEWORK</phrase>
        </para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" inheritnum="inherit"
                 numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" inheritnum="inherit"
                     numeration="loweralpha">
          <listitem>
            <para><emphasis role="bold"/>Definition of Flood Probability
            Model</para>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" inheritnum="inherit"
                 numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" inheritnum="inherit"
                     numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" inheritnum="inherit"
                         numeration="lowerroman">
              <listitem>
                <para><emphasis role="bold"/>General Definition</para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>In flood frequency analysis flood peaks are considered to be random
    variables. Following convention with flood frequency analyses, the random
    variable denoting the flood peak is denoted by an upper-case symbol (e.g.
    Q) whereas a specific realization (or sample) is denoted by the lower-case
    symbol (e.g. q). Where there is no ambiguity lower-case symbols will be
    used.</para>

    <para/>

    <para>It is assumed that each realization q is statistically independent
    of other realizations. This is the standard assumption applied in flood
    frequency analysis and is believed to be widely applicable (e.g. Stedinger
    <emphasis role="italic">et al</emphasis>., 1993). It is for this reason
    that a discussion of data requirements is presented in Section <xref
    linkend="WPGeneratedID_Xref_ValidDataSection_1"/> of this Chapter in Book
    1 of Australian Rainfall and Runoff.</para>

    <para/>

    <para>In its most general form, the flood probability model can be
    described by its probability density function (pdf) p(q | θ, x, M) where q
    is the flood peak. The pdf of q is determined by the vector of parameters
    θ belonging to the probability distribution family M and by x defined as
    the vector of exogenous or external variables which affect the values of
    θ. The notation “|” refers to conditioning: the variables to the left of
    “|” depend on the values taken by variables to the right of “|”.</para>

    <para/>

    <para>The distribution function of Q is defined as the non-exceedance
    probability P(Q≤q) and is related to the pdf by</para>

    <para>
      <anchor id="WPGeneratedID_Xref_pdfeqn_1"/>
    </para>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.1)FUNC { P `( ` Q ` &lt;= ` q ` LINE ` theta ` , ` x ` , ` M
      ~ = INT FROM 0 TO q ~ p ` ( ` z ` LINE ` theta ` , ` x ` , ` M ` ) ` dz
      }</para>
    </section>

    <para>Empirically the pdf of q is the limiting form of the histogram of q
    as the number of samples approaches infinity. Importantly, as shown in
    equation 1.1.<link linkend="WPGeneratedID_TOC_1_2">1</link>, the area
    under the pdf is interpreted as probability.</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" numeration="lowerroman">
              <listitem>
                <para>
                  <emphasis role="bold"/>

                  <phrase id="WPGeneratedID_TOC_1_2">Homogeneous flood
                  probability model</phrase>
                </para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>The simplest form of the flood probability model arises when the
    parameter vector θ does not depend on an exogenous vector x. In that case
    each flood peak is considered to be a random realization from the same
    probability model p(q | θ, M). Under this assumption flood peaks form a
    homogeneous time series.</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" numeration="lowerroman">
              <listitem>
                <para><emphasis role="bold"/>Non-homogeneous flood probability
                model</para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>A more complicated situation arises when flood peaks do not form a
    homogeneous time series. This may arise for a number of reasons including
    the following:</para>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>Rainfall and flood mechanisms may be changing over time. For
        example, long-term climate change due to global warming, land use
        change and river regulation may render the flood record
        non-homogeneous; and</para>
      </listitem>
    </itemizedlist>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>Climate may experience pseudo-periodic shifts that persist over
        periods lasting from several years to several decades. There is
        growing evidence that parts of Australia are subject to such forcing
        and that this significantly affects flood risk (see, for example,
        Warner and Erskine, 1988; Harvey et. al, 1991; Franks and Kuczera,
        2002; Kiem <emphasis role="italic">et al</emphasis>., 2003; Micevski
        <emphasis role="italic">et al</emphasis>., 2003).</para>
      </listitem>
    </itemizedlist>

    <para/>

    <para>Although this chapter in Book 1 of Australian Rainfall and Runoff
    will provide some guidance on non-homogeneous flood probability models it
    needs to be stressed that this is an area of continuing research and,
    therefore, users are therefore advised to keep abreast of new
    developments.</para>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="loweralpha">
      <listitem>
        <para><emphasis role="bold"/>Flood Risk Perspectives</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" inheritnum="inherit"
                 numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" inheritnum="inherit"
                     numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" inheritnum="inherit"
                         numeration="lowerroman">
              <listitem>
                <para><emphasis role="bold"/>General</para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>Flood frequency analysis deals with the probability distribution of
    significant flood peaks. Throughout the year, there are typically many
    flood peaks associated with individual storm events. This is illustrated
    in Figure 1.1.<link linkend="WPGeneratedID_TOC_1_3">2</link> where a time
    series plot of a streamflow discharge is presented.</para>

    <para/>

    <para>There are two ways of describing the probability of exceeding a
    significant flood magnitude:</para>

    <para/>

    <orderedlist continuation="restarts" numeration="arabic">
      <listitem>
        <para>The probability distribution of the largest flood peak occurring
        over a particular interval of time, which, in practice, is one year;
        and</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <para>The probability distribution of the time between consecutive
        flood peaks that exceed a particular magnitude.</para>
      </listitem>
    </orderedlist>

    <para>
      <graphic align="center" depth="2.994in" fileref="ARRFFA/V5FIG4~1.JPG"
               format="JPG" valign="top" width="4.723in"/>
    </para>

    <para>Figure 1.1.1 - Peak-over-threshold series<anchor
    id="WPGeneratedID_Xref_potseriesfigure_1"/></para>

    <para/>

    <para>These two perspectives are intimately connected as the following
    exposition will show. Central to this connection is the annual maximum
    series, obtained by extracting the largest flood peak in every year of the
    record, and the peak-over-threshold (POT) series, obtained by extracting
    independent flood peaks above some threshold discharge.</para>

    <para/>

    <para>Referring to Figure 1.1.<link
    linkend="WPGeneratedID_TOC_1_3">2</link>, let the random variable q be a
    local peak discharge defined as a discharge which has lower discharge on
    either side of the peak. This presents an immediate problem as any bump on
    the hydrograph would produce a local peak. To circumvent this problem we
    focus on peak flows greater than some threshold discharge defined as
    q<subscript>o</subscript>. The threshold is selected so that the peaks
    above the threshold are sufficiently separated in time to be statistically
    independent of each other.</para>

    <para/>

    <para>Suppose over a time interval of length T there are n peaks over the
    threshold q<subscript>o</subscript>. This defines the POT time series of n
    independent realizations
    {q<subscript>1</subscript>,…,q<subscript>n</subscript>}.</para>

    <para>Let w be the maximum value in the POT time series; that is,</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.2)FUNC { w ~ = ~ max \{ ` q SUB ` , ` DOTSLOW ` , ` q SUB n `
      \} }</para>
    </section>

    <para>For w to be the maximum value each observed peak must be less than
    or equal to w. In probability theory, this condition is expressed by the
    joint event consisting of the intersection of the following n events,
    ie</para>

    <section label="inline" role="textbox">
      <para>FUNC { \{ ` ( ` q SUB 1 ~ &lt;= ~ w ` ) ~ CAP ~ ( ` q SUB 2 ~
      &lt;= ~ w ` ) ~ CAP ~ DOTSLOW ~ CAP ~ ( ` q SUB n ~ &lt;= ~ w ` ) ` \}
      }</para>
    </section>

    <para>.</para>

    <para/>

    <para>Because the peaks are assumed to be statistically independent the
    probability of the joint event is the product of the probabilities of the
    individual events. Therefore the probability that the random variable W ≤
    w in a POT series with n events occurring over the interval T simplifies
    to</para>

    <para>
      <anchor id="WPGeneratedID_Xref_GKequation3_1"/>
    </para>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.3)FUNC { MATRIX { P ` ( ` W ~ &lt;= ~ w ` LINE ` n ` , ` T `
      ) &amp; ALIGNL = ~ P ` [ ` ( ` q SUB 1 ~ &lt;= ~ w ` ) ~ CAP ~ ( ` q SUB
      2 ~ &lt;= ~ w ` ) ~ CAP ~ DOTSLOW ~ CAP ~ ( ` q SUB n ~ &lt;= ~ w ` ) `
      ] # ~ &amp; ALIGNL = ~ P `( ` q SUB 1 ~ &lt;= ~ w ` ) ` P ` ( ` q SUB 2
      ~ &lt;= ~ w ` ) ~ DOTSLOW ~ P `( ` q SUB n ~ &lt;= ~ w ` ) #~ &amp;
      ALIGNL = ~ P ` ( ` q ~ &lt;= ~ w ` ) SUP n } }</para>
    </section>

    <para>The last term in equation 1.1.<link
    linkend="WPGeneratedID_TOC_1_4">3</link> comes from the assumption that
    all the peaks above the threshold q<subscript>o</subscript> are sampled
    from the same distribution with the pdf p(q | q &gt;
    q<subscript>o</subscript>).</para>

    <para/>

    <para>The number of POT events n occurring over an interval T is random.
    Suppose that the random variable n follows a Poisson distribution with ν
    being the average number of POT events per unit time; that is,</para>

    <para>
      <anchor id="WPGeneratedID_Xref_POToccurrenceeqn_1"/>
    </para>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.4)FUNC { P ` ( ` n ` LINE ` nu ` ) ~ = ~ { ( ` nu ` T ` ) SUP
      n ~ exp ` ( ` - ` nu ` T ` ) } OVER { n ` ! } ~ , ~ n ~ = ~ 0 ` , ` 1 `
      , ` 2 ` , ` DOTSLOW ~ }</para>
    </section>

    <para>Equation 1.1.<link linkend="WPGeneratedID_TOC_1_5">4</link> can be
    applied to cases where the distribution of POT occurrences varies
    according to the seasons encountered within time interval T. Provided the
    distribution of POT occurrences in each season is Poisson, the
    distribution of POT occurrences over interval T is Poisson with the
    parameter νT equal to the sum of the average number of occurrences in each
    season. This result is a consequence of the fact that the sum of Poisson
    random variables is Poisson distributed.</para>

    <para/>

    <para>After some algebra, application of the total probability theorem
    yields the distribution of the largest flood peak magnitude over the
    interval with duration T</para>

    <para>
      <anchor id="WPGeneratedID_Xref_excprobeqn5_1"/>
    </para>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.5)FUNC { MATRIX { P ` ( ` W ~ &lt;= ~ w ` LINE ` T ` ) ~
      &amp; ALIGNL = ~ SUM FROM { n ` = ` 0 } TO INF ` P ` ( ` W ~ &lt;= ~ w `
      LINE ` n ` , ` T ` ) ` P ` ( ` n ` LINE ` nu ` ) # ~ &amp; ALIGNL = ~
      exp ` [ ` - ` ( ` nu ` T ` ) ` P ` ( ` Q ~ &gt; ~ w ` ) ` ] } }</para>
    </section>

    <para>where P(W ≤ w | T) is the probability that the largest flood peak
    over time interval T is less than or equal to w. This result hinges on the
    assumption that all the peaks above the threshold
    q<subscript>o</subscript> are sampled from the same distribution. If the
    pdf p(q | q &gt; q<subscript>o</subscript>) exhibits significant seasonal
    differences equation 1.1.<link linkend="WPGeneratedID_TOC_1_4">3</link>
    cannot be simplified necessitating a more involved analysis.</para>

    <para/>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="lowerroman">
      <listitem>
        <para>
          <emphasis role="bold"/>

          <phrase id="WPGeneratedID_TOC_1_6">Distribution of Time Between
          Floods</phrase>
        </para>
      </listitem>
    </orderedlist>

    <para/>

    <para>The objective is to derive the probability distribution of the time
    between consecutive flood peaks with magnitude in excess of w. With regard
    to equation 1.1.<link linkend="WPGeneratedID_TOC_1_6">5</link>, if the
    largest flood peak during time T is less than or equal to w, then the time
    to the next peak with magnitude in excess of w must be greater than T. It
    therefore follows from equation 1.1.<link
    linkend="WPGeneratedID_TOC_1_6">5</link> that the distribution of time
    between flood peaks with magnitude exceeding w is</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.6)FUNC { P ` ( ` Time ~ \to ~ next ~ peak ~ exceeding ~ w ~
      &lt;= ~ T ` ) ~ = } # ~ # FUNC { ~ 1 ~ - ~ exp ` [ ` - ` nu ` P ` ( ` Q
      ~ &gt; ~ w ` ) ` T ` ] }</para>
    </section>

    <para>This is recognized as an exponential distribution with parameter
    νP(Q&gt;w) which is interpreted as the expected number of peaks exceeding
    w per unit time. An important property of the exponential distribution is
    that its expected value is equal to the inverse of its parameter. It
    therefore follows that the expected time interval between peaks that
    exceed w is</para>

    <para>
      <anchor id="WPGeneratedID_Xref_excprobeqn7_1"/>
    </para>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.7)FUNC { STACKALIGN { T SUB P ` ( ` w ` ) &amp; = ~ 1 OVER {
      Expected ~ number ~ of ~ peaks ~ &gt; ~ w ~ per ~ unit ~ time } # ~
      &amp; = ~ 1 OVER { nu ` P ` ( ` Q ~ &gt; ~ w ` ) } } }</para>
    </section>

    <para>T<subscript>P</subscript>(w) is termed the average recurrence
    interval (ARI) for magnitude w and provides a convenient probability
    measure for POT series.</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" numeration="lowerroman">
              <listitem>
                <para>
                  <emphasis role="bold"/>

                  <phrase id="WPGeneratedID_TOC_1_7">Annual Maximum Flood
                  Risk</phrase>
                </para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>With regard to equation 1.1.<link
    linkend="WPGeneratedID_TOC_1_6">5</link>, P (W ≤ w | T = 1) represents the
    probability that the largest flood peak during the year is less than or
    equal to w. The annual exceedance probability AEP(w) is given by 1 - P (W
    ≤ w | T = 1) and provides a convenient probability measure for the annual
    maximum series. It is understood that the 1 in Y AEP flood has a
    probability of 1/Y of being equalled or exceeded in any one year.</para>

    <para/>

    <para>An alternative measure is based on the average recurrence interval
    of annual maximum flood peaks exceeding magnitude w. The probability of
    waiting n years for the next annual maximum flood to exceed w can be shown
    to be</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.8)FUNC { P `( ` Annual ~ maximum ~ peak ~ exceeds ~ w ~ \in ~
      n SUP { th } ~ year ` ) ~ = } #</para>

      <para>~ #</para>

      <para>FUNC { P ` ( ` W ~ &lt;= ~ w ` | ` T ~ = ~ 1 ` ) SUP { n ` - ` 1 }
      ~ P ` ( ` W ~ &gt; ~ w ` | ` T ~ = ~ 1 ` ) }</para>
    </section>

    <para>This is known as the geometric distribution. The expected number of
    years between annual maximum flood peaks with magnitude in excess of w can
    be shown to be</para>

    <para>
      <anchor id="WPGeneratedID_Xref_excprobeqn9_1"/>
    </para>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.9)FUNC { T SUB A ` ( ` w ` ) ~ = ~ 1 OVER { 1 ~ - ~ P ` ( ` W
      ~ &lt;= ~ w ` LINE ` T ~ = ~ 1 ` ) } ~ = ~ 1 OVER { AEP ` ( ` w ` ) }
      }</para>
    </section>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" numeration="lowerroman">
              <listitem>
                <para>
                  <emphasis role="bold"/>

                  <phrase id="WPGeneratedID_TOC_1_8">Choice of Series</phrase>
                </para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>It is important to distinguish between T<subscript>P</subscript> and
    T<subscript>A</subscript> as they have different meanings even though they
    are referred to as Average Recurrence Intervals. From equations 1.1.<link
    linkend="WPGeneratedID_TOC_1_6">5</link>, 1.1.<link
    linkend="WPGeneratedID_TOC_1_7">7</link>,and 1.1.<link
    linkend="WPGeneratedID_TOC_1_8">9</link>, it follows that the relationship
    between T<subscript>P</subscript> (w) and T<subscript>A</subscript> (w)
    is</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.10)FUNC { T SUB A ` ( ` w ` ) ~ = ~ 1 OVER { 1 ~ - ~ exp `
      LEFT ( ` - ` 1 OVER { T SUB P ` ( ` w ` ) } ` RIGHT ) } }</para>
    </section>

    <para>
      <anchor id="WPGeneratedID_Xref_TATPRelationship_1"/>

      <graphic align="center" depth="4.698in" fileref="ARRFFA/V5FIG4~2.JPG"
               format="JPG" valign="top" width="4.723in"/>
    </para>

    <para>Figure 1.1.2 - Relationship between POT and Annual Maximum Average
    Recurrence Intervals</para>

    <para>This relationship is in Figure 1.1.<link
    linkend="WPGeneratedID_TOC_1_9">2</link>. For an ARI of 10 or more years
    the difference between T<subscript>A</subscript> and
    T<subscript>P</subscript> is minimal and POT and annual maximum analyses
    should yield similar results. For ARIs less than 10 years, however, the
    ARI for annual maximum series will exceed the ARI for the POT series for
    the same discharge. This conclusion arises from the smaller floods during
    years with large annual maximum floods may exceed the annual maximum flood
    in other years.</para>

    <para/>

    <para>These considerations make it essential when quoting ARIs to make
    clear whether they refer to annual maximum or POT series. The annual
    maximum and POT ARIs refer to different properties of the flood time
    series. This motivates the convention used in Australian Rainfall and
    Runoff, namely AEPs are used when referring to annual maximum series,
    while ARIs are used when referring to POT series.</para>

    <para/>

    <para>Consideration of Figure 1.1.<link
    linkend="WPGeneratedID_TOC_1_9">2</link> leads to the following
    guidelines:</para>

    <para>i. T<subscript>A</subscript> &gt; 10 yearsUse of annual maximum
    series is generally preferred because it is straightforward to ensure
    statistical independence between annual maxima and allows the analysis to
    better focus on low AEP events. This series is generally used in design,
    as low AEPs in this range are generally required for estimation of a
    design flood for a structure or works at a particular site.</para>

    <para/>

    <para>ii. T<subscript>A</subscript> &lt; 10 yearsUse of POT series is
    generally preferred because all floods are of interest in this range,
    whether they are the highest in the particular year of record or not. The
    annual maximum series may omit many floods of interest. The POT series is
    appropriate for estimating design flows of low ARI in urban stormwater
    contexts including water quality treatment devices and measures and for
    diversion works, coffer dams and other temporary structures. However, it
    is infrequently used because flow records are not often available at sites
    where minor works with a design ARI of less than 10 years are required. A
    significant application is in the development of regional flood estimation
    methods for small to medium sized catchments (e.g. Pilgrim and McDermott,
    1982; Flavell, 1983; Adams and McMahon, 1985; and Adams, 1987). It should
    also be noted that the design rainfall data in Book II effectively
    represent the results of frequency analysis of POT series rainfall
    data.</para>

    <para/>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="loweralpha">
      <listitem>
        <para>
          <emphasis role="bold"/>

          <phrase id="WPGeneratedID_TOC_1_9">Advantages and Disadvantages of
          Flood Frequency Analysis</phrase>
        </para>
      </listitem>
    </orderedlist>

    <para/>

    <para>Analysts need to be aware of the advantages and disadvantages of
    flood frequency analysis.</para>

    <para/>

    <para>Flood peaks are the product of a complex joint probability process
    involving the interaction of many random variables associated with the
    rainfall event, antecedent conditions and rainfall-runoff transformation.
    Peak flood records represent the integrated response of the storm event
    with the catchment. They provide a direct measure of flood exceedance
    probabilities. As a result flood frequency analysis is less susceptible to
    bias, possibly large, that can affect alternative methods based on design
    rainfall (Kuczera <emphasis role="italic">et al</emphasis>., 2003).</para>

    <para/>

    <para>Other advantages of flood frequency analysis include its comparative
    simplicity and capacity to quantify uncertainty arising from limited
    information. It remains a moot point whether flood frequency methods are
    less accurate than rainfall-based methods for which rigorous uncertainty
    analysis is yet to be developed.</para>

    <para/>

    <para>Offsetting these significant advantages are several
    disadvantages:</para>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>The true probability distribution family M is unknown.
        Unfortunately, different models can fit the bulk of the flood data
        with similar capability, yet can diverge in the right hand tail when
        extrapolated beyond the data and have difficulty dealing with the left
        hand tail.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>Short records may produce flood estimates with considerable
        uncertainty. However, the ready ability to compute confidence limits
        directly informs the user about the credibility of the
        estimate.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>It may be difficult or impossible to adjust the data if the
        catchment conditions under which the flood data were obtained have
        changed during the period of record, or are different to those
        applying to the future design life of a structure or works being
        designed.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>Considerable extrapolation of rating curves is necessary to
        convert recorded stage to discharge for the largest flood peaks at
        most Australian gauging stations. Also the probability of malfunction
        of recording instruments is increased during major floods. Suspect
        floods and the years in which they occurred may be omitted in analysis
        of annual maximum series, but this reduces the sample size and may
        introduce bias if the suspect floods are all major events. Though
        these problems are inherent to the calibration of all methods
        employing major flood peaks, flood frequency analysis is more
        sensitive because of its need to use major flood peaks.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="loweralpha">
      <listitem>
        <para><emphasis role="bold"/>Range of Application</para>
      </listitem>
    </orderedlist>

    <para/>

    <para>As noted, the true flood probability family is unknown. In practice
    the choice of model is guided by goodness of fit to data, understanding of
    the hydrologic characteristics of the region, and regional hydrologic
    experience. Therefore, use of the fitted frequency curve for ARIs up to
    that of the data is regarded as an interpolation exercise deemed to be
    reliable in the sense that confidence limits capture the uncertainty.
    However, when the frequency curve is extrapolated well beyond the observed
    data, confidence limits which quantify the effect of sampling variability
    on parameter uncertainty may underestimate the true uncertainty. In these
    circumstances, model bias may be significant and even dominant.
    Demonstrated in Example <link linkend="WPGeneratedID_TOC_1_35">1</link> is
    the need to understand the processes affecting flood peaks beyond the
    observed record and illustrates the pitfall of blind extrapolation.</para>

    <para/>

    <para>Large extrapolations of flood frequency analyses are not
    recommended. It is acknowledged that prescribing strict limits on maximum
    ARIs or minimum AEPs does not have a strong conceptual foundation. The
    limits to extrapolation should be guided by consideration of confidence
    limits, which are affected by the information content of the data and
    choice of flood model, and by judgments about model bias which cannot be
    quantified. In situations where the analyst is prepared to make the
    judgment that the processes operating in the range of the observed record
    continue to dominate for larger floods, model bias may be deemed to be
    manageable - of course the effects of sampling uncertainty may be so
    amplified for significant extrapolation to render the frequency estimate
    of little value.</para>

    <para/>

    <para>In the absence of user analysis about the degree of model bias when
    extrapolating, the following guidelines are offered to promote consistency
    with previous practice: As discussed further in Book VI, the 1 in 100 AEP
    flood is the largest event that should be estimated by direct frequency
    analysis for important work. The maximum flood that should be estimated by
    this means under any circumstances is the 1 in 500 AEP event. Where a
    regional flood frequency method transfer of data from an adjacent
    catchment is used, the limiting AEP should be 1 in 100. Described in Book
    VI are procedures for estimating floods beyond the probabilities noted
    above. These procedures interpolate flood magnitudes between the 1 in 100
    AEP event and the probable maximum flood. While these procedures involve
    some arbitrary assumptions, they provide a consistent approach and
    overcome many of the problems involved in extrapolation of flood frequency
    analysis resulting from choice of the appropriate probability
    distribution.</para>

    <para/>

    <orderedlist continuation="continues" inheritnum="inherit"
                 numeration="arabic">
      <listitem>
        <para><emphasis role="bold"/>SELECTION AND PREPARATION OF DATA</para>
      </listitem>
    </orderedlist>

    <para/>

    <para>
      <anchor id="WPGeneratedID_Xref_ValidDataSection_1"/>
    </para>

    <orderedlist continuation="continues" inheritnum="inherit"
                 numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" inheritnum="inherit"
                     numeration="loweralpha">
          <listitem>
            <para>
              <emphasis role="bold"/>

              <phrase id="WPGeneratedID_TOC_1_10">Requirements of Data for
              Valid Analysis</phrase>
            </para>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>For valid frequency analysis, the data used should constitute a
    random sample of independent values, ideally from a homogeneous
    population. Streamflow data are collected as a continuous record, and
    discrete values must be extracted from this record as the events to be
    analysed. The problem of assessing independence of events, and of
    selecting all independent events, is illustrated by the streamflow record
    for a 1000 km<superscript>2</superscript> catchment shown in Figure
    1.1.<link linkend="WPGeneratedID_TOC_1_11">3</link>. There is little doubt
    that peaks A and B are not independent or that they are serially
    correlated, while peak D is independent of A and B. However, the
    independence of peak C from A and B is open to question, and there is
    doubt as to whether the independent peaks in the record are B and D, or B,
    C and D. Methods for selecting peak flows to be included in the analysis
    are described in the following subsections.</para>

    <para>
      <anchor id="WPGeneratedID_Xref_IndepPeaksFig_1"/>

      <graphic align="center" depth="3.769in"
               fileref="ARRFFA/WPGeneratedName1.JPG" format="JPG" valign="top"
               width="4.723in"/>
    </para>

    <para>Figure 1.1.3 - Hydrograph for to 1000 km2 catchment illustrating
    difficulty of assessing independence of floods</para>

    <para>Lack of homogeneity of the population of floods is also a practical
    problem, particularly as the data sample from the past is used to derive
    flood estimates applicable to the design life of the structure or works in
    the future. Examples of changes in the collection of the data or in the
    nature of the catchment that lead to lack of homogeneity are:</para>

    <para/>

    <orderedlist continuation="restarts" numeration="arabic">
      <listitem>
        <para>undetected change in station rating curve;</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <para>change of gauging station site;</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <para>construction of large storages, levees and channel improvements;
        and</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <para>changes in land use such as clearing, growth in the number of
        farm dams on the catchment, different farming practices, soil
        conservation works, reafforestation, and urbanization.</para>
      </listitem>
    </orderedlist>

    <para/>

    <para>The record should be carefully examined for these and other causes
    of lack of homogeneity. In some cases recorded values can be adjusted by
    means such as routing pre-dam floods through the storage to adjust them to
    equivalent present values, correcting rating errors where this is
    possible, or making some adjustment for urbanization. Such decisions must
    be made largely by judgment. As with all methods of flood estimation, it
    is important that likely conditions during the design life be considered
    rather than those existing at the time of design. Some arbitrary
    adjustment of derived values for likely changes in the catchment may be
    possible, but the recorded data must generally be accepted for analysis
    and design. Fortunately, the available evidence indicates that unless
    changes to the catchment involve large proportions of the total area or
    large changes in the storage on the catchment, the effects on flood
    magnitudes are likely to be modest. Also, the effects are likely to be
    larger for small floods than for the large floods.</para>

    <para/>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="loweralpha">
      <listitem>
        <para>
          <emphasis role="bold"/>

          <phrase id="WPGeneratedID_TOC_1_11">Types of Flood Data</phrase>
        </para>
      </listitem>
    </orderedlist>

    <para/>

    <para>It is convenient to classify the flood peak data used in a flood
    frequency analysis as either being gauged or censored.</para>

    <para/>

    <orderedlist continuation="continues" inheritnum="inherit"
                 numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" inheritnum="inherit"
                     numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" inheritnum="inherit"
                         numeration="lowerroman">
              <listitem>
                <para><emphasis role="bold"/>Gauged Data</para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>Gauged data consist of a time series of flood discharge estimates.
    Such estimates are based on observed peak (or instantaneous) stages (or
    water levels). A rating curve is used to transform stage observations to
    discharge estimates. When extrapolated, the rating curve can introduce
    large systematic error into discharge estimates.</para>

    <para/>

    <para>It is important to check how the peak discharges were obtained from
    the gauged record. Peak discharges may be derived from daily readings,
    possibly with some intermediate readings during some floods for part of
    the record, and continuous readings from the remainder of the record. If
    the daily reading is deemed an unreliable estimate of the peak discharge
    during that day, the reading need not be discarded but treated as an
    imprecise measurement. Micevski <emphasis role="italic">et al</emphasis>.
    (2005) present a likelihood-based method for dealing with such cases. The
    consequences of ignoring the error associated with daily readings is
    considered also.</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" numeration="lowerroman">
              <listitem>
                <para><emphasis role="bold"/>Censored Data</para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>Censored data refer to peak flows which are treated as being above
    or below a known threshold. They can be expressed as a time series of
    indicator values defined as</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>FUNC { I SUB t ` ( ` q ` ) ~ = ~ LEFT LBRACE ~ STACK { ALIGNL 1 ~
      if ~ the ~ t SUP { th } ~ flood ~ peak ~ &gt; ~ threshold ~ discharge ~
      q # ALIGNL - ` 1 ~ if ~ the ~ t SUP { th } ~ flood ~ peak ~ &lt;= ~
      threshold ~ discharge ~ q } RIGHT . }</para>
    </section>

    <para/>

    <para>Censored data can arise in a number of ways. For example, prior to
    gauging, water level records may have been kept only for large floods
    above some perception threshold. Therefore, all we may know is that there
    were n<subscript>a</subscript> flood peaks above the threshold and
    n<subscript>b</subscript> peaks below the threshold.</para>

    <para/>

    <para>Sometimes, we may deliberately exclude zero or small gauged floods
    below some threshold because the overall fit is unduly influenced by small
    floods. In such cases, even though the gauged flows are available, they
    are treated as censored data.</para>

    <para/>

    <orderedlist continuation="continues" inheritnum="inherit"
                 numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" inheritnum="inherit"
                     numeration="loweralpha">
          <listitem>
            <para><emphasis role="bold"/>Annual Flood Gauged Series</para>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>This is the most common method of selecting the flood peaks to be
    analysed. The series comprises the highest instantaneous rate of discharge
    in each year of record. The year may be a calendar year or a water year,
    the latter usually commencing at the end of the period of lowest average
    flow during the year. Where flows are highly seasonal, especially with a
    wet summer, use of the water year is preferable. The highest flow in each
    year is selected whether it is a major flood or not, and all other floods
    are neglected, even though some will be much larger than the maximum
    discharges selected from some other years. For N years of data, the annual
    flood series will consist of N values.</para>

    <para/>

    <para>The annual flood series has at least two advantages:</para>

    <para/>

    <orderedlist continuation="restarts" numeration="arabic">
      <listitem>
        <para>As the individual annual maximum flows are likely to be
        separated by considerable intervals of time, it is probable that the
        values will be independent. Checking of dates of the annual maxima to
        ensure that they are likely to be independent is a simple procedure
        that should always be carried out. If the highest annual value
        occurred at the start of a year and was judged to be strongly related
        to the annual maximum at the end of the previous year, the lower of
        these two values should be discarded, and the second highest flow in
        that year substituted; and</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <para>The series is easily and unambiguously extracted. Most data
        collection agencies (see Book I, Chapter 2 for a list of major data
        collection agencies) have annual maxima on computer file and/or hard
        copy.</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="loweralpha">
      <listitem>
        <para><emphasis role="bold"/>Peak-Over-Threshold Gauged Series</para>
      </listitem>
    </orderedlist>

    <para/>

    <para>A POT flood series consists of all floods with peak discharges above
    a selected base value, regardless of the number of such floods occurring
    each year. The POT series may also be termed the partial duration series
    or basic stage series. The number of floods (K) generally will be
    different to the number of years of record (N), and will depend on the
    selected threshold discharge. The American Society of Civil Engineers
    (1949) recommended that the base discharge should be selected so that K is
    greater than N, but that there should not be more than 3 or 4 floods above
    the threshold in any one year. These two requirements can be incompatible.
    The US Geological Survey (Dalrymple, 1960) recommended that K should equal
    3N. If a probability distribution is to be fitted to the POT series the
    desirable threshold discharge and average number of floods per year
    selected depend on the type of distribution. These distributions are
    discussed further in Section <xref
    linkend="WPGeneratedID_Xref_ChoiceofDistSection_1"/> of this Chapter in
    Australian Rainfall and Runoff. For the compound model using a Poisson
    distribution of occurrences and an exponential distribution of magnitudes,
    Tavares and da Silva (1983) and Jayasuriya and Mein (1985) found that K
    should equal 2N or greater, and the UK Flood Studies Report (Natural
    Environment Research Council, 1975) recommended that K should equal 3N to
    5N. For fitting the log Pearson III distribution, the values of the
    moments depend on the number of floods selected and the base discharge.
    McDermott and Pilgrim (1982) and Jayasuriya and Mein (1985) found that
    best results were obtained in this case when K equalled N. Martins and
    Stedinger (2001) found that the precision of flood quantiles derived from
    a GEV-Poisson POT model is fairly insensitive for K ≥ N.</para>

    <para/>

    <para>An important advantage of the POT series is that when the selected
    base value is sufficiently high, small events that are not really floods
    are excluded. With the annual series, non-floods in dry years may have an
    undue influence on shape of the distribution. This is particularly
    important for Australia, where both the range of flows and the
    non-occurrence of floods are greater than in many other countries such as
    the United States and the United Kingdom (Grayson <emphasis
    role="italic">et al</emphasis>., 1996). For this reason it would also be
    expected that the desirable ratio of K to N would be lower in Australia
    than in these countries.</para>

    <para/>

    <para>A criterion for independence of successive peaks must also be
    applied in selecting events. As discussed by Laurenson (1987), statistical
    independence requires physical independence of the causative factors of
    the flood, mainly rainfall and antecedent wetness. This type of
    independence is desirable if the POT series is used to estimate the
    distribution of annual floods. On the other hand, selection of POT series
    floods for design flood studies should consider the consequences of the
    flood peaks in assessing independence of events where damages or financial
    penalties are the most important design variables. Factors to be
    considered might include duration of inundation, and time required to
    repair flood damage. In both cases, the size or response time of the
    catchment will have some effect.</para>

    <para/>

    <para>The decision regarding a criterion for independence therefore
    requires subjective judgment by the designer or analyst in each case.
    There is often some conflict in that some flood effects are short-lived,
    perhaps only as long as inundation, while others such as the destruction
    of an annual crop may last as long as a year. It is thus not possible to
    recommend a simple and clear-cut criterion for independence. The
    circumstances and objectives of each study, and the characteristics of the
    catchment and flood data, should be considered in each case before a
    criterion is adopted. It is inevitable that the adopted criterion will be
    arbitrary to some extent.</para>

    <para/>

    <para>While no specific criterion can be recommended, it may be helpful to
    consider some criteria that have been used in past studies:</para>

    <para/>

    <itemizedlist mark="●">
      <listitem>
        <para>Bulletin 17B of the Interagency Advisory Committee on Water Data
        (1982) states that no general criterion can be recommended and the
        decision should be based on the intended use in each case, as
        discussed above. However in Appendix 14 of that document, a study by
        Beard (1974) is summarised where the criterion used is that
        independent flood peaks should be separated by five days plus the
        natural logarithm of the square miles of drainage area, with the
        additional requirement that intermediate flows must drop to below 75%
        of the lower of the two separate flood peaks. This may be suitable
        only for catchments larger than 1000 km<superscript>2</superscript>.
        Jayasuriya and Mein (1985) used this criterion in their study.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <itemizedlist mark="●">
      <listitem>
        <para>The UK Flood Studies Report (Natural Environment Research
        Council, 1975) used a criterion that flood peaks should be separated
        by three times the time to peak and that the flow should decrease
        between peaks to two thirds of the first peak.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <itemizedlist mark="●">
      <listitem>
        <para>M<superscript>c</superscript>Illwraith (1953), in developing
        design rainfall data for flood estimation, used the following criteria
        based on the rainfall causing the floods:</para>
      </listitem>

      <listitem>
        <itemizedlist mark="○">
          <listitem>
            <para>for rainfalls of short duration up to two hours, only the
            one highest flood within a period of 24 hours; and</para>
          </listitem>

          <listitem>
            <para>for longer rains, a period of 24 hours in which no more than
            5 mm of rain could occur between rain causing separate flood
            events.</para>
          </listitem>
        </itemizedlist>
      </listitem>
    </itemizedlist>

    <para/>

    <itemizedlist mark="●">
      <listitem>
        <para>In a study of small catchments, Potter and Pilgrim (1971) used a
        criterion of three calendar days between separate flood events but
        lesser events could occur in the intervening period. This was the most
        satisfactory of five criteria tested on data from seven small
        catchments located throughout eastern New South Wales. It also gave
        the closest approximation to the above criteria used by McIllwraith
        (1953).</para>
      </listitem>
    </itemizedlist>

    <para/>

    <itemizedlist mark="●">
      <listitem>
        <para>Pilgrim and McDermott (1982) and McDermott and Pilgrim (1983)
        adopted monthly maximum peak flows to give an effective criterion of
        independence in developing a design procedure for small to medium
        sized catchments. This was based primarily on the assumption that
        little additional damage would be caused by floods occurring within a
        month, and thus closer floods would not be independent in terms of
        their effects. This criterion was also used by Adams and McMahon
        (1985) and Adams (1987).</para>
      </listitem>
    </itemizedlist>

    <para/>

    <para>The criteria cited above represent a wide range, and illustrate the
    difficult and subjective nature of the choice. It is stressed that these
    criteria have been described for illustrative purposes only. In each
    particular application the designer or analyst should choose a criterion
    suitable to the analysis and relevant to all of the circumstances and
    objectives.</para>

    <para/>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="loweralpha">
      <listitem>
        <para><emphasis role="bold"/>Monthly and Seasonal Gauged Series</para>
      </listitem>
    </orderedlist>

    <para/>

    <para>In some circumstances, series other than the annual or POT series
    may be used. The monthly and seasonal series are the most useful.</para>

    <para/>

    <para>Maximum monthly flows are an approximation to the POT series in most
    parts of Australia, as the probability of two large independent floods
    occurring in the same month is low. Tropical northern Australia, the west
    coast Tasmania and the south west of Western Australia may be exceptions.
    While the monthly series is easier to compile than a POT series (most
    gauging authorities have monthly maximum flows on file) consideration
    needs to be given to significance of multiple floods in a month causing
    adverse events. It should be noted that not every monthly maximum flood
    will be selected, but only those large enough to exceed a selected base
    discharge, as is the case for the POT series. Care is required to check
    any floods selected in successive months for independence. Where the dates
    are close, the lower value should be discarded. The second highest flood
    in that month could then be checked from the records, but this would
    generally not be worthwhile. An example of use of the monthly series is
    described by Pilgrim and McDermott (1982).</para>

    <para/>

    <para>Seasonal flood frequencies are sometimes required. For these cases,
    the data are selected for the particular month or season as for the annual
    series, and the flood frequency analysis is carried out in a similar
    fashion to that for the annual series.</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <para><emphasis role="bold"/>Extension of Gauged Records</para>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" inheritnum="inherit"
                 numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" inheritnum="inherit"
                     numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" inheritnum="inherit"
                         numeration="lowerroman">
              <listitem>
                <para><emphasis role="bold"/>General</para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>It may sometimes be possible to extend the recorded data by values
    estimated from longer records on adjacent catchments, by use of a
    catchment rainfall-runoff model, or by historical data from before the
    commencement of records. If this can be done validly, the effective sample
    size of the data will be increased and the reliability of the analysis
    will be greater. However, care is necessary to ensure that the extended
    data are valid, and that real information has been added. Several
    procedures can be used. Some of these procedures are described in the
    following sections.</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" numeration="lowerroman">
              <listitem>
                <para><emphasis role="bold"/>Extension Using Data From An
                Adjacent Catchment</para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>Suppose there are n<subscript>1</subscript> years of record at site
    1, the study catchment, and a longer record of length
    n<subscript>2</subscript> years at site 2, an adjacent catchment. Using
    the overlapping record a relationship can be developed between
    q<subscript>1</subscript> and q<subscript>2</subscript>, the peak flows at
    sites 1 and 2, and then used to extend the shorter record at site
    1.</para>

    <para/>

    <para>The most intuitive way to extend the site 1 record is to use the
    best fit relationship between q<subscript>1</subscript> and
    q<subscript>2</subscript> to obtain a smoothed estimate of
    q<subscript>1</subscript> given the observed q<subscript>2</subscript>.
    The best fit usually would be determined using regression methods.
    However, this approach is not recommended as it biases downward the
    variance of the extended record because the scatter about the regression
    line is ignored.</para>

    <para/>

    <para>This bias can be overcome by one of two methods:</para>

    <para/>

    <orderedlist continuation="restarts" numeration="arabic">
      <listitem>
        <para>A random error sampled from the regression error distribution
        can be added to the smoothed estimate for q<subscript>1</subscript>
        (Matalas and Jacobs, 1964). While this will preserve the expected
        variance of the extended record it produces an arbitrary extended
        record.</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <para>A maintenance of variance extension (MOVE) method can be used to
        derive a relationship between q<subscript>1</subscript> and
        q<subscript>2</subscript> which preserves the expected value of
        selected statistics for q<subscript>1</subscript> such as the
        variance. For example, the simplest MOVE method preserves the mean and
        variance of q<subscript>1</subscript> using the following
        relationship</para>
      </listitem>
    </orderedlist>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>FUNC { q SUB 1 ~ = ~ E ` LEFT ( ` q SUB 1 ` RIGHT ) ~ + ~ sign ` (
      ` r ` ) ` SQRT { { Var ` LEFT ( ` q SUB 1 ` RIGHT ) } OVER { Var ` LEFT
      ( ` q SUB 2 ` RIGHT ) } } ~ LEFT [ ` q SUB 2 ~ - ~ E ` LEFT ( ` q SUB 2
      ` RIGHT ) ` RIGHT ] }</para>
    </section>

    <para>where sign® is the correlation between q<subscript>1</subscript> and
    q<subscript>2</subscript> and E() and Var() are the mean and variance.
    More details are provided in Hirsch (1982) and Grygier <emphasis
    role="italic">et al</emphasis>. (1989).</para>

    <para/>

    <para>When annual floods are used, the dates of the corresponding annual
    floods may be different resulting in a lack of physical basis for the
    relation. It is a moot point whether this constitutes grounds for not
    using such data as the purpose of record extension is to exploit the
    association between the annual peaks at the two sites. The existence of a
    causal linkage is an extremely desirable prerequisite for use of an
    association between the peak flows at the two sites but is not an absolute
    prerequisite.</para>

    <para/>

    <para>Wang (2001) presents a Bayesian approach that exploits the useful
    information arising from the association of peak flows between sites 1 and
    2. Though not a record extension method, it does augment the information
    beyond that in the site 1 record.</para>

    <para/>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="lowerroman">
      <listitem>
        <para><emphasis role="bold"/>Use of a Catchment Modelling
        System</para>
      </listitem>
    </orderedlist>

    <para/>

    <para/>

    <para>Suppose a rainfall record longer than the flow record is available
    at the study catchment. A model of the upstream catchment can be
    calibrated and used to extend the flow record; the model may range from a
    simple rainfall-runoff regression to the continuous flow prediction of the
    catchment response to rainfall using a system of relevant process models.
    As discussed in the previous section such an approach is likely to produce
    smoothed flow estimates which bias downwards the variability of the flow
    record. The use of smoothed estimates, therefore, is not recommended. If a
    catchment modelling approach is to be used the variability needs to be
    preserved, for example, by sampling from the error distribution describing
    the discrepancy between observed and predicted, or simulated, flows. This
    error distribution can be complex with the magnitude of errors growing
    with the magnitude of the flow. Care needs to be exercised to ensure that
    such error characteristics are replicated in the sampling scheme.</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" numeration="lowerroman">
              <listitem>
                <para><emphasis role="bold"/>Station-year Method</para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>This is included only to warn against its shortcomings. In this
    procedure, records from several adjacent catchments are joined
    "end-to-end" to give a single record equal in length to the sum of the
    lengths of the constituent records. As discussed by Clarke-Hafstad (1942)
    for rainfall data, spatial correlation between the records of the adjacent
    stations invalidates the procedure.</para>

    <para/>

    <orderedlist continuation="continues" inheritnum="inherit"
                 numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" inheritnum="inherit"
                     numeration="loweralpha">
          <listitem>
            <para>
              <emphasis role="bold"/>

              <phrase id="WPGeneratedID_TOC_1_12">Rating Error in Gauged
              Flows</phrase>
            </para>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>Though it is widely accepted that discharge estimates for large
    floods can be in considerable error, there is limited published
    information on these errors and how they can be allowed for in a flood
    frequency analysis. Rating error can arise from a number of mechanisms
    including:</para>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>For large floods the rating curve typically is extrapolated or
        fitted to indirect discharge estimates. This can introduce a
        systematic but unknown bias.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>If the gauging station is located at a site with an unstable
        cross section the rating curve may shift causing a systematic but
        unknown bias.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <para>The conceptual model of rating error presented in this section is
    based on Kuczera (1999) and is considered to be rudimentary and subject to
    refinement. It is assumed the cross section is stable with the primary
    source of rating error arising from extension of the rating curve to large
    floods. Measurement errors which may be systematic or random are not
    considered.</para>

    <para/>

    <para>Potter and Walker (1981, 1985) observe that flood discharge is
    inferred from a rating curve which is subject to discontinuous measurement
    error. Consider Figure 1.1.<link linkend="WPGeneratedID_TOC_1_13">4</link>
    which depicts a rating curve with two regions having different error
    characteristics. The interpolation zone consists of that part of the
    rating curve well defined by discharge-stage measurements; typically the
    error coefficient of variation (CV) would be practically negligible, say
    5%. In the extension zone the rating curve is extended by methods such as
    slope-conveyance, log-log extrapolation or fitting to indirect discharge
    estimates. Typically such extensions are smooth and, therefore, can induce
    systematic under- or over-estimation of the true discharge over a range of
    stage. In Figure 1.1.<link linkend="WPGeneratedID_TOC_1_13">4</link>, the
    illustrated extension systematically underestimates the true
    discharge.</para>

    <para/>

    <para>The extension zone can be considerable. Hatfield and Muir (1984)
    report that the highest gauged flow at over 50% of NSW stations was less
    than 20% of the estimated highest recorded flow. The extension error CV is
    not well known but Potter and Walker (1981, 1985) suggest it may be as
    high as 30%. Brown (1983) concluded that the accuracy of high floods at
    most stations is probably not much better than 25% and in many cases much
    worse.</para>

    <para>
      <anchor id="WPGeneratedID_Xref_RatingCurveErrorFig_1"/>

      <graphic align="center" depth="3.696in"
               fileref="ARRFFA/WPGeneratedName2.JPG" format="JPG" valign="top"
               width="4.920in"/>
    </para>

    <para>Figure 1.1.4 - Rating Curve Extension Error</para>

    <para>Though Figure 1.1.<link linkend="WPGeneratedID_TOC_1_13">4</link>
    represents an idealization of actual rating curve extension two points of
    practical significance are noted:</para>

    <para/>

    <orderedlist continuation="restarts" numeration="arabic">
      <listitem>
        <para>The error is systematic in the sense that the extended rating is
        likely to diverge from the true rating as the discharge increases. The
        error, therefore, is likely to be highly correlated. In the
        idealization of Figure 1.1.<link
        linkend="WPGeneratedID_TOC_1_13">4</link> it is perfectly correlated;
        this correlation, however, is a result of the simplification adopted;
        and</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <para>The interpolation zone anchors the error in the extension zone.
        Therefore, the error in the extension zone depends on the distance
        from the anchor point and not from the origin. This error is termed
        incremental because it originates from the anchor point rather than
        the origin of the rating curve.</para>
      </listitem>
    </orderedlist>

    <para/>

    <para>This conceptual rating curve error model is incorporated into the
    Bayesian fitting procedure described in Section <xref
    linkend="WPGeneratedID_Xref_BayesianCalSect_1"/> of this Chapter in Book ?
    of Australian Rainfall and Runoff.</para>

    <para/>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="loweralpha">
      <listitem>
        <para>
          <emphasis role="bold"/>

          <phrase id="WPGeneratedID_TOC_1_13">Historical and Paleo Flood
          Information</phrase>
        </para>
      </listitem>
    </orderedlist>

    <para/>

    <para>A flood may have occurred before, during or after the period of
    gauged record, and is known to be the largest flood, or flood of other
    known rank, over a period longer than that of the gauged record. Such
    floods can provide valuable information, and should be included in the
    analysis if possible.</para>

    <para/>

    <para>Care is needed in assessing historical floods. Only stages usually
    are available, and these may be determined by</para>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>Flood marks recorded on buildings or structures; care needs to
        be taken, however, to ensure that buildings and structures have not
        been moved in the intervening period;</para>
      </listitem>
    </itemizedlist>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>Old newspaper reports and photographs; and</para>
      </listitem>
    </itemizedlist>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>Verbal evidence. While verbal evidence often is untrustworthy,
        it still warrants checking to assess its reliability.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <para>A further problem is that the channel morphology, and hence the
    stage-discharge relation of the stream, may be different to that
    applicable to the period of gauged record.</para>

    <para/>

    <para>It is desirable to carry out frequency analyses both by including
    and excluding the historical data. The analysis including the historical
    data should be used unless in the comparison of the two analyses, the
    magnitudes of the observed peaks, uncertainty regarding the accuracy of
    the historical peaks, or other factors, suggest that the historical peaks
    are not indicative of the extended period or are not accurate. All
    decisions made should be thoroughly documented.</para>

    <para/>

    <para>Paleofloods are major floods that have occurred outside the
    historical record, but which are evidenced by geological, geomorphological
    or botanical information. Techniques of paleohydrology have been described
    by Costa (1978, 1983, 1986), Kochel <emphasis role="italic">et
    al</emphasis>. (1982) and O'Connell <emphasis role="italic">et
    al</emphasis>. (2002) with a succinct summary given by Stedinger and Cohn
    (1986). Although high accuracy is not possible with these estimates, they
    may only be marginally less accurate than other estimates requiring
    extrapolation of rating curves, and they have the potential for greatly
    extending the data base and providing valuable information on the right
    hand tail of the underlying flood probability distribution. Hosking and
    Wallis (1986) and Jin and Stedinger (1989) consider procedures for
    assessing the value of paleoflood estimates in flood frequency analysis.
    Only a little work on this topic has been carried out in Australia, but
    its potential has been indicated by its use to identify the five largest
    floods in the last 700 years in the Finke River Gorge in central Australia
    (Baker <emphasis role="italic">et al</emphasis>., 1983; Baker, 1984), and
    for more frequent floods, by identification of the six largest floods that
    occurred since a major flood in 1897 on the Katherine River in the
    Northern Territory (Baker, 1984). The use of paleoflood data should be
    considered where this is possible in view of the potential benefits, and
    further development of the technique would be desirable. The two major
    problems in its use are that there are not many sites where it can be
    employed, and climate changes may have affected the homogeneity of
    long-term flood data.</para>

    <para/>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="loweralpha">
      <listitem>
        <para>
          <emphasis role="bold"/>

          <phrase id="WPGeneratedID_TOC_1_14">Data Characterizing Climate
          Long-Term Persistence</phrase>
        </para>
      </listitem>
    </orderedlist>

    <para/>

    <para>There is growing evidence that flood peaks are not identically
    distributed from year to year in some parts of Australia and that flood
    risk is dependent on long-term climate variability. Much of this evidence
    arises from the idea of alternating flood and drought dominated regimes
    that exist on decadal and longer time-scales which was first proposed by
    Warner and Erskine (1988). The climate-dependence of flood risk is an
    important consideration when assessing flood risk.</para>

    <para/>

    <para>Most flood frequency applications will require assessment of the
    long-term flood risk; that is, the flood risk that is independent of a
    particular current climate state. If a flood record was sufficiently long
    to sample all climate states affecting flood risk, a traditional analysis
    assuming homogeneity would yield the long-term flood risk. Unfortunately
    many flood records are relatively short and may be dominated by one
    climate state. Blind use of such data can result in substantial bias in
    long-term flood risk estimates. For this reason it may be necessary to
    obtain climate data which characterizes long-term persistence in climate
    and to investigate the homogeneity of the flood distribution.</para>

    <para/>

    <para>A number of known climate phenomena impact on Australia climate
    variability. Most well known is the inter-annual El Nino/Southern
    Oscillation (ENSO). The cold ENSO phase, La Nina, results in a marked
    increase in flood risk across Eastern Australia, whereas El Nino years
    typically are periods without the occurrence of major floods (Kiem
    <emphasis role="italic">et al</emphasis>., 2003).</para>

    <para/>

    <para>There is also mounting evidence that longer-term climate processes
    also have a major impact on flood risk. The Interdecadal Pacific
    Oscillation (IPO) is a low frequency climate process related to the
    variable epochs of warming and cooling in the Pacific Ocean and is
    described by an index derived from low pass filtering of sea surface
    temperature (SST) anomalies in the Pacific Ocean (Power <emphasis
    role="italic">et al</emphasis>., 1998, 1999; Allan, 2000). The IPO is
    similar to the Pacific Decadal Oscillation (PDO) of Mantua <emphasis
    role="italic">et al</emphasis>. (1997), which is defined as the leading
    principal component of North Pacific monthly sea surface temperature
    variability.</para>

    <para/>

    <para>The IPO time series from 1880 is displayed in Figure 1.1.<link
    linkend="WPGeneratedID_TOC_1_15">5</link>. It reveals extended periods
    where the index either lies below or above zero. Power <emphasis
    role="italic">et al</emphasis>. (1999) have shown that the association
    between ENSO and Australian climate is modulated by the IPO. A strong
    association was found between the magnitude of ENSO impacts during
    negative IPO phases, whilst during positive IPO phases a weaker, less
    predictable relationship was observed. Additionally, Kiem <emphasis
    role="italic">et al</emphasis>. (2003) and Kiem and Franks (2004) analysed
    NSW flood and drought data and demonstrated that the IPO negative state
    magnified the impact of La Nina events. Moreover, they demonstrated that
    the IPO negative phase, related to midlatitude Pacific Ocean cooling,
    appears to result in an increased frequency of cold La Nina events. The
    net effect of the dual modulation of ENSO by IPO is the occurrence of
    multi-decadal periods of elevated and reduced flood risk.</para>

    <para/>

    <para>To place this in context, shown in Figure 1.1.<link
    linkend="WPGeneratedID_TOC_1_16">6</link> are regional flood index curves
    based on about 40 NSW sites for the different IPO states (Kiem <emphasis
    role="italic">et al</emphasis>., 2003). As shown in that figure, the 1 in
    100 AEP flood during years with a positive IPO index corresponds to the 1
    in 6 AEP flood during years with a negative IPO index. Note that the
    dashed lines in this figure represent the confidence limits of the flood
    frequency curves. Micevski <emphasis role="italic">et al</emphasis>.
    (2003) investigating a range of sites in NSW found that floods occurring
    during IPO "negative" periods were, on average, about 1.8 times larger
    than floods with the same frequency during IPO "positive" periods.</para>

    <para>
      <anchor id="WPGeneratedID_Xref_IPOFig_1"/>

      <graphic align="center" depth="3.382in"
               fileref="ARRFFA/WPGeneratedName3.JPG" format="JPG" valign="top"
               width="5.511in"/>
    </para>

    <para>Figure 1.1.5 - Annual Average IPO Time Series</para>

    <para>
      <anchor id="WPGeneratedID_Xref_IPOEpochFig_1"/>

      <graphic align="center" depth="3.889in"
               fileref="ARRFFA/WPGeneratedName4.JPG" format="JPG" valign="top"
               width="4.723in"/>
    </para>

    <para>Figure 1.1.6 - NSW Regional Flood Index Frequency Curves for
    Positive and Negative IPO Epochs (after Kiem et al., 2003)</para>

    <para>The finding that flood risk in parts of Australia is modulated by
    low frequency climate variability is recent. Users are reminded this is an
    area of active research and to keep abreast of future developments.
    Nonetheless, the practical implication of these findings is considerable.
    A standard flood frequency analysis can produce significantly biassed
    estimates of flood risk if this phenomenon is ignored. Guidance on how to
    conduct a flood frequency analysis in such circumstances is provided in
    Section <xref linkend="WPGeneratedID_Xref_Non-HomogDataSect._1"/> of this
    Book in Australian Rainfall and Runoff.</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <para>
              <emphasis role="bold"/>

              <phrase id="WPGeneratedID_TOC_1_16">Regional Flood
              Information</phrase>
            </para>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para/>

    <para>Whereas the primary focus of this chapter is flood frequency
    analysis using at-site information, the accuracy of the frequency analysis
    can be improved, substantially in some cases, by augmenting at-site
    information with regional information. Subsequent chapters in this Book
    describe methods for estimating flood frequency at ungauged sites.
    Provided such methods also provide estimates of uncertainty, the regional
    information can be pooled with the at-site information to yield more
    accurate results. Section 6.3.5 shows how regional information on flood
    probability model parameters can be pooled with at-site information. When
    pooling at-site and regional information it is important to establish that
    both sources of information are consistent - that is, they yield
    statistically consistent results.</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <para><emphasis role="bold"/>Missing Records</para>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>Streamflow data frequently contain gaps for a variety of reasons
    including the malfunction of recording equipment. Rainfall records on the
    catchment and streamflow data from nearby catchments may indicate the
    likelihood of a large flood having occurred during the gap. A regression
    may be able to be derived to enable a missing flood to be estimated, but
    as discussed in Section of this Chapter in Book of Australian Rainfall and
    Runoff, the degree of correlation is often insufficient for a quantitative
    estimate.</para>

    <para/>

    <para>For annual series the missing record period is of no consequence and
    can be included in the period of record, if it can be determined that the
    largest flow for the year occurred outside the gap, or that no large rains
    occurred during the gap. However the rainfall records and streamflow on
    nearby catchments might indicate that a large flood could have occurred
    during the period of missing record. If a regression with good correlation
    can be derived from concurrent records, the missing flood can be estimated
    and used as the annual flood for the year. If the flood cannot be
    estimated with reasonable certainty, the whole year should be excluded
    from the analysis.</para>

    <para/>

    <para>For POT series data, treatment of missing records is less clear.
    McDermott and Pilgrim (1982) tested seven methods, leading to the
    following recommendations based on the assumption that the periods of
    missing data are random occurrences and are independent of the occurrence
    of flood peaks.</para>

    <para/>

    <orderedlist continuation="restarts" numeration="arabic">
      <listitem>
        <para>Where a nearby station record exists covering the missing record
        period, and a good relation between the flood peaks on the two
        catchments can be obtained, then use this relation and the nearby
        station record to fill in the missing events of interest.</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <para>Where a nearby station record exists covering the missing record
        period, and the relation between the flood peaks on the two catchments
        is such that only the occurrence of an event can be predicted but not
        its magnitude, then:</para>
      </listitem>
    </orderedlist>

    <itemizedlist mark="•">
      <listitem>
        <para>for record lengths less than 20 years, ignore the missing data
        and include the missing period in the overall period of record;</para>
      </listitem>

      <listitem>
        <para>for record lengths greater than 20 years, subtract an amount
        from each year with missing data proportional to the ratio of the
        number of peaks missed to the total number of ranked peaks in the
        year.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <orderedlist continuation="restarts" numeration="arabic">
      <listitem>
        <para>Where no nearby station record exists covering the missing
        record period, or where no relation between flood peaks on the
        catchment exists, then ignore the missing data and include the missing
        record period in the overall period of record.</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="restarts" numeration="arabic">
      <listitem>
        <para><emphasis role="bold"/>CHOICE OF FLOOD PROBABILITY MODEL</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" inheritnum="inherit"
                 numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" inheritnum="inherit"
                     numeration="loweralpha">
          <listitem>
            <para><emphasis role="bold"/>General</para>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>As noted in Section <xref
    linkend="WPGeneratedID_Xref_ConceptualFrameworkSection_1"/> of this
    Chapter, it is assumed that each flood peak in an annual or POT series is
    statistically independent of other flood peaks in the series. In addition
    the flood probability model, described by its probability density function
    (pdf) p(q | θ, x, M), must be specified.</para>

    <para/>

    <para>There is no universally accepted flood probability model. Many types
    of probability distributions have been applied to flood frequency
    analysis. Unfortunately, it is not possible to determine the true or
    correct form of distribution (see, for example, Cunnane, 1985), and there
    is no rigorous analytical proof that any particular probability
    distribution for floods is the correct theoretical distribution. The
    appropriateness of these distributions can be tested by examining the fit
    of each distribution to observed flood data. Various empirical tests of
    different distributions have been carried out with recorded data from many
    catchments but conclusive evidence is not possible as a result of the wide
    range of samples that can be realised from a given population. Examples of
    sampling experiments illustrating this problem are given by Alexander
    (1957) and Benson in Dalrymple (1960). The choice of flood probability
    model is further exacerbated by recent evidence that in certain parts of
    Australia the flood record is not homogeneous due to variations in
    long-term climate controls.</para>

    <para/>

    <para>Given these considerations it is considered inappropriate to be
    prescriptive with regard to choice of flood probability model. As a
    general rule the selected probability distribution family M should be
    consistent with available data. It is recognized that more than one
    probability distribution family may be consistent with the data. One
    approach to deal with this problem is to select the distribution family on
    the basis of best overall fit to a range of catchments within a region or
    landscape space. One approach for assessing overall goodness of fit is
    based on the use of L moment diagrams (see Stedinger <emphasis
    role="italic">et al</emphasis>., 1993 and, in particular, Section 18.3.3
    of that publication for more details).</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <para><emphasis role="bold"/>Choice of Distribution Family for
            Annual Series</para>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" inheritnum="inherit"
                 numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" inheritnum="inherit"
                     numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" inheritnum="inherit"
                         numeration="lowerroman">
              <listitem>
                <para><emphasis role="bold"/>General</para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>Two distribution families are suggested as reasonable initial
    choices, namely:</para>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>generalized extreme value (GEV); and</para>
      </listitem>
    </itemizedlist>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>log Pearson III (LP3) families.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <para>These families have been shown to fit most flood data adequately.
    Nonetheless the user is reminded that there is no rigorous justification
    for these families, which is particularly important when extrapolating.
    Demonstrated in Example <link linkend="WPGeneratedID_TOC_1_35">1</link> is
    the importance of understanding the mechanisms controlling flood
    response.</para>

    <para/>

    <para>The following sections describe the GEV and LP3 distributions and
    also some other distributions which may be more appropriate in certain
    circumstances.</para>

    <para/>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="lowerroman">
      <listitem>
        <para><emphasis role="bold"/>Generalized Extreme Value (GEV)
        Distribution</para>
      </listitem>
    </orderedlist>

    <para/>

    <para>Listed in Table 1.1.<link linkend="WPGeneratedID_TOC_1_17">1</link>
    are the pdf p(q | θ), distribution function P( Q ≤ q | θ) and product
    moments for the generalized extreme value (GEV) distribution. It has three
    parameters which are:</para>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>τ - the location parameter;</para>
      </listitem>

      <listitem>
        <para>α - the scale parameter; and</para>
      </listitem>

      <listitem>
        <para>κ - the shape parameter.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <para>When the shape parameter κ equals 0, the GEV simplifies to the
    Gumbel distribution whose details are provided in Table 1.1.<link
    linkend="WPGeneratedID_TOC_1_17">1</link> also. For positive values of κ
    there exists an upper bound, while for negative values of κ there exists a
    lower bound.</para>

    <para/>

    <para>Of the widely used distribution families, the GEV distribution has
    the strongest theoretical appeal because it is the asymptotic distribution
    of extreme values for a wide range of underlying parent distributions. In
    a flood frequency context, suppose there are N flood peaks in a year.
    Provided N is large and the flood peaks are identically and independently
    distributed, the distribution of the largest peak flow in the year
    approaches the GEV under quite general conditions. Unfortunately,
    hydrologic reality does not always fit comfortably with these assumptions.
    First, the number of independent flood peaks in any year may not be
    sufficient to ensure asymptotic behaviour. Second, it remains unclear
    whether the within-year independent flood peaks are random realizations
    from the same probability distribution.</para>

    <para/>

    <para>The GEV has gained widespread acceptance (for example, Natural
    Environment Research Council, 1975; Wallis and Wood, 1985; Handbook of
    Hydrology, Stedinger <emphasis role="italic">et al</emphasis>.,
    1993).<anchor id="WPGeneratedID_Xref_pdfTable_1"/></para>

    <section label="paragraph" role="textbox">
      <para>Table 1.1.1 - - Selected homogeneous probability models families
      for use in flood frequency analysis</para>

      <table>
        <title>TABLE A</title>

        <tgroup cols="3">
          <colspec align="center" colname="Column1" colwidth="0.688in"/>

          <colspec align="center" colname="Column2" colwidth="3.375in"/>

          <colspec align="center" colname="Column3" colwidth="2.185in"/>

          <tbody>
            <row>
              <entry align="center" valign="center">
                <para>Family</para>
              </entry>

              <entry align="center" valign="center">
                <para>Distribution</para>
              </entry>

              <entry align="center" valign="center">
                <para>Moments</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>Generalised Extreme Value</para>

                <para>(GEV)</para>
              </entry>

              <entry align="center" valign="center">
                <para>FUNC { p ` ( ` q ` LINE LINE ` theta ` ) ~ = ~ 1 OVER
                alpha ` exp ` LEFT LBRACE ` - ` LEFT [ ` 1 ~ - ~ { kappa ` ( `
                q ~ - ~ tau ` ) } OVER alpha ` RIGHT ] SUP { ~ 1 OVER kappa }
                ` RIGHT RBRACE ~ TIMES } # ~ # FUNC { ~ LEFT [ ` 1 ~ - ~ {
                kappa ` ( ` q ~ - ~ tau ` ) } OVER alpha ` RIGHT ] SUP { ~ 1
                OVER kappa ~ - ~ 1 } }</para>

                <para>FUNC { P ` ( ` Q ~ &lt;= ~ q ` LINE ` theta ` ) ~ = } #
                ~ # FUNC { exp LEFT LBRACE ~ - ~ LEFT [ ~ 1 ~ - ~ { kappa ` (
                ` q ~ - ~ tau ` ) } OVER alpha ` RIGHT ] SUP { ~ 1 OVER kappa
                } ~ RIGHT RBRACE }</para>

                <para>when FUNC { kappa ~ &gt; ~ 0 } , FUNC { q ~ &lt; ~ tau ~
                + ~ alpha OVER kappa } ;</para>

                <para/>

                <para>when FUNC { kappa ~ &lt; ~ 0 } , FUNC { q ~ &gt; ~ tau ~
                + ~ alpha OVER kappa }</para>
              </entry>

              <entry align="center" valign="center">
                <para>Mean (q) =</para>

                <para>FUNC { tau ` + ` alpha OVER kappa `LEFT [ ` 1 ` - `
                GAMMA ` ( ` 1 ` + ` kappa ` ) ` RIGHT ] }</para>

                <para>for κ &gt; -1</para>

                <para/>

                <para>Variance (q) = FUNC { { alpha SUP 2 } OVER { kappa SUP 2
                } ` LEFT [ ` GAMMA ( ` 1 ` + ` 2 ` kappa ` ) ` - ` [ ` GAMMA (
                ` 1 ` + ` kappa ` ) ` ] SUP 2 ` RIGHT ] }</para>

                <para>for κ &gt; - 0.5</para>

                <para/>

                <para>where Γ() is the gamma function</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>Gumbel</para>
              </entry>

              <entry align="center" valign="center">
                <para>FUNC { p ` ( ` q ~ LINE ~ theta ` ) ~ = ~ 1 OVER alpha `
                exp ` LEFT [ ~ - ~ { ( ` q ~ - ~ tau ` ) } OVER alpha ` RIGHT
                ] } # ~ # FUNC { exp ` LEFT LBRACE ~ - ~ exp ` LEFT [ ~ - ~ {
                ( ` q ~ - ~ tau ` ) } OVER alpha ` RIGHT ] ~ RIGHT RBRACE }
                FUNC { P ` ( ` Q ~ &lt;= ~ q ` LINE ` theta ` ) ~ = ~ exp `
                LEFT LBRACE ~ - ~ exp ` LEFT [ ~ - ~ { ( ` q ~ - ~ tau ` ) }
                OVER alpha ` RIGHT ] ~ RIGHT RBRACE }</para>
              </entry>

              <entry align="center" valign="center">
                <para>Mean (q) = τ + 0.5772α</para>

                <para/>

                <para>Variance (q) = FUNC { { PI SUP 2 ` alpha SUP 2 } OVER 6
                }</para>

                <para/>

                <para>Skew (q) = 1.1396</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>Log-Pearson III (LP3)</para>
              </entry>

              <entry align="center" valign="center">
                <para>FUNC { p ` ( ` q ~ LINE ~ theta ` ) ~ = ~ { LINE ~ beta
                ~ LINE } OVER { q ` GAMMA ` ( ` alpha ` ) } ` LEFT [ ` beta `
                ( ` ln ` q ~ - ~ tau ` ) ` RIGHT ] SUP { ` alpha ~ - ~ 1 } } ~
                TIMES # ~ # FUNC { exp ` LEFT [ ~ - ~ beta ` ( ` ln ` q ~ - ~
                tau ` ) ` RIGHT ] }</para>

                <para>α &gt; 0</para>

                <para>when β &gt; 0, ln q &gt; τ;</para>

                <para>when β &lt; 0, ln q &lt; τ</para>
              </entry>

              <entry align="center" valign="center">
                <para>Mean (Ln q) = m = τ + FUNC { alpha OVER beta }</para>

                <para/>

                <para>Variance (Ln q) = s2 = FUNC { alpha OVER beta SUP 2
                }</para>

                <para/>

                <para>Skew (Ln q) = g =</para>

                <para>FUNC { LEFT LBRACE ~ STACK { 2 OVER { SQRT alpha } ~ ~
                if ~ beta ~ &gt; ~ 0 # -` 2 OVER { SQRT alpha } ~ ~ if ~ beta
                ~ &lt; ~ 0 } RIGHT . }</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <para/>

    <section label="paragraph" role="textbox">
      <para/>

      <table>
        <title>TABLE A</title>

        <tgroup cols="3">
          <colspec align="center" colname="Column1" colwidth="0.850in"/>

          <colspec align="center" colname="Column2" colwidth="3.307in"/>

          <colspec align="center" colname="Column3" colwidth="2.091in"/>

          <tbody>
            <row>
              <entry align="center" valign="center">
                <para>
                  <emphasis role="bold">Family</emphasis>
                </para>
              </entry>

              <entry align="center" valign="center">
                <para>
                  <emphasis role="bold">Distribution</emphasis>
                </para>
              </entry>

              <entry align="center" valign="center">
                <para>
                  <emphasis role="bold">Moments</emphasis>
                </para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>Log-Normal</para>
              </entry>

              <entry align="center" valign="center">
                <para>FUNC { p ` ( ` q ~ LINE ~ theta ` ) ~ = ~ 1 OVER { SQRT
                { q ` 2 ` pi ` s SUP 2 } } ~ TIMES } # ~ # FUNC { exp ` LEFT [
                ~ - ~ 1 OVER { 2 ` s SUP 2 } ~ ( ` ln ` q ~ - ~ m ` ) SUP 2 `
                RIGHT ] } q &gt; 0, s &gt; 0</para>
              </entry>

              <entry align="center" valign="center">
                <para>Mean (Ln q) = m</para>

                <para/>

                <para>Variance (Ln q) = s<superscript>2</superscript></para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>Generalised Pareto</para>
              </entry>

              <entry align="center" valign="center">
                <para>FUNC { p ` ( ` q ~ LINE ~ theta ` ) ~ = ~ 1 OVER beta `
                LEFT ( ` 1 ~ - ~ { kappa ` ( ` q ~ - ~ q SUB * ` ) } OVER beta
                ` RIGHT ) SUP { 1 OVER kappa ~ - ~ 1 } } FUNC { P ` ( ` Q ~
                &lt;= ~ q ` LINE ` theta ` ) ~ = ~ exp LEFT LBRACE ~ - ~ LEFT
                [ ~ 1 ~ - ~ { kappa ` ( ` q ~ - ~ q SUB * ` ) } OVER alpha `
                RIGHT ] SUP { ~ 1 OVER kappa } ~ RIGHT RBRACE } when κ &lt; 0,
                q<subscript>*</subscript> ≤ q &lt; ∞;</para>

                <para/>

                <para>when κ &gt; 0, q<subscript>*</subscript> ≤ q &lt; FUNC {
                beta OVER kappa }</para>
              </entry>

              <entry align="center" valign="center">
                <para>Mean (q) = FUNC { q SUB * ~ + ~ beta OVER { 1 ~ + ~
                kappa } }</para>

                <para/>

                <para>Variance (q) =</para>

                <para/>

                <para>FUNC { { beta SUP 2 } OVER { ( ` 1 ~ + ~ kappa ` ) SUP 2
                ` ( ` 1 ~ + ~ 2 ` kappa ` ) } }</para>

                <para/>

                <para>Skew (q) =</para>

                <para/>

                <para>FUNC { { 2 ` ( ` 1 ~ - ~ kappa ` ) ` ( ` 1 ~ + ~ 2 `
                kappa ` ) SUP { 1 OVER 2 } } OVER { ( ` 1 ~ + ~ 3 ` kappa ` )
                }} # FUNC { kappa ~ &gt; ~ - ` 1 OVER 3 }</para>

                <para/>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>Exponential</para>
              </entry>

              <entry align="center" valign="center">
                <para>FUNC { p ` ( ` q ~ LINE ~ theta ` ) ~ = ~ 1 OVER beta ~
                exp ~ LEFT ( ` - ` { q ~ - ~ q SUB * } OVER beta ` RIGHT )
                }</para>

                <para>FUNC { P ` ( ` Q ~ &lt;= ~ q ` LINE ` theta ` ) ~ = ~ 1
                ~ - ~ exp ` LEFT ( ` - ` { q ~ - ~ q SUB * } OVER beta ` RIGHT
                ) ` , } # ~ # FUNC { q ~ &gt;= ~ q SUB * }</para>
              </entry>

              <entry align="center" valign="center">
                <para>Mean (q) = q<subscript>*</subscript> + β</para>

                <para/>

                <para>Variance (q) =β<superscript>2</superscript></para>

                <para/>

                <para>Skew (q) = 2</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <para/>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="lowerroman">
      <listitem>
        <para>
          <emphasis role="bold"/>

          <phrase id="WPGeneratedID_TOC_1_17">Log Pearson III (LP3)
          Distribution</phrase>
        </para>
      </listitem>
    </orderedlist>

    <para/>

    <para>Listed in Table 1.1.<link linkend="WPGeneratedID_TOC_1_17">1</link>
    are the pdf p(q | θ) and product moments for log Pearson III (LP3)
    distribution. The LP3 distribution has three parameters:</para>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>τ - the scale parameter; and</para>
      </listitem>

      <listitem>
        <para>α and β - the shape parameters.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <para>When the skew of log q is zero, the distribution simplifies to the
    log normal whose details are provided in Table 1.1.<link
    linkend="WPGeneratedID_TOC_1_17">1</link>.</para>

    <para/>

    <para>The LP3 distribution has been widely accepted in practice because it
    consistently fits flood data as well if not better than other probability
    families. It has performed best of those that have been tested on data for
    Australian catchments (Conway, 1970; Kopittke <emphasis role="italic">et
    al</emphasis>., 1976; McMahon, 1979; and McMahon and Srikanthan, 1981).
    Furthermore, it is the recommended distribution for the United States in
    Bulletin 17B of the Interagency Advisory Committee on Water Data
    (1982).</para>

    <para/>

    <para>The distribution, however, is not well-behaved from an inference
    perspective. Direct inference of the parameters α, β and τ can cause
    numerical problems. For example, when the skew of log q is close to zero,
    the shape parameter α tends to infinity. Experience shows that the
    parameters can be estimated reasonably using the first three moments of
    log(q). Note that τ is a lower bound for positive skew and an upper bound
    for negative skew.</para>

    <para/>

    <para>A problem arises when the absolute value of the skew of log q
    exceeds 2; that is, when α &lt; 1. When α &gt; 1, log q has a gamma-shaped
    density. When α ≤ 1, the density of log q changes to a J-shaped function.
    Indeed when α = 1, the pdf of log q degenerates to that of an exponential
    distribution with scale parameter β and location parameter τ. For α ≤ 1,
    the J-shaped density seems to be over-parameterized with three parameters.
    In such circumstances, it is pointless to use the LP3. It is suggested
    either the GEV or the generalized Pareto distributions be used as a
    substitute.</para>

    <para/>

    <para>An analytical form of the distribution function is not available for
    the LP3 and log-normal distributions. To compute the quantile
    q<subscript>Y</subscript> (that is, the discharge with a 1 in Y AEP) the
    following equation may be used:</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.11)FUNC { log ` q SUB Y ~ = ~ m ~ + ~ K SUB Y ` ( ` g ` ) ` s
      }</para>
    </section>

    <para>where m, s and g are the mean, standard deviation and skewness of
    the log discharge and K<subscript>Y</subscript> is a frequency factor
    well-approximated by the Wilson-Hilferty transformation</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.12)FUNC { kappa SUB y ` ( ` g ` ) ~ = ~ LEFT LBRACE ~ STACK {
      ALIGNL 2 OVER g ` LEFT [ ` LEFT ( ` g OVER 6 ` ( ` z SUB y ~ - ~ g OVER
      6 ` ) ~ + ~ 1 ` RIGHT ) SUP 3 ~ - ~ 1 RIGHT ] ~ ~ if ~ LINE ` g ` LINE ~
      &gt; ~ 0 # ~ # ALIGNL 0 ~ if ~ g ~ = ~ 0 } RIGHT . }</para>
    </section>

    <para>for |g| &lt; 2 and AEPs ranging from 0.99 to 0.01. The term
    z<subscript>Y</subscript> is the frequency factor for the standard normal
    distribution which has a mean of zero and standard deviation of 1. In
    other words, z<subscript>Y</subscript> is the value of the standard normal
    deviate with exceedance probability 1/Y. Listed in Table 1.1.<link
    linkend="WPGeneratedID_TOC_1_18">2</link> are the values of
    z<subscript>Y</subscript> for selected exceedance probabilities.
    Comprehensive tables of KY can be found in Pilgrim (1987, p.208).</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FreqFacTable_1"/>
    </para>

    <section label="paragraph" role="textbox">
      <para>Table 1.1.2 - - Frequency factors for standard normal
      distribution</para>

      <table>
        <title>TABLE A</title>

        <tgroup cols="2">
          <colspec align="left" colname="Column1" colwidth="1.574in"/>

          <colspec align="left" colname="Column2" colwidth="1.181in"/>

          <tbody>
            <row>
              <entry align="center" valign="center">
                <para>Y in AEP of 1 in Y</para>
              </entry>

              <entry align="center" valign="center">
                <para>ZY</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>2</para>
              </entry>

              <entry align="center" valign="center">
                <para>0</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>5</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.8416</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>10</para>
              </entry>

              <entry align="center" valign="center">
                <para>1.2816</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>20</para>
              </entry>

              <entry align="center" valign="center">
                <para>1.6449</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>50</para>
              </entry>

              <entry align="center" valign="center">
                <para>2.0537</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>100</para>
              </entry>

              <entry align="center" valign="center">
                <para>2.3263</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>200</para>
              </entry>

              <entry align="center" valign="center">
                <para>2.5758</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>500</para>
              </entry>

              <entry align="center" valign="center">
                <para>2.8782</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>1000</para>
              </entry>

              <entry align="center" valign="center">
                <para>3.0902</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <para/>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="lowerroman">
      <listitem>
        <para>
          <emphasis role="bold"/>

          <phrase id="WPGeneratedID_TOC_1_18">Generalized Pareto
          Distribution</phrase>
        </para>
      </listitem>
    </orderedlist>

    <para/>

    <para>In POT modelling the generalized Pareto (GP) distribution is often
    found to satisfactorily fit the data. Listed in Table 1.1.<link
    linkend="WPGeneratedID_TOC_1_17">1</link> lists the pdf p ( q | θ),
    distribution function P ( Q ≤ q | θ) and product moments for the GP
    distribution. This distribution has three parameters:</para>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>q<subscript>*</subscript> - the location parameter;</para>
      </listitem>

      <listitem>
        <para>β - the scale parameter; and</para>
      </listitem>

      <listitem>
        <para>κ - the shape parameter.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <para>When κ equals zero, the distribution simplifies to the exponential
    distribution which is summarized in Table 1.1.<link
    linkend="WPGeneratedID_TOC_1_17">1</link>.</para>

    <para/>

    <para>The GP distribution has an intimate relationship with the GEV. If
    the GP describes the distribution of peaks over a threshold, then for
    Poisson arrivals of the peaks with ν being the average number of arrivals
    per year, it can be shown that the distribution of annual maximum peaks is
    GEV with shape parameter ,κ scale parameter α = βν<superscript/>-κ and
    location parameter</para>

    <section label="inline" role="textbox">
      <para>FUNC { tau ~ = ~ q SUB * ~ + ~ beta OVER kappa }</para>
    </section>

    <para>(see Stedinger <emphasis role="italic">et al</emphasis>., 1993,
    p18.39 for more details). Likewise when peaks over a threshold follow an
    exponential distribution then for the Poisson arrival the distribution of
    annual maxima follows a Gumbel distribution.</para>

    <para/>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="lowerroman">
      <listitem>
        <para><emphasis role="bold"/>Threshold Mixture Model</para>
      </listitem>
    </orderedlist>

    <para/>

    <para>In certain parts of Australia annual maximum flood series may
    contain one or more years of zero or negligible flow. This causes a
    lumpiness in the flood probability model which is best handled by
    generalizing the pdf to the threshold mixture form</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.13)FUNC { MATRIX { p ` ( ` q ` LINE ` theta ` , ` M ` ) ~
      &amp; = ~ P ` ( ` q SUB * ` ) ` delta ` ( ` q ~ - ~ q SUB * ` ) ~ + ~
      LEFT [ ` 1 ~ - ~ P ` ( ` q SUB * ` ) ` RIGHT ] ` p ` ( ` q ` LINE ` q ~
      &gt; ~ q SUB * ` , ` theta ` , ` M ` ) # ~ &amp; = ~ P ` ( ` q SUB * ` )
      ` delta ` ( ` q ~ - ~ q SUB * ` ) ~ + ~ LEFT [ ` 1 ~ - ~ P ` ( ` q SUB *
      ` ) ` RIGHT ] ` { p ` ( ` q ` LINE ` theta ` , ` M ` ) ` I ` ( ` q ` , `
      q SUB * ` ) } OVER { P ` ( ` q ~ &gt; ~ q SUB * ` LINE ` theta ` , ` M `
      ) } } }</para>
    </section>

    <para>where P(q<subscript>*</subscript>) is the probability of a flow less
    than or equal to q<subscript>*</subscript>, the threshold value and p( q |
    q &gt; q<subscript>*</subscript>, θ, M) is the conditional pdf of a flood
    peak exceeding the threshold q<subscript>*</subscript>.</para>

    <para/>

    <para>The mixture model is interpreted as follows:</para>

    <para/>

    <para><emphasis role="italic"/>There is a probability
    P(q<subscript>*</subscript>) of the annual maximum flow being less than or
    equal to the threshold q<subscript>*</subscript> in any year. If the
    annual maximum flood exceeds the threshold its value is randomly sampled
    from the conditional pdf p(q | q &gt; q<subscript>*</subscript>, θ,
    M).</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" numeration="lowerroman">
              <listitem>
                <para><emphasis role="bold"/>Multi-component or Mixture
                Models</para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>In some areas, flooding may be affected by different types of
    meteorological events (for example, intense tropical cyclones and storms
    characterized by more usual synoptic conditions) or by changing hydraulic
    controls (e.g., see Example <link
    linkend="WPGeneratedID_TOC_1_35">1</link>), causing abnormal slope changes
    in the frequency curve. In such cases the GEV or LP3 families may not
    adequately fit the data. This type of problem may be of particular
    importance in northern Australia, and an example is described by Ashkanasy
    and Weeks (1975).</para>

    <para/>

    <para>Where components affecting flooding have distinctly different
    distributions one can separate the data by cause and analyse each set
    separately using simple distributions. Ashkanasy and Weeks (1975) used a
    combination of two log Normal distributions. Waylen and Woo (1982)
    considered separately summer runoff and winter snowmelt floods. They
    showed that separation of the flood record into separate series can result
    in a better estimate of the probability of extreme events because the data
    that describes phenomena that produce those large events is better
    represented in the analysis.</para>

    <para/>

    <para>A two-component extreme value (TCEV) distribution has been described
    by Rossi <emphasis role="italic">et al</emphasis>. (1984) and Fiorentino
    <emphasis role="italic">et al</emphasis>. (1985), and developed and
    applied to U.K. data by Beran <emphasis role="italic">et al</emphasis>.
    (1986). It corresponds to the maximum of two independent Gumbel
    distributions. Generally, one of the two distributions is thought of as
    describing the bulk of the data, and the other as the outlier
    distribution. Because the TCEV has four parameters, it is very flexible
    (Beran <emphasis role="italic">et al</emphasis>., 1986). However, if only
    annual maximum series are available, regional estimation methods must be
    used to help estimate the parameters.</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" numeration="lowerroman">
              <listitem>
                <para>
                  <emphasis role="bold"/>

                  <phrase id="WPGeneratedID_TOC_1_19">Non-Homogeneous
                  Models</phrase>
                </para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>If the evidence suggests that flood risk is affected by
    multi-decadal climate persistence, the use of non-homogeneous probability
    models may need to be investigated. The concern is that ignoring the
    non-homogeneity of the flood record may lead to biased estimates of
    long-term flood risk.</para>

    <para/>

    <para>If a non-homogeneous probability model with pdf p(q | θ, x, M) is
    identified this cannot be used to estimate long-term or marginal flood
    risk. This is because the flood risk is dependent on the exogenous
    climate-related variables x. To estimate long-term flood risk the
    dependence on x must be removed using total probability to give</para>

    <para>
      <anchor id="WPGeneratedID_Xref_excprobeqn14_1"/>
    </para>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.14)FUNC { P ` ( ` Q ~ &lt;= ~ q ` LINE ` theta ` , ` M ` ) ~
      = ~ INT SUB INF ` LEFT ( INT FROM q TO 0 ~ p ` LEFT ( ` z ` LINE ` theta
      ` , ` x ` , ` M ` RIGHT ) ` d ` z ` RIGHT ) ` p ` ( ` x ` ) ` d `
      x}</para>
    </section>

    <para>where p(x) is the pdf of the exogenous variables.</para>

    <para/>

    <para>If the gauged record adequately samples the distribution of x, it is
    not necessary to identify a non-homogeneous model. It suffices to fit a
    probability model to all the record to estimate the long-term flood
    risk.</para>

    <para/>

    <para>However, if the gauged record does not adequately sample the
    exogenous variable x, significant bias in long-term flood risk may result.
    In such instances, it is possible to:</para>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>Use a regional frequency method based on sites with sufficiently
        long records to fully sample x; or</para>
      </listitem>

      <listitem>
        <para>Extend the flood record using nearby sites which have more fully
        sampled the exogenous variable x.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <para>Example <link linkend="WPGeneratedID_TOC_1_49">6</link> illustrates
    the impact on flood risk arising from multi-decadal persistence in climate
    state as represented by the IPO index. It demonstrates the serious bias in
    flood risk that can arise if short records do not adequately sample
    different climate states. It also demonstrates use of equation 1.1.<link
    linkend="WPGeneratedID_TOC_1_20">14</link>.</para>

    <para/>

    <para>In the future, it is expected regional frequency methods explicitly
    dealing with non-homogeneity will be developed. This will allow combining
    regional and site information (see Example <link
    linkend="WPGeneratedID_TOC_1_44">4</link>) to yield the most accurate
    frequency analysis.</para>

    <para/>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="loweralpha">
      <listitem>
        <para>
          <emphasis role="bold"/>

          <phrase id="WPGeneratedID_TOC_1_20">Choice of Distribution for POT
          Series</phrase>
        </para>
      </listitem>
    </orderedlist>

    <para/>

    <para>In some cases, it may be desirable to analytically fit a probability
    distribution to POT data. Distributions that have been used to describe
    the flood peak above a threshold include exponential, GP and LP3.</para>

    <para/>

    <orderedlist continuation="continues" inheritnum="inherit"
                 numeration="arabic">
      <listitem>
        <para>
          <emphasis role="bold"/>

          <phrase id="WPGeneratedID_TOC_1_21">CHOICE OF FLOOD
          ESTIMATOR</phrase>
        </para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" inheritnum="inherit"
                 numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" inheritnum="inherit"
                     numeration="loweralpha">
          <listitem>
            <para><emphasis role="bold"/>Introduction</para>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>If the true value of θ were known, then the pdf p(q | θ, M) could be
    used to compute the flood quantile q<subscript>Y</subscript>(θ, M). For
    annual maximum series the quantile for a 1 in Y AEP is defined as</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.15)FUNC { P ` ( ` q ~ &gt; ~ q SUB Y ` LINE ` theta ` , ` M `
      ) ~ = ~ 1 OVER Y ~ = ~ INT FROM { q SUB Y } TO INF ~ p ` ( ` q ` LINE `
      theta ` , ` M ` ) ` d ` q }</para>
    </section>

    <para>However, in practice the true value of θ (as well as the
    distribution family M) is unknown. All that is known about θ, given the
    data D, is summarized by a probability distribution with pdf p(θ | D, M).
    This distribution may be the sampling distribution from a bootstrap
    analysis or the posterior distribution from a Bayesian analysis. Note that
    if an informative prior distribution is employed, the Bayesian analysis
    will produce, in practice, different results to a bootstrap
    analysis.</para>

    <para/>

    <para>This uncertainty in the flood quantile should be considered when
    making predictions, particularly when extrapolating to floods beyond the
    observed record. There are a number of ways the uncertainty in θ can be
    treated leading to different flood quantile estimators. The following
    sections consider two estimators, expected parameter quantiles and
    expected probability quantiles.</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <para><emphasis role="bold"/>Expected Parameter Quantiles</para>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>In general the estimation of a design flood should be guided by the
    consequences of over- and under-design (Slack <emphasis role="italic">et
    al</emphasis>., 1975). In the presence of parameter uncertainty, an
    expected loss approach can be employed.</para>

    <para/>

    <para>Define the loss function</para>

    <section label="inline" role="textbox">
      <para>FUNC { L ` LEFT [ ` q HAT ` ( ` D ` ) ` , ` q ` ( ` θ ` ) ` RIGHT
      ] }</para>
    </section>

    <para>as the loss incurred when the true quantile q(θ), which depends on θ
    is replaced by the quantile estimate</para>

    <section label="inline" role="textbox">
      <para>FUNC { ` q HAT ` ( ` D ` ) ` }</para>
    </section>

    <para>, which depends on the data D. Since the data do not exactly specify
    θ, the loss itself is not exactly known.</para>

    <para/>

    <para>One approach to find the best estimate is to seek the
    estimate</para>

    <section label="inline" role="textbox">
      <para>FUNC { ` q HAT ` ( ` D ` ) SUB {opt } ` }</para>
    </section>

    <para>which minimizes the expected loss defined by</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEq16_1"/>
    </para>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.16)FUNC { q HAT ` ( ` D ` ) SUB {opt } ~ ← ~ min ` INT SUB
      theta ` L ` LEFT [ ` q HAT ( ` D ` ) ` , ` q ` ( ` theta ` ) ` RIGHT ] `
      p ` ( ` theta ` LINE ` D ` , ` M ` ) ` d ` theta }</para>
    </section>

    <para>If the loss function is quadratic</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.17)FUNC { L LEFT [ ` q HAT ` ( ` D ` ) ` , ` q ` ( ` theta `
      ) ` RIGHT ] ~ = ~ alpha ` LEFT [ ` q HAT ` ( ` D ` ) ~ - ~ q ` ( ` theta
      ` ) ` RIGHT ] SUP { ` 2 } }</para>
    </section>

    <para>where α is a constant, the consequences of under- and over-design
    are assumed equal. DeGroot (1970) shows that for this loss function,
    termed a mean-squared-error (MSE) loss function, the optimal estimate is
    the expected value of q(D).</para>

    <para/>

    <para>It follows that the optimal MSE quantile estimate is the expected
    value of the quantile defined as</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.18)FUNC { E LEFT [ ` q SUB Y ` LINE ` D ` , ` M ` RIGHT ] ~ =
      ~ INT SUB theta ` q SUB Y ` ( ` theta ` , ` M ` ) ` p ` ( ` theta ` LINE
      ` D ` , ` M ` ) ` d ` theta }</para>
    </section>

    <para>Using a first-order Taylor series approximation yields</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEq19_1"/>
    </para>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.19)FUNC { E LEFT [ ` q SUB Y ` LINE ` D ` , ` M ` RIGHT ] ~ =
      ~ q SUB Y ` LEFT [ ` E ` ( ` theta ` LINE ` D ` , ` M ` RIGHT ] }</para>
    </section>

    <para>where E( θ ∣ D, M) is the expected parameter given the data D and
    model family M.</para>

    <para/>

    <para>The term q<subscript>Y</subscript> [ E( θ ∣ D, M) ] is referred to
    as the 1 in Y AEP quantile. When this term is used it is understood that
    the expected parameter vector conditioned on the data D, E( θ ∣ D, M) ],
    is being treated as if it were the true value of θ.</para>

    <para/>

    <para>A closely related estimate to E( θ ∣ D, M) is
    θ<subscript>MP</subscript> defined as the value of θ that maximizes the
    posterior pdf p(θ ∣ D, M). Asymptotically θ<subscript>MP</subscript> and
    E( θ ∣ D, M) should converge to the same value. Martins and Stedinger
    (2000, 2001) show that when a prior distribution is used to restrict
    parameters to a statistically/physically reasonable range
    θ<subscript>MP</subscript> (or generalized maximum likelihood estimate)
    yields MSE quantile estimates for GEV distributions with negative shape
    factors which are substantially better than method of moment and L moment
    estimates.</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <para>
              <emphasis role="bold"/>

              <phrase id="WPGeneratedID_TOC_1_23">Expected Probability
              Quantiles</phrase>
            </para>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>Stedinger (1983) showed that the dependence of the flood quantile on
    uncertain parameters can be removed using total probability to yield the
    design flood distribution</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEq20_1"/>
    </para>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.20)FUNC { p ` LEFT ( ` q ` LINE ` D ` , ` M ` RIGHT ) ~ = ~
      INT SUB theta ~ p ` LEFT ( ` q ` LINE ` theta ` , ` M ` RIGHT ) ` p `
      LEFT ( ` theta ` LINE ` D ` , ` M ` RIGHT ) ~ d ` theta }</para>
    </section>

    <para>This distribution only depends on the data D and the assumed
    probability family M.</para>

    <para/>

    <para>Suppose the 1 in Y AEP quantile q<subscript>Y</subscript> is
    evaluated. Using the design flood distribution given by equation 1.1.<link
    linkend="WPGeneratedID_TOC_1_24">20</link> and changing the order of
    integration yields the AEP for q<subscript>Y</subscript> as</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEq21_1"/>
    </para>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.21)FUNC { MATRIX { P ` LEFT ( ` q ~ &gt; ~ q SUB Y ` LINE ` D
      ` , ` M ` RIGHT ) &amp; = ~ INT SUB theta ~ LEFT ( ` INT SUB { q SUB Y }
      SUP INF ~ p ` LEFT ( ` q ` LINE ` theta ` , ` M ` RIGHT ) ~ d ` q ~
      RIGHT ) ` p ` LEFT ( ` theta ` LINE ` D ` , ` M ` RIGHT ) ~ d ` theta #
      ~ &amp; = ~ INT SUB theta ~ p ` LEFT ( ` q ~ &gt; ~ q SUB Y ` LINE `
      theta ` , ` M ` RIGHT ) ` p ` LEFT ( ` theta ` LINE ` D ` , ` M ` RIGHT
      ) ~ d ` theta } }</para>
    </section>

    <para>Demonstrated in Equation 1.1.<link
    linkend="WPGeneratedID_TOC_1_25">21</link> is that the term P (q &gt;
    q<subscript>Y</subscript> | D, M) is an expected probability. For this
    reason, the design flood distribution also is called the expected
    probability flood distribution.</para>

    <para/>

    <para>In general the presence of parameter uncertainty results in P (q
    &gt; q<subscript>Y</subscript> | D, M) exceeding 1/Y;that is, the
    magnitude of the 1 in Y AEP quantile is expected to be exceeded more often
    than 1 in Y years.</para>

    <para/>

    <para>The term P (q &gt; q<subscript>Y</subscript> | D, M) is called the
    expected probability for quantile q<subscript>Y</subscript>.</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <para>
              <emphasis role="bold"/>

              <phrase id="WPGeneratedID_TOC_1_25">Selection of Flood Quantile
              Estimator</phrase>
            </para>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>Ideally the flood estimator should be derived by minimizing a loss
    criterion such as the expected loss given by equation 1.1.<link
    linkend="WPGeneratedID_TOC_1_22">16</link>. This is particularly important
    as the loss function is not necessarily quadratic (see Stedinger, 1997 and
    Al-Futaisi and Stedinger, 1999) thereby undermining the justification for
    use of equation 1.1.<link
    linkend="WPGeneratedID_TOC_1_23">19</link>.</para>

    <para/>

    <para>In most situations, however, it is neither practical nor warranted
    to perform a formal loss analysis. In such cases the decision reduces to
    choosing between the 1 in Y AEP quantile or the expected probability to
    the quantile defined by equation 1.1.<link
    linkend="WPGeneratedID_TOC_1_23">19</link>.</para>

    <para>This decision is somewhat subjective and a matter of policy. The
    decision depends on whether design is being undertaken for many sites or a
    single site, and whether the risk, the actual discharge or the average
    annual damage is the primary interest. As a general guide, some, although
    not exhaustive, recommendations are:</para>

    <para/>

    <para><emphasis role="italic">Situations where expected probability should
    be considered</emphasis>:</para>

    <para/>

    <orderedlist continuation="restarts" numeration="arabic">
      <listitem>
        <para>Where design is required for many sites in a region, and the
        objective is to attain a desired average exceedance frequency over all
        sites.</para>
      </listitem>

      <listitem>
        <para>Where flood estimates are made at multiple sites with different
        record lengths, and a comparable basis is required for flood estimates
        of a given AEP.</para>
      </listitem>

      <listitem>
        <para>Design for a single site, where risk or frequency of exceedance
        is of primary importance, and it is desired that the actual frequency
        of exceedance will equal the design frequency.</para>
      </listitem>

      <listitem>
        <para>Flood estimation for flood plain management where the actual
        frequency of flooding is required to equal the design frequency, as in
        point 3.</para>
      </listitem>

      <listitem>
        <para>Design for sites where underestimated frequency of surcharging
        carries large penalties, such as would occur with too-frequent closure
        of a railway or major road, or overtopping of a levee.</para>
      </listitem>

      <listitem>
        <para>The investigation of appropriate levels of design frequency for
        particular sites or structures, and the development of generalized
        data-based design standards.</para>
      </listitem>

      <listitem>
        <para>Split sample testing of flood estimation procedures (Beard,
        1960).</para>
      </listitem>
    </orderedlist>

    <para/>

    <para><emphasis role="italic">Situations where neglect of expected
    probability should be considered</emphasis>:</para>

    <para/>

    <orderedlist continuation="restarts" numeration="arabic">
      <listitem>
        <para>Where sizing of a structure is of primary interest rather than
        actual frequency of flooding, such as some bridges, culverts or
        drainage pipes where overdesign may have large cost penalties relative
        to the benefits.</para>
      </listitem>

      <listitem>
        <para>Where design is carried out to an arbitrary standard which has
        been judged to be satisfactory on the basis of experience from
        previous practice. Many of the design standards in Book III fall into
        this category.</para>
      </listitem>

      <listitem>
        <para>Where unbiased estimates of annual flood damages are required
        (Doran and Irish, 1980).</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="restarts" numeration="arabic">
      <listitem>
        <para><emphasis role="bold"/>FITTING FLOOD PROBABILITY MODELS TO
        ANNUAL SERIES</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" inheritnum="inherit"
                 numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" inheritnum="inherit"
                     numeration="loweralpha">
          <listitem>
            <para><emphasis role="bold"/>Overview of Methods</para>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>Fitting a flood probability model involves three steps:</para>

    <para/>

    <orderedlist continuation="restarts" numeration="arabic">
      <listitem>
        <para>Calibrating the model to available data D to determine the
        parameter values consistent with the data D;</para>
      </listitem>

      <listitem>
        <para>Estimation of confidence limits, flood quantiles and expected
        probability floods; and</para>
      </listitem>

      <listitem>
        <para>Evaluation of goodness of fit and consistency of model with
        data.</para>
      </listitem>
    </orderedlist>

    <para/>

    <para>Two calibration approaches are described herein. The recommended
    approach uses Bayesian methods on account of their ability to handle
    gauged and censored data, errors in data and regional information. The
    second approach based on L moments is included because of its intrinsic
    value in regionalisation studies and usefulness in dealing with
    difficult-to-fit data. For each approach the methods are documented and
    illustrated with worked examples.</para>

    <para/>

    <para>It is worthwhile noting that implementation of the methods within
    software packages requires specialist skills. Therefore, for typical
    applications, users are advised to refer to relevant practice notes
    (published as Addenda to Australian Rainfall and Runoff) for discussions
    of available software.</para>

    <para/>

    <para>Other methods may be used for calibration provided they make
    efficient use of information. The use of the method of product-moments
    with the LP3 distribution is not recommended because more efficient
    approaches are available.</para>

    <para/>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="loweralpha">
      <listitem>
        <para><emphasis role="bold"/>Probability Plots</para>
      </listitem>
    </orderedlist>

    <para/>

    <para>An essential part of a flood frequency analysis is the construction
    of an empirical distribution function, better known as a probability plot.
    In such a plot an estimate of AEP is plotted against the observed
    discharge. This enables one to draw a smooth curve as an empirical
    probability distribution or to visually check the adequacy of a fitted
    distribution.</para>

    <para/>

    <para>The following steps describe the production of a probability plot
    for gauged annual maximum floods:</para>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>Rank the gauged flows in descending order (that is, from largest
        to smallest) yielding the series {q<subscript>(1)</subscript>,
        q<subscript>(2)</subscript>,…,q<subscript>(n)</subscript>} where
        q<subscript>(i)</subscript> is the rank i or the ith largest
        flood;</para>
      </listitem>

      <listitem>
        <para>Estimate the AEP for each q<subscript>(i)</subscript> - this
        estimate is referred to as a plotting position; and</para>
      </listitem>

      <listitem>
        <para>Using suitable scales plot the estimated AEP against
        q<subscript>(i)</subscript>.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <para>For analysis of the annual flood series, a general formula (Blom,
    1958) for estimating the AEP of an observed flood is</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.22)FUNC { P SUB { ( ` i ` ) } ~ = ~ { i ~ - ~ alpha } OVER {
      n ~ + ~ 1 ~ - ~ 2 ` alpha } }</para>
    </section>

    <para>where i is the rank of the gauged flood, n is the number of years of
    gauged floods and α is a constant whose value is selected to preserve
    desirable statistical properties.</para>

    <para/>

    <para>There are several choices for α:</para>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>α = 0 yields the Weibull plotting position which produces
        unbiased estimates of the AEP of q(i);</para>
      </listitem>

      <listitem>
        <para>α = 0.375 yields the Blom's plotting position which produces
        unbiased quantile estimates for the normal distribution; and</para>
      </listitem>

      <listitem>
        <para>α = 0.4 yields the Cunnane's (1978) plotting position which
        produces nearly unbiased quantile estimates for a range of probability
        families.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <para>While there are arguments in favour of plotting positions that yield
    unbiased AEPs, usage has favoured plotting positions that yield unbiased
    quantiles. To maintain consistency it is recommended that the Cunnane
    plotting position be used, namely</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.23)FUNC { P SUB { ( ` i ` ) } ~ = ~ { i ~ - ~ 0.4 } OVER { n
      ~ + ~ 0.2 } }</para>
    </section>

    <para>A more complete discussion on plotting positions can be found in
    Stedinger <emphasis role="italic">et al</emphasis>. (1993).</para>

    <para/>

    <para>It is stressed that plotting positions should not be used as an
    estimate of the actual AEP or ARI of an observed flood discharge. Such
    estimates should be obtained from the fitted distribution.</para>

    <para/>

    <para>Judicious choice of scale for the probability plot can assist the
    evaluation of goodness of fit. The basic idea is to select a scale so that
    the data plot as a straight line if the data are consistent with the
    assumed probability model.</para>

    <para/>

    <para>This is best illustrated by an example. Suppose it is assumed that
    floods follow an exponential distribution. From Table 1.1.<link
    linkend="WPGeneratedID_TOC_1_17">1</link> the distribution function
    is</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.24)FUNC { P ` ( ` Q ~ &lt;= ~ q ` ) ~ = ~ 1 ~ - ~ exp ` LEFT
      ( ` - ` { q ~ - ~ q SUB * } OVER beta ` RIGHT ) }</para>
    </section>

    <para>Replacing q by q<subscript>(i)</subscript> and 1 - P ( Q ≤ q ) by
    the plotting position of q<subscript>(i)</subscript> gives</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.25)FUNC { 1 ~ - ~ P SUB { ( ` i ` ) } ~ = ~ 1 ~ - ~ exp `
      LEFT ( ` - ` { q SUB { ( ` i ` ) } ~ - ~ q SUB * } OVER beta ` RIGHT )
      }</para>
    </section>

    <para>Making q<subscript>(i)</subscript> the subject of the equation
    yields</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.26)FUNC { q SUB { ( ` i ` ) } ~ = ~ q SUB * ~ - ~ beta ` log
      ` P SUB { ( ` i ` ) } }</para>
    </section>

    <para>If q<subscript>(i)</subscript> is plotted against
    logP<subscript>(i)</subscript>, the data will plot approximately as a
    straight line if they are consistent with the exponential
    distribution.</para>

    <para/>

    <para>Examples for other distributions include:</para>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>For the Gumbel distribution plot q<subscript>(i)</subscript>
        against -log[-log(1-P<subscript>(i)</subscript>)]. Data following a
        GEV distribution will plot as a curved line.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>For the log normal distribution plot log
        q<subscript>(i)</subscript> against the standard normal deviate with
        exceedance probability P<subscript>(i)</subscript>. Data following a
        LP3 distribution will plot as a curved line.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <para>When visually evaluating the goodness of fit care needs to exercised
    in judging the significance of departures from the assumed distribution.
    Plotting positions are not scattered about the true distribution
    independently of each other. As a result, plotting positions exhibit a
    wave-like behaviour about the true distribution. To guard against
    misinterpreting such behaviour it is recommended that statistical tests be
    used to assist goodness-of-fit assessment. Stedinger <emphasis
    role="italic">et al</emphasis>. (1993) discuss the use of the
    Kolmogorov-Smirnov test, the Filiben probability plot correlation test and
    L moment diagrams and ratio tests.</para>

    <para/>

    <para>The estimation of plotting positions for censored and historic data
    is more involved and in some cases can be inaccurate; see Stedinger
    <emphasis role="italic">et al</emphasis>. (1993, p 18.41) for more details
    of plotting positions for these data types.</para>

    <para/>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="loweralpha">
      <listitem>
        <para>
          <emphasis role="bold"/>

          <phrase id="WPGeneratedID_TOC_1_26">Bayesian Calibration</phrase>
        </para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" inheritnum="inherit"
                 numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" inheritnum="inherit"
                     numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" inheritnum="inherit"
                         numeration="lowerroman">
              <listitem>
                <para><emphasis role="bold"/>Overview</para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>The Bayesian approach is a very general approach for calibrating and
    identifying models. Stedinger <emphasis role="italic">et al</emphasis>.
    (1993, p18.8) observes that "the Bayesian approach .. allows the explicit
    modeling of uncertainty in parameters and provides a theoretically
    consistent framework for integrating systematic flow records with regional
    and other hydrologic information". However, it is only with the advent of
    new computational methods that Bayesian methods can be routinely applied
    to flood frequency applications.</para>

    <para/>

    <para>The core of the Bayesian approach is described herein; Lee (1989)
    and Gelman <emphasis role="italic">et al</emphasis>., 1995) present
    general expositions. Consider a set of data D hypothesized to be a random
    realization from the probability model M with pdf p (D | θ, M) where θ is
    an unknown finite-dimensioned parameter vector. The pdf p (D | θ, M) is
    given two labels depending on the context. When p (D | θ,M) is used to
    describe the probability model generating the sample data D for a given θ,
    it is called the sampling distribution. However, when inference about the
    parameter θ is sought, p (D | θ, M) is called the likelihood function to
    emphasize that the data D is known and the parameter θ is the object of
    attention. The same notation for the sampling distribution and likelihood
    function is used to emphasize its oneness.</para>

    <para/>

    <para>In Bayesian inference the parameter vector θ is considered to be a
    random vector whose probability distribution describes what is known about
    the true value of θ. Prior to analyzing the data D knowledge about θ given
    the probability model M is summarized by the pdf p (θ | M). This density,
    referred to as the prior density, can incorporate subjective belief about
    θ.</para>

    <para/>

    <para>Bayes theorem is used to process the information contained in the
    data D by updating what is known about the true value of θ as
    follows:</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.27)FUNC { p ` ( ` theta ` LINE ` D ` , ` M ` ) ~ = ~ { p ` (
      ` D ` LINE ` theta ` , ` M ` ) ~ p ` ( ` theta ` LINE ` M ` ) } OVER { p
      ` ( ` D ` LINE ` M ` ) } }</para>
    </section>

    <para>The posterior density p (θ | D, M) describes what is known about the
    true value of θ given the data D and the model M. The denominator p (D |
    M) is the marginal likelihood defined as</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.28)FUNC { p ` ( ` D ` LINE ` M ` ) ~ = ~ INT ~ p ` ( ` D `
      LINE ` theta ` , ` M ` ) ~ p ` ( ` theta ` LINE ` M ` ) ~ d ` theta
      }</para>
    </section>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" numeration="lowerroman">
              <listitem>
                <para><emphasis role="bold"/>Likelihood Function</para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>The key to a Bayesian analysis is the formulation of the likelihood
    function. In the context of flood frequency analysis two formulations are
    considered. The first assumes there is no error in the flood data. The
    focus is on the contribution to the likelihood function made by gauged and
    censored data. The second case then generalizes the likelihood function to
    allow for error-in-discharge estimates.</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" numeration="lowerroman">
              <listitem>
                <para><emphasis role="bold"/>Likelihood Function:
                No-Error-Discharge Case</para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para/>

    <para>Suppose the following data are available:</para>

    <para/>

    <orderedlist continuation="restarts" numeration="arabic">
      <listitem>
        <para>A gauged record of n true flood peaks
        {q<subscript>1</subscript>,...,q<subscript>n</subscript>}; and</para>
      </listitem>

      <listitem>
        <para>m censored records in which a<subscript>i</subscript> annual
        flood peaks in (a<subscript>i</subscript> + b<subscript>i</subscript>)
        ungauged years exceeded a threshold with true discharge
        Q<subscript>i</subscript>, i=1,..,m.</para>
      </listitem>
    </orderedlist>

    <para/>

    <para>Denote the flood data as D = {q<subscript>i</subscript>, i=1,..,n;
    (a<subscript>i</subscript>, b<subscript>i</subscript>,
    Q<subscript>i</subscript>), i = 1, .. ,m}. The likelihood function is the
    joint pdf of D given the parameter vector θ. Noting the statistical
    independence of flood peaks it follows that the likelihood function for
    the gauged data is the joint pdf of the n gauged floods (Stedinger and
    Cohn, 1986); that is:</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.29)FUNC { p ` ( ` q SUB 1 ` , ` DOTSAXIS ` , ` q SUB n ` LINE
      ` theta ` , ` M ` ) ~ = ~ PI FROM { i ` = ` 1 } TO n ~ p ` ( ` q SUB i `
      LINE ` theta ` , ` M ` ) }</para>
    </section>

    <para>The probability of observing exactly k exceedances in n years is
    given by the binomial distribution</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.30)FUNC { p ` ( ` k ` LINE ` n ` , ` pi ` ) ~ = ~ \ SUP n C
      SUB k ` ( ` 1 ~ - ~ pi ` ) SUP { n ` - ` k } ` pi SUP k }</para>
    </section>

    <para>where π is the probability of an exceedance. Hence the likelihood of
    the censored data becomes</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.31)FUNC { p ~ ( ` censored ` data ~ LINE ~ theta ` , ` M ` )
      } # ~ #</para>

      <para>FUNC { = ~ PI FROM { i ` = ` 1 } TO m \ SUP { a SUB i ` + ` b SUB
      i } ` C SUB a SUB i ` LEFT [ ` 1 ~ - ~ P ` ( ` Q ~ &lt;= ~ Q SUB i ~
      LINE ~ theta ` , ` M ` ) ` RIGHT ] SUP { ` a SUB i } ~ P ` ( ` Q ~ &lt;=
      ~ Q SUB i ~ LINE ~ theta ` , ` M ` ) SUP { ` b SUB i } } # ~ #</para>

      <para>FUNC { = ~ PI FROM { i ` = ` 1 } TO m ~ P ` ( ` a SUB i ` , ` b
      SUB i ~ LINE ~ Q SUB i ` , ` theta ` , ` M ` ) }</para>
    </section>

    <para>where P (a<subscript>i</subscript>, b<subscript>i</subscript> |
    Q<subscript>i</subscript>, θ, M) is the binomial probability of observing
    exactly a<subscript>i</subscript> exceedances above the threshold
    discharge Q<subscript>i</subscript> in (a<subscript>i</subscript> +
    b<subscript>i</subscript>) years.</para>

    <para/>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="lowerroman">
      <listitem>
        <para><emphasis role="bold"/>The Likelihood Function:
        Error-in-Discharge Case</para>
      </listitem>
    </orderedlist>

    <para/>

    <para>The incorporation of rating errors into the likelihood function
    complicates matters. A simple model based on the rudimentary concepts
    presented in Section <xref
    linkend="WPGeneratedID_Xref_RatingErrorSection_1"/> of this Chapter in
    Book 1 of Australian Rainfall and Runoff is presented. Presented in Figure
    1.1.<link linkend="WPGeneratedID_TOC_1_27">7</link> is a rating error
    space diagram. In zone 1, or the interpolation zone, it is assumed that
    the rating error multiplier e<subscript>1</subscript> equals 1; in other
    words, errors within the rated part of the rating curve are deemed
    negligible. As a result the estimated flow w equals the true flow q.
    However, in zone 2 or the extension zone, the rating error multiplier
    e<subscript>2</subscript> is assumed to be a random variable with mean of
    1. The anchor point (q<subscript>1</subscript>,w<subscript>1</subscript>)
    separates the interpolation and extension zones.</para>

    <para>
      <anchor id="WPGeneratedID_Xref_RatingErrorFig2_1"/>

      <graphic align="center" depth="4.955in"
               fileref="ARRFFA/WPGeneratedName5.JPG" format="JPG" valign="top"
               width="4.723in"/>
    </para>

    <para>Figure 1.1.7 - Rating error multiplier space diagram for rating
    curve shown in Figure 1.1.<link
    linkend="WPGeneratedID_TOC_1_13">4</link></para>

    <para>The rating error model can represented mathematically as</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.32)FUNC { w ~ = ~ LEFT LBRACE ~ MATRIX { q &amp; ~ &amp; if ~
      q ~ &lt;= ~ q SUB 1 # w SUB 1 ~ + ~ e SUB 2 ` ( ` q ~ - ~ q SUB 1 ` )
      &amp; ~ &amp; if ~ q ~ &gt; ~ q SUB 1 } RIGHT . }</para>
    </section>

    <para>The rating error multiplier e<subscript>2</subscript> is sampled
    only once at the time of extending the rating curve. Therefore, all flood
    discharge estimates exceeding the anchor value of
    q<subscript>1</subscript> (which equals w<subscript>1</subscript>) are
    corrupted by the same rating error multiplier. It must be stressed that
    the error e<subscript>2</subscript> is not known; at best, only the
    probability distribution of e<subscript>2</subscript> can be estimated.
    For practical applications it can be assumed that
    e<subscript>2</subscript> is distributed as either a log-normal or normal
    distribution with a mean of 1 and a standard deviation of
    σ<subscript>2</subscript>.</para>

    <para/>

    <para>Data are assigned to each of the two zones (I = 1, 2) in the rating
    error space diagram. The rating error multiplier standard deviation for
    the extension zone σ<subscript>2</subscript> is assigned a value with
    σ<subscript>1</subscript> = 0. There are n<subscript>i</subscript> annual
    flood peak estimates w<subscript>ji</subscript> satisfying the zone
    constraint w<subscript>i-1</subscript> ≤ w<subscript>ji</subscript> &lt;
    w<subscript>i</subscript>, j = 1, .., n<subscript>i</subscript> where
    w<subscript>0</subscript> = 0 and w<subscript>2</subscript> = ∞. In
    addition, there are m<subscript>i</subscript> threshold discharge
    estimates W<subscript>ji</subscript> for which there are
    a<subscript>ji</subscript> exceedances in (a<subscript>ji</subscript>
    b<subscript>ji</subscript>) years, j=1, .., m<subscript>i</subscript>.
    Collectively this data is represented as</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>FUNC { D ~ = ~ LEFT LBRACE ` D SUB i ` , ` i ~ = ~ 1 ` , ` 2 `
      RIGHT RBRACE ~ = } # ~ # FUNC { LEFT LBRACE ` LEFT [ ` w SUB { j ` i } `
      , ~ j ~ = ~ 1 ` , ~ DOTSLOW ` , ~ n SUB i ~ ; W SUB { j ` i } ` , ~ a
      SUB { j ` i } ` , ~ b SUB { j ` i } ` , ~ j ~ = ~ 1 ` , ~ DOTSLOW ` , ~
      m SUB i ` RIGHT ] ` , ~ i ~ = ~ 1 ` , ~ 2 ` RIGHT RBRACE }</para>
    </section>

    <para>Following Kuczera (1999) it can be shown for the two-zone rating
    error model of Figure 1.1.<link linkend="WPGeneratedID_TOC_1_27">7</link>
    that the likelihood reduces to</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.33)FUNC { p ` LEFT ( ` D SUB 1 ` , ~ D SUB 2 ~ LINE ~ theta `
      , ~ sigma SUB 2 ` , ~ M ` RIGHT ) ~ = ~ p ` LEFT ( ` D SUB 1 ` , ~ e SUB
      1 ~ = ~ 1 ~ LINE ~ theta ` , ~ M ` RIGHT ) } # ~ # FUNC { LEFT [ ~ INT
      FROM 0 TO INF ~ p ` LEFT ( ` D SUB 2 ` , ~ e SUB 2 ~ LINE ~ theta ` , ~
      M ` RIGHT ) ` g ` LEFT ( ` e SUB 2 ~ LINE ~ sigma SUB 2 ` RIGHT ) ` d `
      e SUB 2 ` RIGHT ] }</para>
    </section>

    <para>where</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>FUNC { p ` LEFT ( ` D SUB i ` , ~ e SUB i ~ LINE ~ theta ` , ~ M `
      RIGHT ) ~ = ~ PI FROM { j ` = ` 1 } TO n SUB j ~ 1 OVER e SUB i ` p `
      LEFT ( ` q SUB { i ` - ` 1 } ~ + ~ { w SUB { j ` i } ~ - ~ q SUB { i ~ -
      ~ 1 } } OVER e SUB i ~ LINE ~ theta ` RIGHT ) } # ~ # FUNC { PI FROM { j
      ` = ` 1 } TO m SUB j ~ P ` LEFT [ ` a SUB { j ` i } ` , ~ b SUB { j ` i
      } ~ LINE ~ q SUB { i ` - ` 1 } ~ + ~ { w SUB { j ` i } ~ - ~ q SUB { i ~
      - ~ 1 } } OVER e SUB i `, ~ theta ` , ~ M ` RIGHT ] }</para>
    </section>

    <para>and g(e<subscript>i</subscript> | σ<subscript>i</subscript>) is the
    rating error multiplier pdf with a mean of 1 and a standard deviation of
    σ<subscript>i</subscript>. This is a complex expression which can be
    evaluated only with numerical approaches. However, it makes the fullest
    use of information on annual flood peaks and binomial-censored data in the
    presence of rating curve error. Some guidance on the choice of
    σ<subscript>2</subscript> is offered in Section 3.7 of this Chapter in
    Book 1 of Australian Rainfall and Runoff.</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" numeration="lowerroman">
              <listitem>
                <para>
                  <emphasis role="bold"/>

                  <phrase id="WPGeneratedID_TOC_1_27">Prior
                  Distributions</phrase>
                </para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>The prior pdf p(θ | M) reflects the worth of prior information on θ
    obtained preferably from a regional analysis. A convenient distribution is
    the multivariate normal with mean μ<subscript>p</subscript> and covariance
    Σ<subscript>p</subscript>; that is,</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.34)FUNC { theta ~ SIM ~ N ` ( mu SUB p ` , ~ SIGMA SUB p ` )
      }</para>
    </section>

    <para>In the absence of prior information, the covariance matrix can be
    made non-informative as illustrated in the following equation for a
    three-parameter model:</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.35)FUNC { SIGMA SUB { p SUB { M ` → ` INF } } ~ → ~ LEFT ( ~
      MATRIX { M &amp; 0 &amp; 0 # 0 &amp; M &amp; 0 # 0 &amp; 0 &amp; M } ~
      RIGHT ) }</para>
    </section>

    <para>Informative prior information can be obtained from a regional
    analysis of flood data using methods such index flood, skew maps, and
    generalized least squares regression. Importantly the reliability of the
    prior information must accompany the estimates.</para>

    <para/>

    <para>Although this Chapter has focussed on the use of at-site data, the
    importance of using well-founded prior information cannot be stressed
    strongly enough. The use of prior information on one or more parameters
    confers very significant benefits as demonstrated in Example 4 of this
    Chapter in Book 1 of Australian Rainfall and Runoff.</para>

    <para/>

    <para>Martins and Stedinger (2000, 2001) demonstrate that the method of
    generalized maximum likelihood produce results superior to L moment and
    method-of-moment estimators for the GEV. This result is attributed to the
    use of a geophysical prior on the shape parameter κ. The prior, a beta
    distribution over the range -0.5 &lt; κ &lt; 0.5 with mean -0.10 and
    standard deviation 0.122, confines κ to a range consistent with worldwide
    experience on rainfall depths and flood flows. Where possible, users are
    encouraged to make use of well-founded prior information.</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" numeration="lowerroman">
              <listitem>
                <para>
                  <emphasis role="bold"/>

                  <phrase id="WPGeneratedID_TOC_1_28">Monte Carlo Sampling
                  from the Posterior Distribution</phrase>
                </para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>The posterior pdf p(θ | D, M) fully defines the parameter
    uncertainty. However, until the last decade, interpreting this
    distribution was problematic. Modern Monte Carlo methods for sampling from
    the posterior have overcome this limitation; for example, see Gelman
    <emphasis role="italic">et al</emphasis>. (1995). The sampling procedure
    described here involves a method called importance sampling. Three steps
    are involved with these steps being:</para>

    <para/>

    <itemizedlist mark="O">
      <listitem>
        <para><emphasis role="italic"/>Find Most Probable Posterior
        Parameters</para>
      </listitem>
    </itemizedlist>

    <para/>

    <para>Any robust search method can be used to locate the value of which
    maximizes the logarithm of the posterior probability density; that
    is,</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.36)FUNC { theta HAT ~ ← ~ max ~ log ~ p ` ( ` theta ` LINE `
      D ` , ` M ` ) }</para>
    </section>

    <para>where</para>

    <section label="inline" role="textbox">
      <para>FUNC { theta HAT }</para>
    </section>

    <para>is the most probable value of θ. The shuffled complex evolution
    algorithm presented by Duan <emphasis role="italic">et al</emphasis>.
    (1992) is a recommended search method.</para>

    <para/>

    <itemizedlist mark="O">
      <listitem>
        <para><emphasis role="italic"/>Multi-normal Approximation to Posterior
        Distribution</para>
      </listitem>
    </itemizedlist>

    <para/>

    <para>Almost always, the log of posterior pdf p(θ | D, M) can be
    approximated by a second-order Taylor series expansion about the most
    probable parameter to yield the multivariate normal approximation</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.37)FUNC { theta ` LINE ` D ` , ` M ~ SIM ~ N ` LEFT ( ` theta
      HAT ` , ` SIGMA ` RIGHT ) }</para>
    </section>

    <para>where</para>

    <section label="inline" role="textbox">
      <para>FUNC { theta HAT }</para>
    </section>

    <para>is interpreted as the mean and the posterior covariance Σ is defined
    as the inverse of the Hessian</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.38)FUNC { SIGMA ~ = ~ LEFT ( ~ - ` { PARTIAL SUP 2 ~ log SUB
      e ~ p ` ( ` theta ` LINE ` D ` , ` M ` ) } OVER { PARTIAL SUP 2 ` theta
      } ` RIGHT ) SUP { ` - ` 1 } }</para>
    </section>

    <para>An adaptive difference scheme should be used to evaluate the
    Hessian. Particular care needs to be exercised when selecting finite
    difference perturbations for the GEV and LP3 distributions when upper or
    lower bounds are close to the observed data.</para>

    <itemizedlist mark="O">
      <listitem>
        <para><emphasis role="italic"/>Importance Sampling of Posterior
        Distribution</para>
      </listitem>
    </itemizedlist>

    <para/>

    <para>Importance sampling is a Monte Carlo technique (Gelman <emphasis
    role="italic">et al</emphasis>., 1995) for random sampling from a
    probability distribution. The basic idea is to randomly sample parameters
    (called particles) from a reasonable approximation to the posterior
    distribution (for example, the mult-inormal approximation obtained in step
    2) and then make a correction to the probability mass assigned to the
    particle. The algorithm proceeds as follows:</para>

    <para/>

    <orderedlist continuation="restarts" numeration="upperroman">
      <listitem>
        <para>Sample N particles according to</para>

        <para>where p<subscript>N</subscript>(θ) is the pdf of the
        multi-normal approximation in Step 2;</para>
      </listitem>

      <listitem>
        <para>Calculate particle probability weights according to</para>

        <para>; and</para>
      </listitem>

      <listitem>
        <para>Scale the particle weights so they sum to 1.</para>
      </listitem>
    </orderedlist>

    <section label="inline" role="textbox">
      <para>FUNC { theta SUB i ~ ← ~ p SUB N ` ( ` theta ` ) ` , ~ i ~ = ~ 1 `
      , ~ DOTSLOW ` , ~ N }</para>
    </section>

    <section label="inline" role="textbox">
      <para>FUNC { P ` ( ` theta SUB i ` ) ~ = ~ { p ` ( ` theta SUB i ` LINE
      ` D ` , ` M ` ) } OVER { p SUB N ` ( ` theta SUB i ` ) } ` , ~ i ~ = ~ 1
      ` , ~ DOTSLOW ` , ~ N }</para>
    </section>

    <para/>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="lowerroman">
      <listitem>
        <para><emphasis role="bold"/>Quantile Confidence Limits and Expected
        Probability</para>
      </listitem>
    </orderedlist>

    <para/>

    <para>Using the N particles produced by the importance sampling the
    posterior distribution of any function dependent on θ can be readily
    approximated.</para>

    <para/>

    <para>Confidence limits <footnote id="1">
        <para>In Bayesian statistics, the correct terminology is credible
        interval rather than confidence interval. Given the widespread usage
        of the term “confidence interval” and its popular mis-interpretation
        as a probability interval, the term “confidence interval” is retained
        in this document.</para>
      </footnote> describe the uncertainty about quantiles arising from
    uncertainty in the fitted parameters. In general, confidence limits are
    used in conjunction with a probability plot to evaluate the
    goodness-of-fit. 100(1-α)% quantile confidence limits, or more correctly
    probability limits, can be obtained as follows:</para>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>Rank in ascending order the N quantiles
        {q<subscript>Y</subscript>(θ<subscript>i</subscript>),M), i = 1, ,
        N};</para>
      </listitem>

      <listitem>
        <para>For each ranked quantile evaluate the non-exceedance
        probability</para>

        <para>where</para>

        <para>are the particle weights for the j<superscript>th</superscript>
        ranked quantile q<subscript>Y</subscript> (θ<subscript>j</subscript>,
        M);</para>
      </listitem>

      <listitem>
        <para>The lower and upper confidence limits are the quantiles whose
        non-exceedance probabilities are nearest to</para>

        <para>and</para>

        <para>respectively.</para>
      </listitem>
    </itemizedlist>

    <section label="inline" role="textbox">
      <para>FUNC { SUM FROM { j ` = ` 1 } TO N ~ P `( ` theta SUB j ` )
      }</para>
    </section>

    <section label="inline" role="textbox">
      <para>FUNC { P `( ` theta SUB j ` ) }</para>
    </section>

    <section label="inline" role="textbox">
      <para>FUNC { alpha OVER 2 }</para>
    </section>

    <section label="inline" role="textbox">
      <para>FUNC { 1~ - ~ alpha OVER 2 }</para>
    </section>

    <para/>

    <para>The expected posterior parameters can be estimated from</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.39)FUNC { E ` LEFT [ ` theta ` LINE ` D ` , ` M ` RIGHT ] ~ =
      ~ SUM FROM { i ` = ` 1 } TO N ~ P ` ( ` theta SUB i ` ) ~ q SUB Y ` ( `
      theta SUB i ` , ` M ` ) }</para>
    </section>

    <para>These parameters can then be used to compute the 1 in Y AEP
    quantiles described in Section 5.1 of this Chapter in Book 1 of Australian
    Rainfall and Runoff.</para>

    <para/>

    <para>Finally, the expected exceedance probability for a flood of
    magnitude q<subscript>Y</subscript> can be estimated</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.40)FUNC { E ` LEFT [ ` q ~ &gt; ~ q SUB Y ` LINE ` D ` , ` M
      ` RIGHT ] ~ = ~ SUM FROM { i ` = ` 1 } TO N ~ P ` ( ` theta SUB i ` ) ~
      P ( ` q ~ &gt; ~ q SUB Y ` LINE ` theta SUB i ` , ` M ` ) }</para>
    </section>

    <para/>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="lowerroman">
      <listitem>
        <para><emphasis role="bold"/>Software</para>
      </listitem>
    </orderedlist>

    <para/>

    <para>The Bayesian approach to calibrating flood probability models is
    numerically complex and is best implemented in a high level programming
    language such as FORTRAN or C. Users are referred the Australian Rainfall
    and Runoff web site <emphasis role="underline">www.arr.org.au</emphasis>
    for discussions of some available software. It should be noted that other
    software not noted on the Australian Rainfall and Runoff website may be
    available also.</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" numeration="lowerroman">
              <listitem>
                <para><emphasis role="bold"/>Treatment of Poor Fits</para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>The standard probability models described in Section 4 of this
    Chapter in Book 1 of Australian Rainfall and Runoff may sometimes poorly
    fit flood data. Typically the goodness-of-fit is assessed by comparing
    observed data against the fitted probability model and its confidence
    limits. A poor fit may be characterized by:</para>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>Presence of outliers in the upper or lower tail of the
        distribution. Outliers in flood frequency analysis represent
        observations that are inconsistent with the trend of the remaining
        data and typically would lie well outside appropriate confidence
        limits; or</para>
      </listitem>
    </itemizedlist>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>Systematic discrepancies between observed and fitted
        distributions. Caution is required in interpreting systematic
        departures on probability plots. Confidence limits can help guide the
        interpretation.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <para>Poor fits to the standard probability models may arise for a variety
    of reasons including the following:</para>

    <para/>

    <orderedlist continuation="restarts" numeration="arabic">
      <listitem>
        <para>By chance, one or more observed floods may be unusually rare for
        the length of gauged record. Recourse to the historical flood record
        may be useful in resolving this issue;</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <para>Rating curve extensions are biassed resulting in a systematic
        under or over-estimate of large floods (see discussion in Section 3.7
        of this Chapter in Book 1 of Australian Rainfall and Runoff);</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <para>A change in hydraulic control with discharge may affect the
        shape of the frequency curve as illustrated in Example 1;</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <para>Storm events responsible for significant flooding may be caused
        by different meteorological mechanisms which lead to a mixed
        population not amenable to three-parameter distributions. This may
        arise when the majority of flood-producing storms are generated by one
        meteorological mechanism and the minority by an atypical mechanism
        such as a tropical cyclone; and</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <para>Nonhomogeneity of the flood record.</para>
      </listitem>
    </orderedlist>

    <para/>

    <para>The potential causes of a poor fit need careful
    investigation.</para>

    <para/>

    <para>If it is decided the poor fit is due to inadequacy of the
    probability model, two strategies are available to deal with the
    problem:</para>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>The data responsible for the unsatisfactory fit may be given
        less weight; or</para>
      </listitem>
    </itemizedlist>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>A more flexible probability model be used to fit the
        data.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <para>Data in the upper part of the distribution is typically of more
    interest and therefore a strong case needs to be made to justify reduction
    in weight of such data. A corollary of this is that data in the lower part
    of the distribution is typically of less interest and therefore more
    amenable to weighting. Examples <emphasis role="bold">5 and 8</emphasis>
    illustrate two approaches to devaluing low flow data which are responsible
    for a poor fit to the GEV distribution.</para>

    <para/>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="lowerroman">
      <listitem>
        <para><emphasis role="bold"/>Worked Examples</para>
      </listitem>
    </orderedlist>

    <para/>

    <para>A total of 6 worked examples using Bayesian principles are presented
    herein. While these worked examples are intended to illustrate aspects of
    the preceding discussion, many users will find that the problem they are
    considering will not be identical to these worked examples. Nonetheless,
    these worked examples are important for the illustration of the concepts
    and approaches recommended herein.</para>

    <para/>

    <para>The worked examples are</para>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>Illustrated in Example <link
        linkend="WPGeneratedID_TOC_1_38">2</link> is the fitting of an LP3
        distribution to a 31-year gauged record. Although the fit is judged
        satisfactory, considerable uncertainty in the 1 in 100 AEP flood flow
        is noted.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>Example <link linkend="WPGeneratedID_TOC_1_42">3</link> is a
        continuation of Example <link
        linkend="WPGeneratedID_TOC_1_38">2</link> and illustrates the benefit
        of incorporating censored historic flood information. In the 118 years
        prior to continuous gauging only one flood exceeds the largest gauged
        flood. This information is shown to reduce substantially quantile
        uncertainty.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>Example <link linkend="WPGeneratedID_TOC_1_44">4</link> is also
        a continuation of Example . Illustrated in this example is the value
        of regional information in reducing uncertainty in parameters and
        quantiles.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>Shown in Example <link linkend="WPGeneratedID_TOC_1_46">5</link>
        is how three-parameter distributions such as the GEV and LP3 can be
        made to fit data exhibiting sigmoidal behaviour. In this example, the
        interest in fitting the higher flows so the low flows are
        de-emphasized by treating them as censored observations.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>Finally illustrated in Example <link
        linkend="WPGeneratedID_TOC_1_49">6</link> is the application of a
        non-homogeneous flood probability model conditioned on the IPO index.
        Shown in this example is how the long-term flood risk may be
        estimated.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="loweralpha">
      <listitem>
        <para><emphasis role="bold"/>L Moments Approach</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" inheritnum="inherit"
                 numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" inheritnum="inherit"
                     numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" inheritnum="inherit"
                         numeration="lowerroman">
              <listitem>
                <para><emphasis role="bold"/>Overview</para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>L moments were developed to provide estimators less sensitive to
    outliers than product moments which involve raising observations to an
    integer power. They are of particular interest in regionalization studies
    because they yield estimators of dimensionless moment ratios that are
    almost unbiased and normally distributed. (Stedinger <emphasis
    role="italic">et al</emphasis>., 1993).</para>

    <para/>

    <para>For at-site flood frequency analysis the evidence suggests L moments
    provide reasonably efficient estimators. Wang (1999) compared the
    efficiency of L moments using the GEV against product-moments using LP3.
    This comparison was undertaken using a Monte Carlo study fitting LP3 with
    product moments and GEV with L moments to data generated from LP3
    distributions fitted to 151 Australian flood records. He showed that the
    GEV L moment estimates of 50- and 100-year quantiles had mean-squared
    errors substantially lower than the LP3 product-moment estimates for most
    of the 151 distributions. However, Rosbjerg <emphasis role="italic">et
    al</emphasis>. (1992), Madsen <emphasis role="italic">et al</emphasis>.
    (1997) and Martins and Stedinger (2000, 2001) demonstrate the overall
    superiority of the method of moments over L moments when estimating GEV
    quantiles. In view of this result, the Bayesian approach is the
    recommended approach.</para>

    <para/>

    <para>Presented in this section are details of L moments because of their
    intrinsic value in regionalisation studies. Moreover, when standard
    three-parameter distributions do not adequately fit the data, a
    generalization of L moments, called LH moments, can be used to obtain a
    good fit to the right hand tail. This can be used to assist setting the
    censoring threshold as illustrated in Example 5.</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" numeration="lowerroman">
              <listitem>
                <para><emphasis role="bold"/>L-moments for Summarizing
                Distributions</para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>Hosking (1990) developed the L-moment theory based on order
    statistics. The first four L-moments are defined as</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.41) FUNC { lambda SUB 1 ~ = ~ E ` [ ` X SUB { 1 ` : ` 1 } ` ]
      } # ~ # FUNC { lambda SUB 2 ~ = ~ 1 OVER 2 ~ E ` [ ` X SUB { 2 ` : ` 2 }
      ~ - ~ X SUB { 1 ` : ` 2 } ` ] } # ~ # FUNC { lambda SUB 3 ~ = ~ 1 OVER 3
      ~ E ` [ ` X SUB { 3 ` : ` 3 } ~ - ~ 2 ` X SUB { 2 ` : ` 3 } ~ + ~ X SUB
      { 1 ` : ` 3 } ] } # ~ # FUNC { lambda SUB 4 ~ = ~ 1 OVER 4 ~ E ` [ ` X
      SUB { 4 ` : ` 4 } ~ - ~ 3 ` X SUB { 3 ` : ` 4 } ~ + ~ 3 ` X SUB { 2 ` :
      ` 4 } ~ - ~ X SUB { 1 ` : ` 4 } ` ] }</para>
    </section>

    <para>where X<subscript>j:m</subscript> is the
    j<superscript>th</superscript> smallest variable in a sample of size m and
    E stands for expectation.</para>

    <para/>

    <para>Wang (1996) justifies L-moments as follows:</para>

    <para/>

    <para>"When there is only one value in a sample, it gives a feel of the
    magnitude of the random variable. When there are two values in a sample,
    their difference gives a sense of how varied the random variable is. When
    there are three values in a sample, they give some indication on how
    asymmetric the distribution is. When there are four values in a sample,
    they give some clue on how peaky, roughly speaking, the distribution
    is."</para>

    <para/>

    <para>When many such samples are considered, the expectations
    λ<subscript>1</subscript> and λ<subscript>2</subscript> give measures of
    location and scale. Moreover, the L moment ratios</para>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.42)FUNC { tau SUB 3 ~ = ~ lambda SUB 3 OVER lambda SUB 2 } #
      ~ # FUNC { tau SUB 4 ~ = ~ lambda SUB 4 OVER lambda SUB 2 }</para>
    </section>

    <para>give measures of skewness and kurtosis respectively. Hosking termed
    τ<subscript>3</subscript> L-skewness and τ<subscript>4</subscript>
    L-kurtosis. Hosking also defined the L-coefficient of variation as</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.43)FUNC { tau SUB 2 ~ = ~ lambda SUB 2 OVER lambda SUB 1
      }</para>
    </section>

    <para>Summarized in Table 1.1.<link
    linkend="WPGeneratedID_TOC_1_29">3</link> are L moments for a range of
    distributions.</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFATable3_1"/>
    </para>

    <section label="page" role="textbox">
      <para>Table 1.1.3 - - L Moments for Several Probability Distributions
      (From Stedinger et al., 1993)</para>

      <table>
        <title>TABLE A</title>

        <tgroup cols="2">
          <colspec align="center" colname="Column1" colwidth="1.968in"/>

          <colspec align="center" colname="Column2" colwidth="4.299in"/>

          <tbody>
            <row>
              <entry valign="center">
                <para>Family</para>
              </entry>

              <entry valign="center">
                <para>L Moments</para>
              </entry>
            </row>

            <row>
              <entry valign="center">
                <para>Generalised Extreme Value (GEV)</para>
              </entry>

              <entry valign="center">
                <para>FUNC { lambda SUB 1 ~ = ~ tau ~ + ~ alpha OVER kappa `
                LEFT [ ` 1 ~ - ~ GAMMA ` ( ` 1 ~ + ~ kappa ` ) ` RIGHT ] } # ~
                # FUNC { lambda SUB 2 ~ = ~ alpha OVER kappa ` GAMMA ` ( ` 1 ~
                + ~ kappa ` ) ` LEFT [ ` 1 ~ - ~ 2 SUP { ` - ` kappa } ` RIGHT
                ] } # ~ # FUNC { tau SUB 3 ~ = ~ { 2 ` LEFT ( ` 1 ~ - ~ 3 SUP
                { ` - ` kappa } ` RIGHT )} OVER { 1 ~ - ~ 2 SUP { ` - ` kappa
                } } ~ - ~ 3 } # ~ # FUNC { tau SUB 4 ~ = ~ { 1 ~ - ~ 5 ` LEFT
                ( ` 4 SUP { ` - ` kappa } ` RIGHT ) ~ + ~ 10 ` LEFT ( ` 3 SUP
                { ` - ` kappa } ` RIGHT ) ~ - ~ 6 ` LEFT ( ` 2 SUP { ` - `
                kappa } ` RIGHT ) } OVER { 1 ~ - ~ 2 SUP { ` - ` kappa } } } ,
                κ ≠0</para>
              </entry>
            </row>

            <row>
              <entry valign="center">
                <para>Gumbel</para>
              </entry>

              <entry valign="center">
                <para>FUNC { lambda SUB 1 ~ = ~ tau ~ + ~ 0.5772 ` alpha } # ~
                # FUNC { lambda SUB 2 ~ = ~ alpha ` log SUB e ` 2 } # ~ # FUNC
                { tau SUB 3 ~ = ~ 0.1699 } # ~ # FUNC { tau SUB 4 ~ = ~ 0.1504
                }</para>
              </entry>
            </row>

            <row>
              <entry valign="center">
                <para>Generalised Pareto</para>
              </entry>

              <entry valign="center">
                <para>FUNC { lambda SUB 1 ~ = ~ q SUB * ~ + ~ beta OVER { 1 ~
                + ~ kappa } } # ~ # FUNC { lambda SUB 2 ~ = ~ beta OVER { ( `
                1 ~ + ~ kappa ` ) ` ( ` 2 ~ + ~ kappa ` ) } } # ~ # FUNC { tau
                SUB 3 ~ = ~ { 1 ~ - ~ kappa } OVER { 3 ~ + ~ kappa } } # ~ #
                FUNC { tau SUB 4 ~ = ~ { ( ` 1 ~ - ~ kappa ` ) ` ( ` 2 ~ - ~
                kappa ` ) } OVER { ( ` 3 ~ + ~ kappa ` ) ` ( ` 4 ~ + ~ kappa `
                ) } }</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" numeration="lowerroman">
              <listitem>
                <para>
                  <emphasis role="bold"/>

                  <phrase id="WPGeneratedID_TOC_1_29">L-moments Estimates for
                  Gauged Data Sample Data</phrase>
                </para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>The traditional L moment estimator is based on probability weighted
    moments. However, Wang (1996) derived the following sample estimators
    directly from the definition of the first four L-moments:</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEq44_1"/>
    </para>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.44)FUNC { lambda HAT SUB 1 ~ = ~ 1 OVER { \ SUP { n } C SUB 1
      } ` SUM FROM { i ` = ` 1 } TO n ` q SUB { ( ` i ` ) } } # ~ # FUNC {
      lambda HAT SUB 2 ~ = ~ 1 OVER 2 ~ 1 OVER { \ SUP { n ` } C SUB 2 } ` SUM
      FROM { i ` = ` 1 } TO n ` LEFT ( ` \ SUP { i ` - ` 1 } C SUB 1 ~ - ~ \
      SUP { n ` - ` i ` } C SUB 1 ` RIGHT ) ~ q SUB { ( ` i ` ) } } # ~ # FUNC
      { lambda HAT SUB 3 ~ = ~ 1 OVER 3 ~ 1 OVER { \ SUP { n ` } C SUB 3 } `
      SUM FROM { i ` = ` 1 } TO n ` LEFT ( ` \ SUP { i ` - ` 1 } C SUB 2 ~ - ~
      2 ` \ SUP { i ` - ` 1 } C SUB 1 ` \ SUP { n ` - ` i } C SUB 1 ~ + ~ \
      SUP { n ` - ` i } C SUB 2 ` RIGHT ) ~ q SUB { ( ` i ` ) } } # ~ # FUNC {
      lambda HAT SUB 4 ~ = ~ 1 OVER 4 ~ 1 OVER { \ SUP n C SUB 4 } ` SUM FROM
      { i ` = ` 1 } TO n ` LEFT ( ` \ SUP { i ` - ` 1 } C SUB 3 ~ - ~ 3 ` \
      SUP { i ` - ` 1 } C SUB 2 ` \ SUP { n ` - ` i } C SUB 1 ~ + ~ 3 ` \ SUP
      { i ` - ` 1 } C SUB 1 ` \ SUP { n ` - ` i } C SUB 2 ~ - ~ \ SUP { n ` -
      ` i } C SUB 3 ` RIGHT ) ~ q SUB { ( ` i ` ) } }</para>
    </section>

    <para>where q<subscript>(i)</subscript>, i=1, 2, .., n are gauged peak
    flows ranked in <emphasis role="underline">ascending</emphasis> order
    and</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>FUNC { \ SUP m C SUB k ~ = ~ MATRIX { { m ` ! } OVER { k ` ! ~ ( `
      m ~ - ~ k ` ) ~ ! } &amp; ~ &amp; if ~ k ~ &lt;= ~ m # 0 &amp; ~ &amp;
      if ~ k ~ &gt; ~ m } }</para>
    </section>

    <para>is the number of combinations of any k items from m items.</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" numeration="lowerroman">
              <listitem>
                <para>
                  <emphasis role="bold"/>

                  <phrase id="WPGeneratedID_TOC_1_30">Parameter and Quantile
                  Estimation</phrase>
                </para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>The method of L moments involves matching theoretical and sample L
    moments to estimate parameters. The L moments in Table 1.1.<link
    linkend="WPGeneratedID_TOC_1_29">3</link> are replaced by their sample
    estimates given by equation 1.1.<link
    linkend="WPGeneratedID_TOC_1_30">44</link>. The resulting equations are
    then solved to obtain estimates of the parameters. These parameters are
    used to calculate the 1 in Y AEP quantiles.</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" numeration="lowerroman">
              <listitem>
                <para><emphasis role="bold"/>LH-Moments for Fitting the GEV
                Distribution</para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>When the selected probability model does not adequately fit all the
    gauged data the lower flows may exert undue influence on the fit and give
    insufficient weight to the higher flows which are the principal object of
    interest. To deal with this situation Wang (1997) introduced a
    generalization of L moments called LH moments.</para>

    <para/>

    <para>LH moments are based on linear combinations of higher
    order-statistics. A shift parameter η = 0, 1, 2, 3, … is introduced to
    give more emphasis to higher ranked flows. LH moments are defined
    as:</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEqn45_1"/>
    </para>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.45) FUNC { lambda SUB 1 ~ = ~ E ` [ ` X SUB { ` eta ` + ` 1 `
      : ` eta ` + ` 1 } ` ] } # ~ # FUNC { lambda SUB 2 ~ = ~ 1 OVER 2 ~ E ` [
      ` X SUB { `eta ` + ` 2 ` : ` eta ` + ` 2 } ~ - ~ X SUB { ` eta ` + ` 1 `
      : ` eta ` + ` 2 } ` ] } # ~ # FUNC { lambda SUB 3 ~ = ~ 1 OVER 3 ~ E ` [
      ` X SUB { ` eta ` + ` 3 ` : ` eta ` + ` 3 } ~ - ~ 2 ` X SUB { ` eta ` +
      ` 2 ` : ` eta ` + ` 3 } ~ + ~ X SUB { ` eta ` + ` 1 ` : ` eta ` + ` 3 }
      ] } # ~ # FUNC { lambda SUB 4 ~ = ~ 1 OVER 4 ~ E ` [ ` X SUB { ` eta ` +
      ` 4 ` : ` eta ` + ` 4 } ~ - ~ 3 ` X SUB { ` eta ` + ` 3 ` : ` eta ` + `
      4 } ~ + ~ 3 ` X SUB { ` eta ` + ` 2 ` : ` eta ` + ` 4 } ~ - ~ X SUB { `
      eta ` + ` 1 ` : ` eta ` + ` 4 } ` ] }</para>
    </section>

    <para>Presented in Table 1.1.<link
    linkend="WPGeneratedID_TOC_1_32">4</link> is the relationship between the
    first four LH moments and the parameters of the GEV and Gumbel
    distributions.</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFATable4_1"/>
    </para>

    <section label="page" role="textbox">
      <para>Table 1.1.4 - - LH Moments for GEV and Gumbel Distributions (from
      Wang, 1997)</para>

      <table>
        <title>TABLE A</title>

        <tgroup cols="2">
          <colspec align="center" colname="Column1" colwidth="1.574in"/>

          <colspec align="center" colname="Column2" colwidth="4.693in"/>

          <tbody>
            <row>
              <entry valign="center">
                <para>Family</para>
              </entry>

              <entry valign="center">
                <para>LH Moments</para>
              </entry>
            </row>

            <row>
              <entry valign="center">
                <para>Generalised Extreme Value (GEV)</para>
              </entry>

              <entry valign="center">
                <para>FUNC { lambda SUB 1 SUP eta ~ = ~ tau ~ + ~ alpha OVER
                kappa ` LEFT [ ` 1 ~ - ~ GAMMA ` ( ` 1 ~ + ~ kappa ` ) ` LEFT
                ( ` eta ~ + ~ 1 ` RIGHT ) SUP { ` - ` kappa } ` RIGHT ] } # ~
                # FUNC { lambda SUB 2 SUP eta ~ = ~ { LEFT ( ` eta ~ + ~ 2 `
                RIGHT ) ` alpha ` GAMMA ` ( ` 1 ~ + ~ kappa ` ) } OVER { 2 ` !
                ~ kappa } ` LEFT [ ` - ~ ( ` eta ~ + ~ 2 ` ) SUP { ` - ` kappa
                } ~ + ~ ( ` eta ~ + ~ 1 ` ) SUP { - ` kappa } ` RIGHT ] } # ~
                # FUNC { lambda SUB 3 SUP eta ~ = ~ { ( ` eta ~ + ~ 3 ` ) `
                alpha ` GAMMA ` ( ` 1 ~ + ~ kappa ` ) } OVER { 3 ` ! ~ kappa }
                ` LEFT [ ` - ~ ( ` eta ~ + ~ 4 ` ) ` ( ` eta ~ + ~ 3 ` ) SUP {
                ` - ` kappa } ~ + RIGHT . } # FUNC { LEFT . ~ 2 ` ( ` eta ~ +
                ~ 3 ~ ) ~ ( ~ eta ~ + ~ 2 ` ) SUP { ` - ` kappa } ~ - ~ ( `
                eta ~ + ~ 2 ` ) ` ( ` eta ~ + ~ 1 ` ) SUP { ` - ` kappa } `
                RIGHT ] } # ~ # FUNC { lambda SUB 4 SUP eta ~ = ~ { ( ` eta ~
                + ~ 4 ` ) ` alpha ` GAMMA ` ( ` 1 ~ + ~ kappa ` ) } OVER { 4 `
                ! ` kappa } ` LEFT [ ` - ~ ( ` eta ~ + ~ 6 ` ) ` ( ` eta ~ + ~
                5 ` ) ` ( ` eta ~ + ~ 4 ` ) SUP { ` - ` kappa } ~ + RIGHT . }
                # ~ # FUNC { 3 ` ( ` eta ~ + ~ 5 ` ) ` ( ` eta ~ + ~ 4 ` ) ` (
                ` eta ~ + ~ 3 ` ) SUP { ` - ` kappa } ~ - } # ~ # FUNC { LEFT
                . 3 ` ( ` eta ~ + ~ 4 ` ) ` ( ` eta ~ + ~ 3 ` ) ` ( ` eta ~ +
                ~ 2 ` ) SUP { ` - ` kappa } ~ + ~ ( ` eta ~ + ~ 3 ` ) ` ( `
                eta ~ + ~ 2 ` ) ` ( ` eta ~ + ~ 1 ` ) SUP { ` - ` kappa } `
                RIGHT ] } , κ ≠0</para>
              </entry>
            </row>

            <row>
              <entry valign="center">
                <para>Gumbel</para>
              </entry>

              <entry valign="center">
                <para>FUNC { lambda SUB 1 SUP eta ~ = ~ tau ~ + ~ alpha ` LEFT
                [ ` 0.5722 ~ + ~ ln ` ( ` eta ~ + ~ 1 ` ) ` RIGHT ] } # ~ #
                FUNC { lambda SUB 2 SUP eta ~ = ~ { ( ` eta ~ + ~ 2 ` ) `
                alpha } OVER { 2 ` ! } ` LEFT [ ` ln ` ( ` eta ~ + ~ 2 ` ) ~ -
                ~ ln ` ( ` eta ~ + ~ 1 ` ) ` RIGHT ] } # ~ # FUNC { lambda SUB
                3 SUP eta ~ = ~ { ( ` eta ~ + ~ 3 ` ) ` alpha } OVER { 3 ` ! }
                ` LEFT [ ` ( ` eta ~ + ~ 4 ` ) ` ln ` ( ` eta ~ + ~ 3 ` ) ~ -
                ~ 2 ` ( ` eta ~ + ~ 3 ` ) ` ln ` ( ` eta ~ + ~ 2 ` ) ~ + RIGHT
                . } # ~ # FUNC { LEFT . ( ` eta ~ + ~ 2 ` ) ` ln ` ( ` eta ~ +
                ~ 1 ` ) ` RIGHT ] } # ~ # FUNC { lambda SUB 4 SUP eta ~ = ~ {
                ( ` eta ~ + ~ 4 ` ) ` alpha } OVER { 4 ` ! } ` LEFT [ ` ( `
                eta ~ + ~ 6 ` ) ` ( ` eta ~ + ~ 5 ` ) ` ln ` ( ` eta ~ + ~ 4 `
                ) ~ - RIGHT . } # ~ # FUNC { LEFT . 3 ` ( ` eta ~ + ~ 5 ` ) `
                ( ` eta ~ + ~ 4 ` ) ` ln ` ( ` eta ~ + ~ 3 ` ) ~ + ~ RIGHT . }
                # ~ # FUNC { LEFT . ( ` eta ~ + ~ 4 ` ) ` ( ` eta ~ + ~ 3 ` )
                ` ln ` ( ` eta ~ + ~ 2 ` ) ~ - RIGHT . } # ~ # FUNC { LEFT . ~
                ( ` eta ~ + ~ 3 ` ) ` ( ~ eta ~ + ~ 2 ` ) ` ln ` ( ` eta ~ + ~
                1 ` ) ` RIGHT ] }</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <para>For ease of computation Wang (1997) derived the following
    approximation for the shape parameter κ:</para>

    <para>
      <anchor id="WPGeneratedID_Xref_LHPolynomEqn_1"/>
    </para>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.46)FUNC { kappa ~ = ~ a SUB 0 ~ + ~ a SUB 1 ` LEFT [ ` tau
      SUB 3 SUP eta ` RIGHT ] ~ + ~ a SUB 2 ` LEFT [ ` tau SUB 3 SUP eta `
      RIGHT ] SUP { ~ 2 } ~ + ~ a SUB 3 ` LEFT [ ` tau SUB 3 SUP eta ` RIGHT ]
      SUP { ~ 3 } }</para>
    </section>

    <para>where the polynomial coefficients vary with η according to Table
    1.1.<link linkend="WPGeneratedID_TOC_1_34">5</link>.</para>

    <para>
      <anchor id="WPGeneratedID_Xref_PolyCoefTable_1"/>
    </para>

    <section label="page" role="textbox">
      <para>Table 1.1.5 - - Polynomial Coefficients for use with Equation
      1.1.<link linkend="WPGeneratedID_TOC_1_33">46</link></para>

      <table>
        <title>Table_A</title>

        <tgroup cols="5">
          <colspec align="center" colname="Column1" colwidth="0.463in"/>

          <colspec align="center" colname="Column2" colwidth="0.788in"/>

          <colspec align="center" colname="Column3" colwidth="0.788in"/>

          <colspec align="center" colname="Column4" colwidth="0.788in"/>

          <colspec align="center" colname="Column5" colwidth="0.788in"/>

          <tbody>
            <row>
              <entry align="center" valign="center">
                <para>η</para>
              </entry>

              <entry align="center" valign="center">
                <para>a0</para>
              </entry>

              <entry align="center" valign="center">
                <para>a1</para>
              </entry>

              <entry align="center" valign="center">
                <para>a2</para>
              </entry>

              <entry align="center" valign="center">
                <para>a3</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>0</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.2849</para>
              </entry>

              <entry align="center" valign="center">
                <para>1.8213</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.8140</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.2835</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>1</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.4823</para>
              </entry>

              <entry align="center" valign="center">
                <para>2.1494</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.7269</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.2103</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>2</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.5914</para>
              </entry>

              <entry align="center" valign="center">
                <para>2.3351</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.6442</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.1616</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>3</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.6618</para>
              </entry>

              <entry align="center" valign="center">
                <para>2.4548</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.5733</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.1273</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>4</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.7113</para>
              </entry>

              <entry align="center" valign="center">
                <para>2.5383</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.5142</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.1027</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <para>Wang (1997) derived the following estimators for LH moments with
    shift parameter η:</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.47)MATRIX { FUNC { lambda HAT SUB 1 SUP eta ~ = ~ 1 OVER { \
      SUP { n ` } C SUB { eta ` + ` 1 } } ~ SUM FROM { i ` = ` 1 } TO n ` \
      SUP { i ` - ` 1 } C SUB eta ~ q SUB { ( ` i ` ) } } # ~ #</para>

      <para>FUNC { lambda HAT SUB 2 SUP eta ~ = ~ 1 OVER 2 ` 1 OVER { \ SUP n
      C SUB { eta ` + ` 2 } } ~ SUM FROM { i ` = ` 1 } TO n ` LEFT ( ` \ SUP {
      i ` - ` 1 } C SUB { eta ` + ` 1 } ~ - ~ \ SUP { i ` - ` 1 } C SUB eta `
      \ SUP { n ` - ` i } C SUB 1 ` RIGHT ) ~ q SUB { ( ` i ` ) } } # ~
      #</para>

      <para>FUNC { lambda HAT SUB 3 SUP eta ~ = ~ 1 OVER 3 ` 1 OVER { \ SUP n
      C SUB { eta ` + ` 3 } } ~ SUM FROM { i ` = ` 1 } TO n ~ LEFT ( ` \ SUP {
      i ` - ` 1 } C SUB { eta ` + ` 2 } ~ - ~ 2 ` \ SUP { i ` - ` 1 } C SUB {
      eta ` + ` 1 } ` \ SUP { n ` - ` i ` } C SUB 1 ~ + ~ \ SUP { i ` - ` 1 `
      } C SUB eta ` \ SUP { n ` - ` i ` } C SUB 2 ` RIGHT ) ~ q SUB { ( ` i `
      ) } } # ~ #</para>

      <para>FUNC { lambda HAT SUB 4 SUP eta ~ = ~ { 1 OVER 4 } ` { 1 OVER { \
      SUP n C SUB { eta ` + ` 4 } } } ~ SUM FROM { i ` = ` 1 } TO n ~ LEFT ( \
      SUP { i ` - ` 1 } C SUB { eta ` + ` 3 } ~ - ~ 3 ` \ SUP { i ` - ` 1 ` }
      C SUB { eta ` + ` 2 } ` \ SUP { n ` - ` i ` } C SUB 1 ~ + RIGHT . }
      #</para>

      <para>FUNC { LEFT . ~ 3 ` \ SUP { i ` - ` 1 ` } C SUB { eta ` + ` 1 } \
      SUP { n ` - ` i ` } C SUB 2 ~ - ~ \ SUP { i ` - ` 1 ` } C SUB eta ` \
      SUP { n ` - ` i ` } C SUB 3 ` RIGHT ) ~ q SUB { ( ` i ` ) } } }</para>
    </section>

    <para>The selection of the best shift parameter requires some form of
    goodness-of-fit test. Wang (1998) argued that the first three LH moments
    are used to fit the GEV model leaving the fourth LH moment available for
    testing the adequacy of the fit. He proposed the following approximate
    test statistic:</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.48)FUNC { z ~ = ~ { tau HAT SUB 4 SUP eta ~ - ~ tau SUB 4 SUP
      eta } OVER { sigma ` LEFT ( ` tau HAT SUB 4 SUP eta ` LINE ` tau HAT SUB
      3 SUP eta ~ = ~ tau SUB 3 SUP eta ` RIGHT ) } }</para>
    </section>

    <para>where</para>

    <section label="inline" role="textbox">
      <para>FUNC { tau HAT SUB 4 SUP eta }</para>
    </section>

    <para>is the sample estimate of the LH-kurtosis,</para>

    <section label="inline" role="textbox">
      <para>FUNC { tau SUB 4 SUP eta }</para>
    </section>

    <para>is the LH-kurtosis derived from the GEV parameters fitted to the
    first three LH moments, and</para>

    <section label="inline" role="textbox">
      <para>FUNC { sigma ` LEFT ( ` tau HAT SUB 4 SUP eta ` LINE ` tau HAT SUB
      3 SUP eta ~ = ~ tau SUB 3 SUP eta ` RIGHT ) }</para>
    </section>

    <para>is the standard deviation of</para>

    <section label="inline" role="textbox">
      <para>FUNC { tau HAT SUB 4 SUP eta }</para>
    </section>

    <para>assuming the sample LH-skewness equals the LH-skewness derived from
    the GEV parameters fitted to the first three LH moments. Under the
    hypothesis that the underlying distribution is GEV, the test statistic z
    is approximately normal distributed with mean 0 and variance 1. Wang
    (1998) describes a simple relationship to estimate</para>

    <section label="inline" role="textbox">
      <para>FUNC { sigma ` LEFT ( ` tau HAT SUB 4 SUP eta ` LINE ` tau HAT SUB
      3 SUP eta ~ = ~ tau SUB 3 SUP eta ` RIGHT ) }</para>
    </section>

    <para>.</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" numeration="lowerroman">
              <listitem>
                <para>
                  <emphasis role="bold"/>

                  <phrase id="WPGeneratedID_TOC_1_34">Parameter Uncertainty
                  and Quantile Confidence Limits</phrase>
                </para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>The sampling distribution p(θ | D, M)<superscript/> <footnote id="2">
        <para>Some liberties are taken with notation to provide a consistent
        treatment of uncertainty. In Section 6.3 p(q|D,M) referred to a
        Bayesian posterior pdf. Here it refers to the sampling distribution of
        q. Although the distributions are different, they do describe
        uncertainty about q. That said, the Bayesian approach is more general
        in its ability to make full use of the information in the data and
        accept prior information.</para>
      </footnote> can be approximated using the Monte Carlo method known as
    the parametric bootstrap:</para>

    <para/>

    <orderedlist continuation="restarts" numeration="arabic">
      <listitem>
        <para>Fit the model M to n years of gauged flows using L or LH moments
        to yield the parameter estimate</para>

        <para>;</para>
      </listitem>
    </orderedlist>

    <section label="inline" role="textbox">
      <para>FUNC { theta HAT }</para>
    </section>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <para>Set i = 1;</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <para>Randomly sample n flows from the fitted distribution; that is,
        q<subscript>ij</subscript> ← p(q ∣</para>

        <para>, M), j = 1, .., n;</para>
      </listitem>
    </orderedlist>

    <section label="inline" role="textbox">
      <para>FUNC { theta HAT }</para>
    </section>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <para>Fit the model to the sampled flows {q<subscript>ji</subscript>,
        j = 1, .., n} using L or LH moments to yield the parameter estimate
        i.</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <para>Increment I and go to step 3 if i does not exceed N.</para>
      </listitem>
    </orderedlist>

    <para/>

    <para>This procedure yields N equi-weighted samples that approximate the
    sampling distribution p(θ | D, M). As a result, they can be used to
    quantify parameter uncertainty and estimate quantile confidence limits.
    However, because the parametric bootstrap assumes</para>

    <section label="inline" role="textbox">
      <para>FUNC { theta HAT }</para>
    </section>

    <para>is the true parameter, it underestimates the uncertainty and
    therefore should not be used to estimate expected probabilities.</para>

    <para/>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="lowerroman">
      <listitem>
        <para><emphasis role="bold"/>Software</para>
      </listitem>
    </orderedlist>

    <para/>

    <para>Implementation of L and LH moments requires extensive computation.
    Users are referred the Australian Rainfall and Runoff web site <emphasis
    role="underline">www.arr.org.au</emphasis> for discussions of some
    available software. It should be noted that other software not noted on
    the Australian Rainfall and Runoff website may be available also. For
    example, an extensive library of FORTRAN L moment procedures can be found
    at <emphasis
    role="underline">www.research.ibm.com/people/h/hosking</emphasis>.</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <orderedlist continuation="restarts" numeration="lowerroman">
              <listitem>
                <para><emphasis role="bold"/>Worked Examples</para>
              </listitem>
            </orderedlist>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>While these worked examples of the L Moment and LH Moment approaches
    are intended to illustrate aspects of the preceding discussion, many users
    will find that the problem they are considering will not be identical to
    these worked examples. Nonetheless, these worked examples are important
    for the illustration of the concepts and approaches recommended
    herein.</para>

    <para/>

    <para>The worked examples are</para>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>Fitting the GEV distribution using L moments to a 47-year gauged
        record is illustrated in Example <link
        linkend="WPGeneratedID_TOC_1_55">7</link>.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <itemizedlist mark="•">
      <listitem>
        <para>Example <link linkend="WPGeneratedID_TOC_1_57">8</link> is
        similar to Example <link linkend="WPGeneratedID_TOC_1_46">5</link>.
        Demonstrated in this example is the search procedure for finding the
        optimal shift in LH moments fitting.</para>
      </listitem>
    </itemizedlist>

    <para/>

    <orderedlist continuation="restarts" inheritnum="inherit"
                 numeration="loweralpha">
      <listitem>
        <para><emphasis role="bold"/>Method of Product Moments Approach</para>
      </listitem>
    </orderedlist>

    <para/>

    <para>The method of product moments used in conjunction with the LP3
    distribution was the recommended method in the previous edition of
    Australian Rainfall and Runoff (Pilgrim, 1987). The method was simple to
    implement and was consistent with US practice. However, in view of its
    inferior performance to other methods, its use is no longer recommended
    with LP3 using at-site data.</para>

    <para/>

    <orderedlist continuation="continues" inheritnum="inherit"
                 numeration="arabic">
      <listitem>
        <para><emphasis role="bold"/>FITTING FLOOD PROBABILITY MODELS TO POT
        SERIES</para>
      </listitem>
    </orderedlist>

    <para/>

    <para>In Section 2.2.3 it was noted that when interest is in events with
    T<subscript>A</subscript> &lt; 10 years generally it is recommended that a
    POT series is used rather than an annual series. This recommendation
    arises because all floods (above some threshold) are of interest in this
    range, whether they are the highest in the particular year of record or
    not. The annual maximum series may omit many floods of interest; Examples
    7 and 9 show examples of this. Outlined in this section are procedures for
    fitting a probability model to a POT series.</para>

    <para/>

    <orderedlist continuation="continues" inheritnum="inherit"
                 numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" inheritnum="inherit"
                     numeration="loweralpha">
          <listitem>
            <para><emphasis role="bold"/>Probability Plots</para>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>As with the analysis of the annual maximum series, it is recommended
    that a probability plot of the POT data series be prepared. This involves
    plotting an estimate of the observed ARI against the observed discharge.
    The ARI of a gauged flood can be estimated using</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>(1.1.49)FUNC { T SUB { ( ` i ` ) } ~ = ~ { n ~ + ~ 0.2 } OVER { i
      ~ - ~ 0.4 } }</para>
    </section>

    <para>where i is the rank of the gauged flood (in descending order) and n
    is the number of years of record.</para>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <orderedlist continuation="restarts" numeration="loweralpha">
          <listitem>
            <para><emphasis role="bold"/>Fitting POT Models</para>
          </listitem>
        </orderedlist>
      </listitem>
    </orderedlist>

    <para/>

    <para>In some cases, it may be desirable to analytically fit a probability
    distribution to POT data. Two approaches can be employed.</para>

    <para/>

    <para>The first employs the following steps:</para>

    <para/>

    <orderedlist continuation="restarts" numeration="arabic">
      <listitem>
        <para>For a given threshold q<subscript>0</subscript>, fit a
        probability distribution with pdf p(q | q &gt;
        q<subscript>0</subscript>, θ) to the POT series;</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <para>Estimate ν the average number of POT events per unit;</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <para>The expected time interval between peaks that exceed w is given
        by equation 1.1.<link linkend="WPGeneratedID_TOC_1_7">7</link>
        as</para>
      </listitem>
    </orderedlist>

    <para/>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>FUNC { T SUB P ` ( ` w ` ) ~ = ~ 1 OVER { nu ` P ` ( ` Q ~ &gt; ~
      w ~ LINE ~ q SUB 0 , ` theta ` ) } }</para>
    </section>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <para>Check that the fitted distribution is consistent with the POT
        data using the probability plot described in Section 4.2.7.1.</para>
      </listitem>
    </orderedlist>

    <para/>

    <para>A number of authors have investigated the proposition that it is
    better to infer the annual flow series distribution from the POT series on
    the grounds that the POT series considers all relevant data. For the
    exponential-Poisson POT model with an average arrival rate ν &gt; 1.65
    floods per year, Cunnane (1973) showed that the sample variance of T-year
    quantiles (T &gt; 10 years) is less than that obtained for a Gumbel
    distribution fitted to annual maximum series. Jayasuriya and Mein (1985),
    Ashkar and Rousselle (1983) and Tavares and da Silva (1983) noted that the
    exponential-Poisson POT model was fairly successful, but some results
    diverged from the distribution derived directly from the annual series.
    Madsen <emphasis role="italic">et al</emphasis>. (1997) investigated this
    issue using a GP-Poisson POT model. They concluded that for negative shape
    parameters κ the POT model generally was preferable for at-site quantile
    estimation. However, this conclusion is dependent on the choice of
    estimator. Martins and Stedinger (2001) showed for negative κ that the
    generalized maximum likelihood estimator performs better than method of
    moments and L moments. Significantly, they found that the precision of
    flood quantiles derived from a POT model is insensitive to the arrival
    rate ν and for the 100-year quantile is similar to the precision obtained
    fitting a GEV model to annual maximum data.</para>

    <para/>

    <para>At this stage, the POT approach cannot be recommended as a
    replacement for the analysis of annual series data. If possible, the
    threshold for the exponential-Poisson POT model should be selected so that
    the number of floods in the POT series K is at least 2 to 3 times the
    number of years of record N. For the GP-Poisson POT model it suffices that
    exceed N. However, it may be necessary to use a much lower value of K in
    regions with low rainfall where the number of recorded events that could
    be considered as floods is low.</para>

    <para/>

    <para>The second approach uses a probability distribution as an arbitrary
    means of providing a consistent and objective fit to POT series data. For
    example, McDermott and Pilgrim (1982), Adams and McMahon (1985) and
    Jayasuriya and Mein (1985) used the LP3 distribution and found that
    selecting a threshold discharge such that K equalled N was best.</para>

    <para/>

    <orderedlist continuation="restarts" numeration="arabic">
      <listitem>
        <para><emphasis role="bold"/>WORKED EXAMPLES</para>
      </listitem>
    </orderedlist>

    <para/>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEx1_1"/>
    </para>

    <para>
      <emphasis role="bold">EXAMPLE 1: EXTRAPOLATION AND PROCESS
      UNDERSTANDING</emphasis>
    </para>

    <para/>

    <para>The importance of process understanding when extrapolating beyond
    the observed record is illustrated by a simple Monte Carlo experiment. A
    Poisson rectangular pulse rainfall model is used to generate a long record
    of high resolution rainfall. This rainfall record is routed through a
    rainfall-runoff model to generate runoff into the stream system. The
    storage-discharge relationship for the stream is depicted by the bilinear
    relationship shown in Figure 1.1.<link
    linkend="WPGeneratedID_TOC_1_36">8</link>. A feature of this relationship
    is the activation of significant flood terrace storage once a threshold
    discharge is exceeded.</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEx1Fig_1"/>

      <graphic align="center" depth="3.437in"
               fileref="ARRFFA/WPGeneratedName6.JPG" format="JPG" valign="top"
               width="4.527in"/>
    </para>

    <para>Figure 1.1.8 - Bilinear Channel Storage-Discharge
    Relationship</para>

    <para>The routing model parameters were selected so that major flood
    terrace storage is activated by floods with an ARI in excess of 100 years.
    This situation was chosen to represent a river with multiple flood
    terraces with the lowest terraces accommodating the majority of floods and
    the highest terrace only inundated by extreme floods.</para>

    <para/>

    <para>Presented in Figure 1.1.<link
    linkend="WPGeneratedID_TOC_1_37">9</link> is the flood frequency curve
    based on 30,000 simulated years. As shown in this figure, there is a clear
    break in slope around the 100 year ARI corresponding to the activation of
    major flood terrace storage. Indeed the flood frequency curve displays
    downward curvature despite that the fact the rainfall frequency curve
    displays upward curvature in the 100 to 1000 year ARI range. In contrast
    the flood frequency curve based on 100 years of “data” has no evidence of
    downward curvature. This result is because in a 100-year record there is
    little chance of the major flood terrace storage being activated. Indeed
    without knowledge of the underlying hydraulics, an analyst would be
    tempted to extrapolate the 100-year flood record using a straight line
    extrapolation. Such an extrapolation would rapidly diverge from the “true”
    frequency curve.</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEx2Fig_1"/>

      <graphic align="center" depth="4.302in"
               fileref="ARRFFA/WPGeneratedName7.JPG" format="JPG" valign="top"
               width="4.920in"/>
    </para>

    <para>Figure 1.1.9 - Simulated Rainfall and Flood Frequency Curves with
    Major Floodplain Storage Activated at a Threshold Discharge of
    3500m3/s</para>

    <para>Although the dominant rainfall-runoff dynamics are idealised in this
    example, a very strong message is obtained from interpretation of the
    results. Extrapolation of flood frequency curves fitted to gauged flow
    records requires the exercise of hydrologic judgment backed up by
    appropriate modelling. Furthermore, the problem of extrapolation is more
    general. For example, in this example, if a rainfall-runoff catchment
    modelling approach were used with the catchment model calibrated only to
    small events (ie events less than 100 year ARI), the simulated data used
    for development of the flood frequency curve are likely to be compromised
    in a similar way.</para>

    <para/>

    <para/>

    <para/>

    <para><emphasis role="bold"/>EXAMPLE <anchor
    id="WPGeneratedID_Xref_FFAEx2_1"/>2: FITTING A PROBABILITY MODEL TO GAUGED
    DATA</para>

    <para/>

    <para>Illustrated in this example is the fitting of a probability model to
    gauged annual maximum flood data. Listed in Table 1.1.<link
    linkend="WPGeneratedID_TOC_1_39">6</link> are 31 years of gauged annual
    maximum discharges (m<superscript>3</superscript>/s) for the Hunter river
    at Singleton.</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEx2Tab1_1"/>
    </para>

    <section label="page" role="textbox">
      <para>Table 1.1.6 - - Annual Series for the Hunter River at
      Singleton</para>

      <table>
        <title>TABLE A</title>

        <tgroup cols="8">
          <colspec align="center" colname="Column1" colwidth="0.708in"/>

          <colspec align="center" colname="Column2" colwidth="0.708in"/>

          <colspec align="center" colname="Column3" colwidth="0.708in"/>

          <colspec align="center" colname="Column4" colwidth="0.708in"/>

          <colspec align="center" colname="Column5" colwidth="0.708in"/>

          <colspec align="center" colname="Column6" colwidth="0.708in"/>

          <colspec align="center" colname="Column7" colwidth="0.708in"/>

          <colspec align="center" colname="Column8" colwidth="0.708in"/>

          <tbody>
            <row>
              <entry valign="center">
                <para>76.19</para>
              </entry>

              <entry valign="center">
                <para>171.7</para>
              </entry>

              <entry valign="center">
                <para>218.0</para>
              </entry>

              <entry valign="center">
                <para>668.2</para>
              </entry>

              <entry valign="center">
                <para>1373.</para>
              </entry>

              <entry valign="center">
                <para>124.0</para>
              </entry>

              <entry valign="center">
                <para>276.0</para>
              </entry>

              <entry valign="center">
                <para>894.8</para>
              </entry>
            </row>

            <row>
              <entry valign="center">
                <para>1373.</para>
              </entry>

              <entry valign="center">
                <para>279.9</para>
              </entry>

              <entry valign="center">
                <para>202.4</para>
              </entry>

              <entry valign="center">
                <para>4049.</para>
              </entry>

              <entry valign="center">
                <para>2321.</para>
              </entry>

              <entry valign="center">
                <para>2534.</para>
              </entry>

              <entry valign="center">
                <para>3313.</para>
              </entry>

              <entry valign="center">
                <para>1231.</para>
              </entry>
            </row>

            <row>
              <entry valign="center">
                <para>1390.</para>
              </entry>

              <entry valign="center">
                <para>12515</para>
              </entry>

              <entry valign="center">
                <para>1098.</para>
              </entry>

              <entry valign="center">
                <para>447.4</para>
              </entry>

              <entry valign="center">
                <para>478.5</para>
              </entry>

              <entry valign="center">
                <para>180.3</para>
              </entry>

              <entry valign="center">
                <para>164.2</para>
              </entry>

              <entry valign="center">
                <para>229.3</para>
              </entry>
            </row>

            <row>
              <entry valign="center">
                <para>2123.</para>
              </entry>

              <entry valign="center">
                <para>965.5</para>
              </entry>

              <entry valign="center">
                <para>2749.</para>
              </entry>

              <entry valign="center">
                <para>48.98</para>
              </entry>

              <entry valign="center">
                <para>76.45</para>
              </entry>

              <entry valign="center">
                <para>911.7</para>
              </entry>

              <entry valign="center">
                <para>925.9</para>
              </entry>

              <entry valign="center"/>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <para>An LP3 probability distribution was fitted to these data using a
    Bayesian approach. Presented in Table 1.1.<link
    linkend="WPGeneratedID_TOC_1_40">7</link> are the posterior mean, standard
    deviation and correlation for the LP3 parameters m, s and g which are
    respectively the mean, standard deviation and skewness of
    log<subscript>e</subscript> q.</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEx2Tab2_1"/>
    </para>

    <section label="page" role="textbox">
      <para>Table 1.1.7 - - LP3 Parameters Determined by Bayesian
      Approach</para>

      <table>
        <title>Table_B</title>

        <tgroup cols="6">
          <colspec align="center" colname="Column1" colwidth="1.354in"/>

          <colspec align="center" colname="Column2" colwidth="0.788in"/>

          <colspec align="center" colname="Column3" colwidth="0.984in"/>

          <colspec align="center" colname="Column4" colwidth="0.590in"/>

          <colspec align="center" colname="Column5" colwidth="0.591in"/>

          <colspec align="center" colname="Column6" colwidth="0.590in"/>

          <tbody>
            <row>
              <entry align="center" valign="center">
                <para>LP3 Parameter</para>
              </entry>

              <entry align="center" valign="center">
                <para>Mean</para>
              </entry>

              <entry align="center" valign="center">
                <para>Std Deviation</para>
              </entry>

              <entry align="center" nameend="Column6" namest="Column4"
                     valign="center">
                <para>Correlation</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>m</para>
              </entry>

              <entry align="center" valign="center">
                <para>6.426</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.236</para>
              </entry>

              <entry align="center" valign="center">
                <para>1.000</para>
              </entry>

              <entry align="center" valign="center"/>

              <entry align="center" valign="center"/>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>loge s</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.350</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.127</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.046</para>
              </entry>

              <entry align="center" valign="center">
                <para>1.000</para>
              </entry>

              <entry align="center" valign="center"/>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>g</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.146</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.643</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.000</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.068</para>
              </entry>

              <entry align="center" valign="center">
                <para>1</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <para>Plotted in Figure 1.1.<link
    linkend="WPGeneratedID_TOC_1_41">10</link> on a log normal probability
    plot are</para>

    <itemizedlist mark="•">
      <listitem>
        <para>the gauged flows;</para>
      </listitem>

      <listitem>
        <para>the 1 in Y AEP quantile curve (derived using the posterior mean
        parameters);</para>
      </listitem>

      <listitem>
        <para>the 90% quantile confidence limits; and</para>
      </listitem>

      <listitem>
        <para>the expected probability curve.</para>
      </listitem>
    </itemizedlist>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEx2Fig1_1"/>

      <graphic align="center" depth="3.813in"
               fileref="ARRFFA/WPGeneratedName8.JPG" format="JPG" valign="top"
               width="4.920in"/>
    </para>

    <para>Figure 1.1.10 - Bayesian LP3 Fit to 31 Years of Gauged Annual
    Maximum Flow in the Hunter River at Singleton</para>

    <para>The good fit to the gauged data and the tight confidence limits
    needs to be tempered by the fact that the logarithm of the flood peaks are
    plotted in Figure 1.1.<link linkend="WPGeneratedID_TOC_1_41">10</link>.
    The table of selected 1 in Y AEP quantiles q<subscript>Y</subscript> and
    their 90% confidence limits presents a more sobering perspective. For
    example, for the 1 in 100 AEP flood the 5% and 95% confidence limits are
    respectively 38% and 553% of the quantile q<subscript>Y</subscript>! The 1
    in 500 AEP confidence limits are so wide as to render estimation
    meaningless. Note the expected AEP for the quantile
    q<subscript>Y</subscript> consistently exceeds the nominal 1 in Y AEP. For
    example, the 1 in 100 AEP quantile of 19548
    m<superscript>3</superscript>/s has an expected AEP of 1 in 74.</para>

    <section label="page" role="textbox">
      <para/>

      <table>
        <title>TABLE A</title>

        <tgroup cols="5">
          <colspec align="center" colname="Column1" colwidth="0.777in"/>

          <colspec align="center" colname="Column2" colwidth="1.279in"/>

          <colspec align="center" colname="Column3" colwidth="0.788in"/>

          <colspec align="center" colname="Column4" colwidth="0.886in"/>

          <colspec align="center" colname="Column5" colwidth="1.401in"/>

          <tbody>
            <row>
              <entry align="center" morerows="1" valign="center">
                <para>
                  <emphasis role="bold">1 in Y AEP</emphasis>
                </para>
              </entry>

              <entry align="center" morerows="1" valign="center">
                <para>
                  <emphasis role="bold">Quantile
                  q<subscript>Y</subscript></emphasis>
                </para>
              </entry>

              <entry align="center" nameend="Column4" namest="Column3"
                     valign="center">
                <para>
                  <emphasis role="bold">Quantile Confidence Limits</emphasis>
                </para>
              </entry>

              <entry align="center" morerows="1" valign="center">
                <para>
                  <emphasis role="bold">Expected AEP for
                  q<subscript>Y</subscript></emphasis>
                </para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>
                  <emphasis role="bold">5% limit</emphasis>
                </para>
              </entry>

              <entry align="center" valign="center">
                <para>
                  <emphasis role="bold">95% limit</emphasis>
                </para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>10</para>
              </entry>

              <entry align="center" valign="center">
                <para>3888</para>
              </entry>

              <entry align="center" valign="center">
                <para>2222</para>
              </entry>

              <entry align="center" valign="center">
                <para>8287</para>
              </entry>

              <entry align="center" valign="center">
                <para>1/9.9</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>50</para>
              </entry>

              <entry align="center" valign="center">
                <para>12729</para>
              </entry>

              <entry align="center" valign="center">
                <para>5537</para>
              </entry>

              <entry align="center" valign="center">
                <para>52157</para>
              </entry>

              <entry align="center" valign="center">
                <para>1/43</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>100</para>
              </entry>

              <entry align="center" valign="center">
                <para>19548</para>
              </entry>

              <entry align="center" valign="center">
                <para>7372</para>
              </entry>

              <entry align="center" valign="center">
                <para>108144</para>
              </entry>

              <entry align="center" valign="center">
                <para>1/74</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>500</para>
              </entry>

              <entry align="center" valign="center">
                <para>47366</para>
              </entry>

              <entry align="center" valign="center">
                <para>11799</para>
              </entry>

              <entry align="center" valign="center">
                <para>549773</para>
              </entry>

              <entry align="center" valign="center">
                <para>1/210</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <para/>

    <para/>

    <para><emphasis role="bold">EXAMPLE </emphasis><anchor
    id="WPGeneratedID_Xref_FFAEx3_1"/>3: USE OF CENSORED HISTORICAL
    DATA</para>

    <para/>

    <para>This example is a continuation of Example <link
    linkend="WPGeneratedID_TOC_1_38">2</link>. The benefit of using historical
    flood information Is illustrated. The gauged record spanned the period
    1938 to 1969. The biggest flood in that record occurred in 1955. An
    examination of historic records indicates that during the ungauged period
    1820 to 1937 there was only one flood that exceeded the 1955 flood and
    that this flood occurred in 1820. This information is valuable even though
    the magnitude of the 1820 flood is not reliably known. This is an example
    of censored data. Over the ungauged period 1820 to 1937 there was one
    flood above and 117 floods below the threshold discharge corresponding to
    the 1955 flood.</para>

    <para/>

    <para>An LP3 distribution was fitted to gauged and censored data using the
    Bayesian approach. Presented in the table below are the posterior mean,
    standard deviation and correlation for the LP3 parameters: m, s and g
    which are respectively the mean, standard deviation and skewness of
    log<subscript>e </subscript>q. Comparison with Example <link
    linkend="WPGeneratedID_TOC_1_38">2</link> reveals that inclusion of the
    censored data has reduced by almost 30% the uncertainty in the skewness g,
    which is the parameter that controls the shape of the distribution,
    particularly in the tail region.</para>

    <para/>

    <section label="page" role="textbox">
      <para/>

      <table>
        <title>Table_A</title>

        <tgroup cols="6">
          <colspec align="center" colname="Column1" colwidth="1.354in"/>

          <colspec align="center" colname="Column2" colwidth="0.788in"/>

          <colspec align="center" colname="Column3" colwidth="0.984in"/>

          <colspec align="center" colname="Column4" colwidth="0.590in"/>

          <colspec align="center" colname="Column5" colwidth="0.591in"/>

          <colspec align="center" colname="Column6" colwidth="0.590in"/>

          <tbody>
            <row>
              <entry align="center" valign="center">
                <para>
                  <emphasis role="bold">LP3 Parameter</emphasis>
                </para>
              </entry>

              <entry align="center" valign="center">
                <para>
                  <emphasis role="bold">Mean</emphasis>
                </para>
              </entry>

              <entry align="center" valign="center">
                <para>
                  <emphasis role="bold">Std Deviation</emphasis>
                </para>
              </entry>

              <entry align="center" nameend="Column6" namest="Column4"
                     valign="center">
                <para>
                  <emphasis role="bold">Correlation</emphasis>
                </para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>m</para>
              </entry>

              <entry align="center" valign="center">
                <para>6.359</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.226</para>
              </entry>

              <entry align="center" valign="center">
                <para>1.000</para>
              </entry>

              <entry align="center" valign="center"/>

              <entry align="center" valign="center"/>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>log<subscript>e</subscript> s</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.304</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.116</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.139</para>
              </entry>

              <entry align="center" valign="center">
                <para>1.000</para>
              </entry>

              <entry align="center" valign="center"/>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>g</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.001</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.458</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.261</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.487</para>
              </entry>

              <entry align="center" valign="center">
                <para>1.000</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <para>Plotted in Figure 1.1.<link
    linkend="WPGeneratedID_TOC_1_43">11</link> on a log normal probability
    plot are the gauged flows, the 1 in Y AEP quantile curve (derived using
    the posterior mean parameters), the 90% quantile confidence limits and the
    expected probability curve. Compared with Example <link
    linkend="WPGeneratedID_TOC_1_38">2</link> the tightening of the confidence
    limits is most noticeable.</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEx3Fig_1"/>

      <graphic align="center" depth="3.807in"
               fileref="ARRFFA/WPGeneratedName9.JPG" format="JPG" valign="top"
               width="4.920in"/>
    </para>

    <para>Figure 1.1.11 - Bayesian LP3 fit to 31 Years of Gauged Annual
    Maximum Floods</para>

    <para>The table of selected 1 in Y AEP quantiles q<subscript>Y</subscript>
    and their 90% confidence limits illustrates the benefit of the information
    contained in the censored data. For example, for the 1 in 100 AEP flood
    the 5% and 95% confidence limits are respectively 57% and 200% of the
    quantile q<subscript>Y</subscript>! This represents a major reduction in
    quantile uncertainty compared with Example <link
    linkend="WPGeneratedID_TOC_1_38">2</link> which yielded limits of 38% and
    553%.</para>

    <para/>

    <section label="page" role="textbox">
      <para/>

      <table>
        <title>TABLE A</title>

        <tgroup cols="5">
          <colspec align="center" colname="Column1" colwidth="0.777in"/>

          <colspec align="center" colname="Column2" colwidth="1.279in"/>

          <colspec align="center" colname="Column3" colwidth="0.788in"/>

          <colspec align="center" colname="Column4" colwidth="0.886in"/>

          <colspec align="center" colname="Column5" colwidth="1.401in"/>

          <tbody>
            <row>
              <entry align="center" morerows="1" valign="center">
                <para>
                  <emphasis role="bold">1 in Y AEP</emphasis>
                </para>
              </entry>

              <entry align="center" morerows="1" valign="center">
                <para>
                  <emphasis role="bold">Quantile
                  q<subscript>Y</subscript></emphasis>
                </para>
              </entry>

              <entry align="center" nameend="Column4" namest="Column3"
                     valign="center">
                <para>
                  <emphasis role="bold">Quantile Confidence Limits</emphasis>
                </para>
              </entry>

              <entry align="center" morerows="1" valign="center">
                <para>
                  <emphasis role="bold">Expected AEP for
                  q<subscript>Y</subscript></emphasis>
                </para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>
                  <emphasis role="bold">5% limit</emphasis>
                </para>
              </entry>

              <entry align="center" valign="center">
                <para>
                  <emphasis role="bold">95% limit</emphasis>
                </para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>10</para>
              </entry>

              <entry align="center" valign="center">
                <para>3281</para>
              </entry>

              <entry align="center" valign="center">
                <para>2188</para>
              </entry>

              <entry align="center" valign="center">
                <para>5006</para>
              </entry>

              <entry align="center" valign="center">
                <para>1/9.6</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>50</para>
              </entry>

              <entry align="center" valign="center">
                <para>9351</para>
              </entry>

              <entry align="center" valign="center">
                <para>5786</para>
              </entry>

              <entry align="center" valign="center">
                <para>16416</para>
              </entry>

              <entry align="center" valign="center">
                <para>1/48</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>100</para>
              </entry>

              <entry align="center" valign="center">
                <para>13535</para>
              </entry>

              <entry align="center" valign="center">
                <para>7751</para>
              </entry>

              <entry align="center" valign="center">
                <para>27100</para>
              </entry>

              <entry align="center" valign="center">
                <para>1/93</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>500</para>
              </entry>

              <entry align="center" valign="center">
                <para>28615</para>
              </entry>

              <entry align="center" valign="center">
                <para>12790</para>
              </entry>

              <entry align="center" valign="center">
                <para>87327</para>
              </entry>

              <entry align="center" valign="center">
                <para>1/363</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <para>Highlighted in this example are the significant reductions in
    uncertainty that inclusion of historical data can offer. However, care
    must be exercised ensuring the integrity of the historic information – see
    Section <xref linkend="WPGeneratedID_Xref_HistoriticalDataSect_1"/> of
    this Chapter in Book 1 of Australian Rainfall and Runoff for more
    details.</para>

    <para/>

    <para/>

    <para><emphasis role="bold">EXAMPLE </emphasis><anchor
    id="WPGeneratedID_Xref_FFAEx4_1"/>4: USE OF REGIONAL INFORMATION</para>

    <para/>

    <para>As with the previous example, this example is a continuation of
    Example <link linkend="WPGeneratedID_TOC_1_38">2</link>. In Example <link
    linkend="WPGeneratedID_TOC_1_38">2</link>, the posterior mean of the
    skewness was estimated to be 0.146 with a posterior standard of 0.643. The
    uncertainty in the 1 in 100 AEP quantile was large.</para>

    <para/>

    <para>Suppose hypothetically that a regional analysis of skewness was
    conducted. Furthermore, suppose the expected regional skew is 0.00 with a
    standard deviation of 0.35. Following Section <xref
    linkend="WPGeneratedID_Xref_PriorDistSect_1"/> of this Chapter in Book 1
    of Australian Rainfall and Runoff, this information can be approximated as
    a normally distributed prior distribution for the skewness. A probability
    plot for the LP3 model fitted to the gauged data with prior information on
    the skewness is shown in Figure 1.1.<link
    linkend="WPGeneratedID_TOC_1_45">12</link>. Comparison with Figure
    1.1.<link linkend="WPGeneratedID_TOC_1_41">10</link> reveals substantially
    improved accuracy in the right hand tail.</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEx4Fig_1"/>

      <graphic align="center" depth="3.813in"
               fileref="ARRFFA/WPGeneratedName10.JPG" format="JPG"
               valign="top" width="4.920in"/>
    </para>

    <para>Figure 1.1.12 - Bayesian LP3 Fit to 31 Years of Gauged Annual
    Maximum Floods with Prior Information on Skewness</para>

    <para>A comparison of the LP3 distribution fitted with and without prior
    information on skewness is shown in the table below. The posterior
    uncertainty on the skewness is about 88% of the prior standard deviation
    indicating the gauged data are not very informative about the shape
    parameter of the flood distribution.</para>

    <para/>

    <section label="page" role="textbox">
      <table>
        <title>Table_A</title>

        <tgroup cols="5">
          <colspec align="center" colname="Column1" colwidth="1.354in"/>

          <colspec align="center" colname="Column2" colwidth="0.712in"/>

          <colspec align="center" colname="Column3" colwidth="0.984in"/>

          <colspec align="center" colname="Column4" colwidth="0.665in"/>

          <colspec align="center" colname="Column5" colwidth="0.984in"/>

          <tbody>
            <row>
              <entry align="center" morerows="1" valign="center">
                <para>
                  <emphasis role="bold">LP3 Parameter</emphasis>
                </para>
              </entry>

              <entry align="center" nameend="Column3" namest="Column2"
                     valign="center">
                <para>
                  <emphasis role="bold">No Prior Information</emphasis>
                </para>
              </entry>

              <entry align="center" nameend="Column5" namest="Column4"
                     valign="center">
                <para>
                  <emphasis role="bold">With Prior Information</emphasis>
                </para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>Mean</para>
              </entry>

              <entry align="center" valign="center">
                <para>Std Deviation</para>
              </entry>

              <entry align="center" valign="center">
                <para>Mean</para>
              </entry>

              <entry align="center" valign="center">
                <para>Std Deviation</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>m</para>
              </entry>

              <entry align="center" valign="center">
                <para>6.426</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.236</para>
              </entry>

              <entry align="center" valign="center">
                <para>6.423</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.236</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>log<subscript>e</subscript> s</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.350</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.127</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.322</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.127</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>g</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.146</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.643</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.028</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.305</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <para>Consideration of the table below where selected 1 in Y AEP quantiles
    q<subscript>Y</subscript> and their 90% confidence limits are shown
    further illustrates the benefit of incorporating regional information. For
    example, the 1 in 100 AEP flood 5% and 95% confidence limits are
    respectively 38% and 553% of the quantile q<subscript>100</subscript> when
    no prior information is used. These confidence limits are reduced to 45%
    and 321% of the quantile q<subscript>100</subscript> using prior regional
    information.</para>

    <para/>

    <section label="page" role="textbox">
      <table>
        <title>Table_B</title>

        <tgroup cols="7">
          <colspec align="center" colname="Column1" colwidth="0.664in"/>

          <colspec align="center" colname="Column2" colwidth="0.933in"/>

          <colspec align="center" colname="Column3" colwidth="0.933in"/>

          <colspec align="center" colname="Column4" colwidth="0.933in"/>

          <colspec align="center" colname="Column5" colwidth="0.875in"/>

          <colspec align="center" colname="Column6" colwidth="0.933in"/>

          <colspec align="center" colname="Column7" colwidth="0.933in"/>

          <tbody>
            <row>
              <entry align="center" morerows="2" valign="center">
                <para>
                  <emphasis role="bold">1 in Y AEP</emphasis>
                </para>
              </entry>

              <entry align="center" nameend="Column4" namest="Column2"
                     valign="center">
                <para>
                  <emphasis role="bold">No Prior Information</emphasis>
                </para>
              </entry>

              <entry align="center" nameend="Column7" namest="Column5"
                     valign="center">
                <para>
                  <emphasis role="bold">With Prior Information</emphasis>
                </para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>
                  <emphasis role="bold">Quantile</emphasis>
                </para>

                <para>
                  <emphasis role="bold">q<subscript>Y</subscript></emphasis>
                </para>
              </entry>

              <entry align="center" nameend="Column4" namest="Column3"
                     valign="center">
                <para>
                  <emphasis role="bold"/>

                  <emphasis role="bold">Quantile Confidence Limits</emphasis>
                </para>
              </entry>

              <entry align="center" valign="center">
                <para>
                  <emphasis role="bold">Quantile</emphasis>
                </para>

                <para>
                  <emphasis role="bold">q<subscript>Y</subscript></emphasis>
                </para>
              </entry>

              <entry align="center" nameend="Column7" namest="Column6"
                     valign="center">
                <para>
                  <emphasis role="bold"/>

                  <emphasis role="bold">Quantile Confidence Limits</emphasis>
                </para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>
                  <emphasis role="italic">
                    <subscript/>
                  </emphasis>
                </para>
              </entry>

              <entry align="center" valign="center">
                <para>
                  <emphasis role="italic">5% Limit</emphasis>
                </para>
              </entry>

              <entry align="center" valign="center">
                <para>
                  <emphasis role="italic">95% Limit</emphasis>
                </para>
              </entry>

              <entry align="center" valign="center"/>

              <entry align="center" valign="center">
                <para>
                  <emphasis role="italic">5% Limit</emphasis>
                </para>
              </entry>

              <entry align="center" valign="center">
                <para>
                  <emphasis role="italic">95% Limit</emphasis>
                </para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>10</para>
              </entry>

              <entry align="center" valign="center">
                <para>3888</para>
              </entry>

              <entry align="center" valign="center">
                <para>2222</para>
              </entry>

              <entry align="center" valign="center">
                <para>8287</para>
              </entry>

              <entry align="center" valign="center">
                <para>3619</para>
              </entry>

              <entry align="center" valign="center">
                <para>2184</para>
              </entry>

              <entry align="center" valign="center">
                <para>6933</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>50</para>
              </entry>

              <entry align="center" valign="center">
                <para>12729</para>
              </entry>

              <entry align="center" valign="center">
                <para>5537</para>
              </entry>

              <entry align="center" valign="center">
                <para>52157</para>
              </entry>

              <entry align="center" valign="center">
                <para>10677</para>
              </entry>

              <entry align="center" valign="center">
                <para>5294</para>
              </entry>

              <entry align="center" valign="center">
                <para>29104</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>100</para>
              </entry>

              <entry align="center" valign="center">
                <para>19548</para>
              </entry>

              <entry align="center" valign="center">
                <para>7372</para>
              </entry>

              <entry align="center" valign="center">
                <para>108144</para>
              </entry>

              <entry align="center" valign="center">
                <para>15670</para>
              </entry>

              <entry align="center" valign="center">
                <para>7050</para>
              </entry>

              <entry align="center" valign="center">
                <para>50274</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <para/>

    <para/>

    <para/>

    <para><emphasis role="bold"/>EXAMPLE <anchor
    id="WPGeneratedID_Xref_FFAEx5_1"/>5: IMPROVING POOR FITS USING CENSORING
    OF LOW FLOW DATA</para>

    <para/>

    <para>The standard probability models such as GEV and LP3 may not
    adequately fit flood data for a variety of reasons. Often the poor fit is
    associated with a sigmoidal probability plot as illustrated in Figure
    1.1.<link linkend="WPGeneratedID_TOC_1_47">13</link>. In such cases it is
    possible to employ four or five-parameter distributions which have
    sufficient degrees of freedom to track the data in both the upper and
    lower tails of the sigmoidal curve or adopt a calibration approach that
    gives less weight to the smaller floods. The latter approach is
    illustrated in this example which considers 50 annual maximum floods for
    the Albert River at Broomfleet.</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEx4Fig1_1"/>

      <graphic align="center" depth="3.616in"
               fileref="ARRFFA/WPGeneratedName11.JPG" format="JPG"
               valign="top" width="4.920in"/>
    </para>

    <para>Figure 1.1.13 - Bayesian Fit to All Gauged Data</para>

    <para>These 50 annual maximum floods comprising the data series for the
    Albert River at Broomfleet are</para>

    <section label="page" role="textbox">
      <table>
        <title>Table_A</title>

        <tgroup cols="10">
          <colspec align="center" colname="Column1" colwidth="0.626in"/>

          <colspec align="center" colname="Column2" colwidth="0.627in"/>

          <colspec align="center" colname="Column3" colwidth="0.627in"/>

          <colspec align="center" colname="Column4" colwidth="0.627in"/>

          <colspec align="center" colname="Column5" colwidth="0.627in"/>

          <colspec align="center" colname="Column6" colwidth="0.626in"/>

          <colspec align="center" colname="Column7" colwidth="0.627in"/>

          <colspec align="center" colname="Column8" colwidth="0.627in"/>

          <colspec align="center" colname="Column9" colwidth="0.627in"/>

          <colspec align="center" colname="Column10" colwidth="0.629in"/>

          <tbody>
            <row>
              <entry align="center" valign="center">
                <para>13.02</para>
              </entry>

              <entry align="center" valign="center">
                <para>15.57</para>
              </entry>

              <entry align="center" valign="center">
                <para>15.85</para>
              </entry>

              <entry align="center" valign="center">
                <para>16.7</para>
              </entry>

              <entry align="center" valign="center">
                <para>22.36</para>
              </entry>

              <entry align="center" valign="center">
                <para>36.51</para>
              </entry>

              <entry align="center" valign="center">
                <para>72.73</para>
              </entry>

              <entry align="center" valign="center">
                <para>78.11</para>
              </entry>

              <entry align="center" valign="center">
                <para>87.73</para>
              </entry>

              <entry align="center" valign="center">
                <para>88.30</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>95.65</para>
              </entry>

              <entry align="center" valign="center">
                <para>99.90</para>
              </entry>

              <entry align="center" valign="center">
                <para>113.77</para>
              </entry>

              <entry align="center" valign="center">
                <para>116.88</para>
              </entry>

              <entry align="center" valign="center">
                <para>124.52</para>
              </entry>

              <entry align="center" valign="center">
                <para>131.03</para>
              </entry>

              <entry align="center" valign="center">
                <para>156.22</para>
              </entry>

              <entry align="center" valign="center">
                <para>156.50</para>
              </entry>

              <entry align="center" valign="center">
                <para>190.74</para>
              </entry>

              <entry align="center" valign="center">
                <para>210.55</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>220.74</para>
              </entry>

              <entry align="center" valign="center">
                <para>249.61</para>
              </entry>

              <entry align="center" valign="center">
                <para>249.61</para>
              </entry>

              <entry align="center" valign="center">
                <para>271.68</para>
              </entry>

              <entry align="center" valign="center">
                <para>285.83</para>
              </entry>

              <entry align="center" valign="center">
                <para>302.81</para>
              </entry>

              <entry align="center" valign="center">
                <para>305.64</para>
              </entry>

              <entry align="center" valign="center">
                <para>362.24</para>
              </entry>

              <entry align="center" valign="center">
                <para>384.88</para>
              </entry>

              <entry align="center" valign="center">
                <para>461.29</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>466.95</para>
              </entry>

              <entry align="center" valign="center">
                <para>676.37</para>
              </entry>

              <entry align="center" valign="center">
                <para>752.78</para>
              </entry>

              <entry align="center" valign="center">
                <para>761.27</para>
              </entry>

              <entry align="center" valign="center">
                <para>761.27</para>
              </entry>

              <entry align="center" valign="center">
                <para>860.32</para>
              </entry>

              <entry align="center" valign="center">
                <para>863.15</para>
              </entry>

              <entry align="center" valign="center">
                <para>865.98</para>
              </entry>

              <entry align="center" valign="center">
                <para>1086.72</para>
              </entry>

              <entry align="center" valign="center">
                <para>1177.28</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>1185.77</para>
              </entry>

              <entry align="center" valign="center">
                <para>1214.07</para>
              </entry>

              <entry align="center" valign="center">
                <para>1273.5</para>
              </entry>

              <entry align="center" valign="center">
                <para>1327.27</para>
              </entry>

              <entry align="center" valign="center">
                <para>1341.42</para>
              </entry>

              <entry align="center" valign="center">
                <para>1364.06</para>
              </entry>

              <entry align="center" valign="center">
                <para>1468.77</para>
              </entry>

              <entry align="center" valign="center">
                <para>1652.72</para>
              </entry>

              <entry align="center" valign="center">
                <para>1689.51</para>
              </entry>

              <entry align="center" valign="center">
                <para>1765.92</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <para>Displayed in Figure 1.1.<link
    linkend="WPGeneratedID_TOC_1_47">13</link> is the GEV distribution on a
    Gumbel probability plot fitted using the Bayesian approach. Although the
    observed floods are largely contained within the 90% confidence limits,
    the fit, nonetheless, should be considered poor. The data has a sigmoidal
    trend with reverse curvature occurring for floods less than 1 in 2 AEP. It
    appears that the confidence limits have increased because the GEV fit
    provides a poor representation of the data.</para>

    <para/>

    <para>To deal with this problem, low flows can be censored to obtain a fit
    that favours the right hand tail (the high flows) of the distribution. The
    main idea is to force the assumed distribution to fit only the probability
    of low flows falling below a threshold rather than the low flow part of
    the distribution. As a result, the fitting of the assumed distribution is
    focussed on the right hand tail where the high flows occur. The censoring
    threshold can be obtained by trial-and-error but is better obtained by
    fitting LH moments as in Example <link
    linkend="WPGeneratedID_TOC_1_57">8</link>.</para>

    <para/>

    <para>Illustrated in Figure 1.1.<link
    linkend="WPGeneratedID_TOC_1_48">14</link> is one such fit. To
    de-emphasize the left hand tail the floods below the threshold of
    250m<superscript>3</superscript>/s were censored. This means the GEV
    distribution was fitted to a data record consisting of:</para>

    <para/>

    <orderedlist continuation="restarts" numeration="arabic">
      <listitem>
        <para>a gauged record comprising the 27 floods above
        250m<superscript>3</superscript>/s; and</para>
      </listitem>
    </orderedlist>

    <para/>

    <orderedlist continuation="continues" numeration="arabic">
      <listitem>
        <para>a censored record comprising the 23 floods below the threshold
        of 250m<superscript>3</superscript>/s and 0 floods above this
        threshold.</para>
      </listitem>
    </orderedlist>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEx5Fig2_1"/>

      <graphic align="center" depth="3.807in"
               fileref="ARRFFA/WPGeneratedName12.JPG" format="JPG"
               valign="top" width="4.920in"/>
    </para>

    <para>Figure 1.1.14 - Bayesian Fit with Floods Below 250m3/s Threshold
    Treated as Censored Observations</para>

    <para>The censored record provides an anchor point for the GEV
    distribution. It ensures that the chance of an annual maximum flood being
    less than 250m<superscript>3</superscript>/s is about 0.46 without forcing
    the GEV to fit the annual peak flows below the
    250m<superscript>3</superscript>/s threshold. The fit effectively
    disregards floods lower than 1 in 2 AEP and provides a good fit to the
    high flow tail. As shown in Figure 1.1.<link
    linkend="WPGeneratedID_TOC_1_48">14</link>, another benefit is the
    substantially reduced 90% confidence range.</para>

    <para/>

    <para/>

    <para/>

    <para><emphasis role="bold">EXAMPLE </emphasis><anchor
    id="WPGeneratedID_Xref_FFAEx6_1"/>6: EXAMPLE OF A NON-HOMOGENEOUS FLOOD
    PROBABILITY MODEL</para>

    <para/>

    <para>The work of Micevski <emphasis role="italic">et al</emphasis>.
    (2003) can be used to illustrate an example of a non-homogeneous model. An
    indicator time series based on the IPO time series (Figure 1.1.<link
    linkend="WPGeneratedID_TOC_1_16">6</link>) was used to create the
    exogenous vector x which is given by</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>FUNC { x ~ = ~ LEFT LBRACE ` I SUB { ` t } ` , ~ t ~ = ~ 1 ` , ~
      DOTSLOW ` , ~ n ` RIGHT RBRACE }</para>
    </section>

    <para>where the indicator I<subscript>t</subscript> is given by</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>FUNC { I SUB { ` t } ~ = ~ LEFT LBRACE ~ MATRIX { 1 ` , ~ &amp; if
      ~ IPO SUB { ` t } ~ &gt;= ~ IPO SUB { ` thresh } # 0 ` , ~ &amp; if ~
      IPO SUB { ` t } ~ &lt; ~ IPO SUB { ` thresh } } RIGHT . }</para>
    </section>

    <para>IPO<subscript>t</subscript> is the IPO index for year t and
    IPO<subscript>thresh</subscript> is a threshold value equal to
    –0.125.</para>

    <para>At each of the 33 NSW sites considered by Micevski <emphasis
    role="italic">et al</emphasis>. (2003) the annual maximum peak flows were
    stratified according to the indicator I<subscript>t</subscript>. A
    2-parameter log-normal distribution was fitted to the gauged flows with
    indicator equal to 1; this is the IPO+ distribution. Likewise, a
    2-parameter log-normal distribution was fitted to the gauged flows with
    indicator equal to 0; this is the IPO- distribution. Presented in Figure
    1.1.<link linkend="WPGeneratedID_TOC_1_50">15</link> is the histogram for
    the ratio of the IPO- and IPO+ floods for selected 1 in Y AEPs. If the the
    IPO+ and IPO- distributions were homogeneous then about half of the sites
    should have a flood ratio &lt; 1; consideration of the data shown in
    Figure 1.1.<link linkend="WPGeneratedID_TOC_1_50">15</link> suggests
    otherwise.</para>

    <para/>

    <para>Presented in Figures 1.1.<link
    linkend="WPGeneratedID_TOC_1_51">16</link> and 1.1.<link
    linkend="WPGeneratedID_TOC_1_52">17</link> are the fitted log-normal
    distributions for the IPO+ and IPO- annual maximum flood data for the
    Clarence river at Lilydale respectively. Though the adequacy of the
    log-normal model for high floods may be questioned, in the range of 1 in 2
    AEP to 1 in 10 AEP, the IPO- floods are about 2.6 times the IPO+ floods
    with the same AEP.</para>

    <para/>

    <para>To avoid bias in estimating long-term flood risk it is essential
    that the gauged record adequately span both IPO+ and IPO- years. In this
    example, the IPO+ record is 43 years and the IPO- record is 33 years in
    length. With reference to Figure 1.1.<link
    linkend="WPGeneratedID_TOC_1_16">6</link>, it would appear that this
    length of record has an adequate sample from both IPO epochs. As a result,
    fitting a distribution to all the data will yield an unbiased estimate of
    the long-term flood risk. Shown in Figure 1.1.<link
    linkend="WPGeneratedID_TOC_1_53">18</link> is a log normal distribution
    fitted to all the data.</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEx6Fig1_1"/>

      <graphic align="center" depth="3.292in"
               fileref="ARRFFA/WPGeneratedName13.JPG" format="JPG"
               valign="top" width="4.920in"/>
    </para>

    <para>Figure 1.1.15 - Histogram of IPO- and IPO+ Flood Ratios</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEx6Fig2_1"/>

      <graphic align="center" depth="4.485in"
               fileref="ARRFFA/WPGeneratedName14.JPG" format="JPG"
               valign="top" width="5.904in"/>
    </para>

    <para>Figure 1.1.16 - Log-Normal Fit to 43 Years of IPO+ Data for the
    Clarence River at Lilydale</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEx6Fig3_1"/>

      <graphic align="center" depth="4.485in"
               fileref="ARRFFA/WPGeneratedName15.JPG" format="JPG"
               valign="top" width="5.904in"/>
    </para>

    <para>Figure 1.1.17 - Log-Normal Fit to 33 Years of IPO- Data for the
    Clarence River at Lilydale</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEx6Fig4_1"/>

      <graphic align="center" depth="4.288in"
               fileref="ARRFFA/WPGeneratedName16.JPG" format="JPG"
               valign="top" width="5.904in"/>
    </para>

    <para>Figure 1.1.18 - Log-Normal Fit to 76 Years of Data for the Clarence
    River at Lilydale</para>

    <para>A better appreciation of the differences in flood risk can be
    gleaned by considering Figure 1.1.<link
    linkend="WPGeneratedID_TOC_1_54">19</link> which where the fitted
    log-normal distributions to the IPO+, IPO- and all the data are presented.
    During an IPO+ period a flood with peak discharge of
    100m<superscript>3</superscript>/s has a 1 in 20 AEP while during an IPO-
    period it has a 1 in 4 AEP. Likewise a flood with peak discharge of
    200m<superscript>3</superscript> /s has AEPs of 1 in 100 and 1 in 10 for
    IPO+ and IPO- periods respectively. The differences in flood risk are
    considerable. If a short gauged record falling largely in the IPO+ period
    was used, a standard flood frequency analysis could seriously
    underestimate the long-term or marginal flood risk.</para>

    <para/>

    <para>The marginal flood risk can be derived by combining the IPO+ and
    IPO- distribution using equation 1.1.<link
    linkend="WPGeneratedID_TOC_1_20">14</link> to give</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>FUNC { P ` ( ` Q ~ &lt;= ~ q ` LINE ` theta ` ) ~ = ~ P ` ( ` x ~
      = ~ 0 ` ) ` INT FROM 0 TO q ` p ` ( ` z ~ LINE ~ theta ` , ~ x ~ = ~ 0 `
      ) ` d ` z ~ + ~ P ` ( ` x ~ = ~ 1 ` ) ` INT FROM 0 TO q ` P ` ( ` z ~
      LINE ~ theta ` , ~ x ~ = ~ 1 ` ) ` d ` z }</para>
    </section>

    <para>The exogenous variable x can take two values, 0 or 1, depending on
    the IPO epoch. P(x = 0), the probability of being in an IPO- epoch, is
    assigned the value 33/76 based on the observation that 33 of the 76 years
    of record were in the IPO- epoch. Likewise P(x = 1), the probability of
    being in an IPO+ epoch, is assigned the value 43/76.</para>

    <para/>

    <para>The derived marginal distribution is plotted in Figure 1.1.<link
    linkend="WPGeneratedID_TOC_1_54">19</link>. It almost exactly matches the
    log normal distribution fitted to all the data.</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEx6Fig5_1"/>

      <graphic align="center" depth="3.805in"
               fileref="ARRFFA/WPGeneratedName17.JPG" format="JPG"
               valign="top" width="5.904in"/>
    </para>

    <para>Figure 1.1.19 - Marginal, IPO+ and IPO+ Log-Normal Distributions for
    the Clarence River at Lilydale</para>

    <para/>

    <para/>

    <para><emphasis role="bold">EXAMPLE </emphasis><anchor
    id="WPGeneratedID_Xref_FFAEx7_1"/>7: L MOMENTS FIT TO GAUGED DATA</para>

    <para/>

    <para>Illustrated in this example is the fitting of a GEV distribution to
    gauged data using L moments. The 47 ranked annual maximum flood series for
    the Styx River at Jeogla are:</para>

    <para/>

    <section label="page" role="textbox">
      <table>
        <title>Table_A</title>

        <tgroup cols="8">
          <colspec align="center" colname="Column1" colwidth="0.779in"/>

          <colspec align="center" colname="Column2" colwidth="0.779in"/>

          <colspec align="center" colname="Column3" colwidth="0.779in"/>

          <colspec align="center" colname="Column4" colwidth="0.779in"/>

          <colspec align="center" colname="Column5" colwidth="0.779in"/>

          <colspec align="center" colname="Column6" colwidth="0.779in"/>

          <colspec align="center" colname="Column7" colwidth="0.779in"/>

          <colspec align="center" colname="Column8" colwidth="0.779in"/>

          <tbody>
            <row>
              <entry align="center" valign="center">
                <para>878</para>
              </entry>

              <entry align="center" valign="center">
                <para>541</para>
              </entry>

              <entry align="center" valign="center">
                <para>521</para>
              </entry>

              <entry align="center" valign="center">
                <para>513</para>
              </entry>

              <entry align="center" valign="center">
                <para>436</para>
              </entry>

              <entry align="center" valign="center">
                <para>411</para>
              </entry>

              <entry align="center" valign="center">
                <para>405</para>
              </entry>

              <entry align="center" valign="center">
                <para>315</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>309</para>
              </entry>

              <entry align="center" valign="center">
                <para>300</para>
              </entry>

              <entry align="center" valign="center">
                <para>294</para>
              </entry>

              <entry align="center" valign="center">
                <para>258</para>
              </entry>

              <entry align="center" valign="center">
                <para>255</para>
              </entry>

              <entry align="center" valign="center">
                <para>235</para>
              </entry>

              <entry align="center" valign="center">
                <para>221</para>
              </entry>

              <entry align="center" valign="center">
                <para>220</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>206</para>
              </entry>

              <entry align="center" valign="center">
                <para>196</para>
              </entry>

              <entry align="center" valign="center">
                <para>194</para>
              </entry>

              <entry align="center" valign="center">
                <para>190</para>
              </entry>

              <entry align="center" valign="center">
                <para>186</para>
              </entry>

              <entry align="center" valign="center">
                <para>177</para>
              </entry>

              <entry align="center" valign="center">
                <para>164</para>
              </entry>

              <entry align="center" valign="center">
                <para>126</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>117</para>
              </entry>

              <entry align="center" valign="center">
                <para>111</para>
              </entry>

              <entry align="center" valign="center">
                <para>108</para>
              </entry>

              <entry align="center" valign="center">
                <para>105</para>
              </entry>

              <entry align="center" valign="center">
                <para>92.2</para>
              </entry>

              <entry align="center" valign="center">
                <para>88.6</para>
              </entry>

              <entry align="center" valign="center">
                <para>79.9</para>
              </entry>

              <entry align="center" valign="center">
                <para>74</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>71.9</para>
              </entry>

              <entry align="center" valign="center">
                <para>62.6</para>
              </entry>

              <entry align="center" valign="center">
                <para>61.2</para>
              </entry>

              <entry align="center" valign="center">
                <para>60.3</para>
              </entry>

              <entry align="center" valign="center">
                <para>58</para>
              </entry>

              <entry align="center" valign="center">
                <para>53.5</para>
              </entry>

              <entry align="center" valign="center">
                <para>39.1</para>
              </entry>

              <entry align="center" valign="center">
                <para>26.7</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>26.1</para>
              </entry>

              <entry align="center" valign="center">
                <para>23.8</para>
              </entry>

              <entry align="center" valign="center">
                <para>22.4</para>
              </entry>

              <entry align="center" valign="center">
                <para>22.1</para>
              </entry>

              <entry align="center" valign="center">
                <para>18.6</para>
              </entry>

              <entry align="center" valign="center">
                <para>13</para>
              </entry>

              <entry align="center" valign="center">
                <para>8.18</para>
              </entry>

              <entry align="center" valign="center"/>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <para>Reported in the following table are the results obtained. The GEV
    parameters are estimated by substituting the L moment estimates in
    equation 1.1.<link linkend="WPGeneratedID_TOC_1_31">45</link> to obtain κ
    and then using the equations in Table 1.1.<link
    linkend="WPGeneratedID_TOC_1_29">3</link> to estimate τ and α. The
    standard deviation and correlation were derived from 5000 bootstrapped
    samples following the procedure described in Section 6.4.6 of this Chapter
    in Book 1 of Australian Rainfall and Runoff.</para>

    <para/>

    <section label="page" role="textbox">
      <table>
        <title>TABLE A</title>

        <tgroup cols="8">
          <colspec align="left" colname="Column1" colwidth="0.783in"/>

          <colspec align="left" colname="Column2" colwidth="0.783in"/>

          <colspec align="left" colname="Column3" colwidth="0.783in"/>

          <colspec align="left" colname="Column4" colwidth="0.783in"/>

          <colspec align="left" colname="Column5" colwidth="0.783in"/>

          <colspec align="left" colname="Column6" colwidth="0.783in"/>

          <colspec align="left" colname="Column7" colwidth="0.783in"/>

          <colspec align="left" colname="Column8" colwidth="0.784in"/>

          <tbody>
            <row>
              <entry align="center" nameend="Column2" namest="Column1"
                     valign="center">
                <para>
                  <emphasis role="bold">L Moment Estimates</emphasis>
                </para>
              </entry>

              <entry align="center" nameend="Column4" namest="Column3"
                     valign="center">
                <para>
                  <emphasis role="bold">GEV Parameter Estimates</emphasis>
                </para>
              </entry>

              <entry align="center" valign="center">
                <para>
                  <emphasis role="bold">Std Deviation</emphasis>
                </para>
              </entry>

              <entry align="center" nameend="Column8" namest="Column6"
                     valign="center">
                <para>
                  <emphasis role="bold">Correlation</emphasis>
                </para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>FUNC { lambda HAT SUB 1 }</para>
              </entry>

              <entry align="center" valign="center">
                <para>189.238</para>
              </entry>

              <entry align="center" valign="center">
                <para>τ</para>
              </entry>

              <entry align="center" valign="center">
                <para>100.66</para>
              </entry>

              <entry align="center" valign="center">
                <para>17.657</para>
              </entry>

              <entry align="center" valign="center">
                <para>1</para>
              </entry>

              <entry align="center" valign="center"/>

              <entry align="center" valign="center"/>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>FUNC { lambda HAT SUB 2 }</para>
              </entry>

              <entry align="center" valign="center">
                <para>92.476</para>
              </entry>

              <entry align="center" valign="center">
                <para>α</para>
              </entry>

              <entry align="center" valign="center">
                <para>104.157</para>
              </entry>

              <entry align="center" valign="center">
                <para>15.554</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.597</para>
              </entry>

              <entry align="center" valign="center">
                <para>1</para>
              </entry>

              <entry align="center" valign="center"/>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>FUNC { lambda HAT SUB 3 }</para>
              </entry>

              <entry align="center" valign="center">
                <para>29.264</para>
              </entry>

              <entry align="center" valign="center">
                <para>κ</para>
              </entry>

              <entry align="center" valign="center">
                <para>-0.219</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.130</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.358</para>
              </entry>

              <entry align="center" valign="center">
                <para>0.268</para>
              </entry>

              <entry align="center" valign="center">
                <para>1</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <para>Presented in Figure 1.1.<link
    linkend="WPGeneratedID_TOC_1_56">20</link> is a probability plot of the
    gauged data along with GEV 1 in Y AEP quantiles and their 90% confidence
    limits. The confidence limits widen appreciably for the bigger floods;
    this is due largely to there being insufficient information to accurately
    infer the shape parameter κ.</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEx7Fig1_1"/>

      <graphic align="center" depth="4.173in"
               fileref="ARRFFA/WPGeneratedName18.JPG" format="JPG"
               valign="top" width="5.904in"/>
    </para>

    <para>Figure 1.1.20 - Probability Plot for Styx River at Jeogla</para>

    <para/>

    <para/>

    <para/>

    <para><emphasis role="bold"/>EXAMPLE <anchor
    id="WPGeneratedID_Xref_FFAEx8_1"/>8: IMPROVING POOR FITS USING LH
    MOMENTS</para>

    <para/>

    <para>This example is a continuation of Example 5. The GEV distribution
    was fitted to the gauged data for the Albert River at Broomfleet using LH
    moments.</para>

    <para/>

    <para>Displayed in Figure 1.1.<link
    linkend="WPGeneratedID_TOC_1_58">21</link> the GEV L moment fit on a
    Gumbel probability plot. Although the observed floods are largely
    contained within the 90% confidence limits, the fit, nonetheless, is poor
    with systematic departures from the data which exhibit reverse
    curvature.</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEx8Fig1_1"/>

      <graphic align="center" depth="5.037in"
               fileref="ARRFFA/WPGeneratedName19.JPG" format="JPG"
               valign="top" width="5.904in"/>
    </para>

    <para>Figure 1.1.21 - L Moment Fit</para>

    <para/>

    <para>To deal with this poor fit, an LH moment search was conducted to
    find the optimal value of the shift parameter η using the procedure
    described in Section 6.4.5. The optimal value of the shift parameter was
    found to be 4. Presented in Figure 1.1.<link
    linkend="WPGeneratedID_TOC_1_59">22</link> are the LH moment fit with
    shift η equal to 4. The fit effectively disregards floods with an AEP in
    excess of 1 in 2 and provides a very good fit to upper tail.</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEx8Fig2_1"/>

      <graphic align="center" depth="5.037in"
               fileref="ARRFFA/WPGeneratedName20.JPG" format="JPG"
               valign="top" width="5.904in"/>
    </para>

    <para>Figure 1.1.22 - LH Moment Fit with Shift η = 4</para>

    <para>The very significant reduction in the width of the quantile
    confidence intervals is largely due to the shape parameter κ changing from
    –0.17 to 0.50. The L moment fit in Figure 1.1.<link
    linkend="WPGeneratedID_TOC_1_58">21</link> was a compromise with the bulk
    of the small and medium-sized floods suggesting an upward curvature in the
    probability plot. As a result, the GEV shape parameter κ had to be
    negative to enable upward curvature. In contrast, the LH moment fit
    favoured the large-sized floods which exhibit a downward curvature
    resulting in a positive shape parameter. For positive κ the GEV has an
    upper bound. In this case the upper bound is about
    2070m<superscript>3</superscript>/s which is only 17% greater than the
    largest observed flood.</para>

    <para/>

    <para/>

    <para/>

    <para><emphasis role="bold"/>EXAMPLE 9: FITTING A PROBABILITY MODEL TO POT
    DATA</para>

    <para/>

    <para>An exponential distribution is fitted to a POT series based on
    gauged data for the Styx River at Jeogla. Listed in the table below are
    all the independent peak flows recorded over a 47 year period that
    exceeded a threshold of 74 m<superscript>3</superscript>/s; the total
    number of independent peaks obtained was 47. Comparison with the annual
    maximum flood peaks in Example 7 reveals that in 15 of the 47 years of
    record the annual maximum peak was below the threshold of 74
    m<superscript>3</superscript>/s.</para>

    <para/>

    <section label="page" role="textbox">
      <table>
        <title>Table_A</title>

        <tgroup cols="8">
          <colspec align="center" colname="Column1" colwidth="0.783in"/>

          <colspec align="center" colname="Column2" colwidth="0.783in"/>

          <colspec align="center" colname="Column3" colwidth="0.783in"/>

          <colspec align="center" colname="Column4" colwidth="0.783in"/>

          <colspec align="center" colname="Column5" colwidth="0.783in"/>

          <colspec align="center" colname="Column6" colwidth="0.783in"/>

          <colspec align="center" colname="Column7" colwidth="0.783in"/>

          <colspec align="center" colname="Column8" colwidth="0.788in"/>

          <tbody>
            <row>
              <entry align="center" valign="center">
                <para>878</para>
              </entry>

              <entry align="center" valign="center">
                <para>541</para>
              </entry>

              <entry align="center" valign="center">
                <para>521</para>
              </entry>

              <entry align="center" valign="center">
                <para>513</para>
              </entry>

              <entry align="center" valign="center">
                <para>436</para>
              </entry>

              <entry align="center" valign="center">
                <para>411</para>
              </entry>

              <entry align="center" valign="center">
                <para>405</para>
              </entry>

              <entry align="center" valign="center">
                <para>315</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>309</para>
              </entry>

              <entry align="center" valign="center">
                <para>301</para>
              </entry>

              <entry align="center" valign="center">
                <para>300</para>
              </entry>

              <entry align="center" valign="center">
                <para>294</para>
              </entry>

              <entry align="center" valign="center">
                <para>283</para>
              </entry>

              <entry align="center" valign="center">
                <para>258</para>
              </entry>

              <entry align="center" valign="center">
                <para>255</para>
              </entry>

              <entry align="center" valign="center">
                <para>255</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>238</para>
              </entry>

              <entry align="center" valign="center">
                <para>235</para>
              </entry>

              <entry align="center" valign="center">
                <para>221</para>
              </entry>

              <entry align="center" valign="center">
                <para>220</para>
              </entry>

              <entry align="center" valign="center">
                <para>206</para>
              </entry>

              <entry align="center" valign="center">
                <para>196</para>
              </entry>

              <entry align="center" valign="center">
                <para>194</para>
              </entry>

              <entry align="center" valign="center">
                <para>190</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>186</para>
              </entry>

              <entry align="center" valign="center">
                <para>164</para>
              </entry>

              <entry align="center" valign="center">
                <para>150</para>
              </entry>

              <entry align="center" valign="center">
                <para>149</para>
              </entry>

              <entry align="center" valign="center">
                <para>134</para>
              </entry>

              <entry align="center" valign="center">
                <para>129</para>
              </entry>

              <entry align="center" valign="center">
                <para>129</para>
              </entry>

              <entry align="center" valign="center">
                <para>126</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>119</para>
              </entry>

              <entry align="center" valign="center">
                <para>118</para>
              </entry>

              <entry align="center" valign="center">
                <para>117</para>
              </entry>

              <entry align="center" valign="center">
                <para>117</para>
              </entry>

              <entry align="center" valign="center">
                <para>111</para>
              </entry>

              <entry align="center" valign="center">
                <para>108</para>
              </entry>

              <entry align="center" valign="center">
                <para>105</para>
              </entry>

              <entry align="center" valign="center">
                <para>98</para>
              </entry>
            </row>

            <row>
              <entry align="center" valign="center">
                <para>92.2</para>
              </entry>

              <entry align="center" valign="center">
                <para>92.2</para>
              </entry>

              <entry align="center" valign="center">
                <para>91.7</para>
              </entry>

              <entry align="center" valign="center">
                <para>88.6</para>
              </entry>

              <entry align="center" valign="center">
                <para>85.2</para>
              </entry>

              <entry align="center" valign="center">
                <para>79.9</para>
              </entry>

              <entry align="center" valign="center">
                <para>74</para>
              </entry>

              <entry align="center" valign="center"/>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <para>A GP distribution with distribution function</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>FUNC { P ` ( ` Q ~ &lt;= ~ q ` LINE ` theta ` ) ~ = ~ 1 ~ - ~ LEFT
      ( ` 1 ~ - ~ { kappa ` ( ` q ~ - ~ q SUB * ` ) } OVER beta ` RIGHT ) SUP
      { 1 OVER kappa } }</para>
    </section>

    <para>was fitted to the POT data yielding the following estimates of the
    parameter values:</para>

    <para/>

    <para>β = 148.7,</para>

    <para>κ = -0.024, and</para>

    <para>q<subscript>*</subscript> = 73.999.</para>

    <para/>

    <para>Using equation 1.1.<link linkend="WPGeneratedID_TOC_1_7">7</link>,
    the average recurrence interval between two peaks exceeding a magnitude of
    w is</para>

    <para/>

    <section label="paragraph" role="legacy_equation">
      <para>FUNC { T SUB p ` ( ` w ` ) ~ = ~ 1 OVER { nu ` P ` ( ` Q ~ &gt; ~
      w ` ) } ~ = ~ 1 OVER { nu ~ LEFT [ ` 1 ~ - ~ { kappa ` ( ` w ~ - ~ q SUB
      * ` ) } OVER beta ` RIGHT ] SUP { 1 OVER kappa } } }</para>
    </section>

    <para>where ν is the average number of flood peaks above the threshold
    q<subscript>*</subscript> per year. Given that 47 peaks above the
    threshold occurred in 47 years, the best estimate of ν equals 1.0. Shown
    in Figure 1.1.<link linkend="WPGeneratedID_TOC_1_60">23</link> is a plot
    of the fitted POT exponential model against the observed POT series
    obtained using equation 1.1.?.</para>

    <para>
      <anchor id="WPGeneratedID_Xref_FFAEx9Fig1_1"/>

      <graphic align="center" depth="3.705in"
               fileref="ARRFFA/WPGeneratedName21.JPG" format="JPG"
               valign="top" width="4.723in"/>
    </para>

    <para>Figure 1.1.23- POT Probability Plot for Styx River at Jeogla</para>

    <para/>

    <para/>

    <para>
      <emphasis role="bold">REFERENCES</emphasis>
    </para>

    <para/>

    <para><emphasis role="italic"/>Adams, CA, (1987), Design flood estimation
    for ungauged rural catchments in Victoria Road Construction Authority,
    Victoria, Draft Technical Bulletin.</para>

    <para>Adams, CA and McMahon, TA, (1985), Estimation of flood discharge for
    ungauged rural catchments in Victoria, <emphasis role="italic">Proceedings
    1985 Hydrology and Water Resources Symposium</emphasis>, IEAust Natl Conf.
    Publ. No. 85/2, pp 86-90.</para>

    <para>Alexander, GN, (1957), Flood flow estimation, probability and the
    return period. Journal of Institution of Engineers Australia,
    29:263-278.</para>

    <para>Al-Futaisi, A, and Stedinger, JR, (1999), Hydrologic and Economic
    Uncertainties in Flood Risk Project Design, ASCE, Journal of Water
    Resources Planning and Management, 125(6):314-324.</para>

    <para>Allan, RJ, (2000), ENSO and Climatic Variability in the last 150
    years, in <emphasis role="italic">El Nino and the Southern Oscillation,
    Multi-Scale Variability, Global and Regional Impacts</emphasis>, edited by
    HF Diaz and V Markgraf, Cambridge University Press, Cambridge, UK,
    p3-56.</para>

    <para>American Society of Civil Engineers, (1949), Hydrology Handbook,
    <emphasis role="bold">publisher ?</emphasis></para>

    <para>Ashkanasy, NM and Weeks, WD, (1975), Flood Frequency Distribution in
    A Catchment Subject to Two Storm Rainfall Producing Mechanisms,
    Proceedings 1975 Hydrology Symposium, IEAust Natl Conf. Publ. No. 75/3, pp
    153-157.</para>

    <para>Ashkar, F and Rousselle, J, (1983), Some remarks on the truncation
    used in partial flood series models. Water Resources Research,
    19:477-480.</para>

    <para>
      <emphasis role="italic">Australian Rainfall and Runoff: A Guide to Flood
      Estimation, (1987), Pilgrim, D.H. (ed), The Institution of Engineers,
      Australia, Canberra.</emphasis>
    </para>

    <para>Baker, V, (1984), Recent paleoflood hydrology studies in arid and
    semi-arid environments (abstract). EOS Trans. Amer. Geophys. Union, 65:893
    - <emphasis role="bold">should we be referencing an abstract when people
    are unlikely to be able to obtain the full paper</emphasis>.</para>

    <para>Baker, V, Pickup, G and Polach, HA, (1983), Desert Paleofloods in
    Central Australia, <emphasis role="italic">Nature</emphasis>,
    301:502-504.</para>

    <para>Beard, LR, (1960), Probability Estimates Based in Small
    Normal-distribution Samples, <emphasis role="italic">Journal of
    Geophysical Research</emphasis>, 65:2143-2148.</para>

    <para>Beard, LR, (1974), Flood Flow Frequency Techniques, <emphasis
    role="italic">Tech. Report CRWR119</emphasis>, Center for Research in
    Water Resources, University of Texas at Austin, Austin, TX, USA.</para>

    <para>Beran, M, Hosking, JRM and Arnell, N, (1986), Comment on
    "Two-component extreme value distribution for flood frequency analysis",
    <emphasis role="italic">Water Resources Research</emphasis>,
    22:263-266.</para>

    <para>Blom, G, (1958), <emphasis role="italic">Statistical Estimates and
    Transformed Beta-Variables</emphasis>, Wiley, New York, NY, USA.</para>

    <para>Brown, JAH, (1983), <emphasis role="italic">Australia's Surface
    Water Resources - Water 2000: Consultants Report 1</emphasis>, Australian
    Government Publishing Service, Canberra.</para>

    <para>Clarke-Hafstad, K, (1942), Reliability of station-year
    rainfall-frequency determinations.,. Trans. ASCE, 107:633-652.</para>

    <para>Conway, KM, (1970), Flood Frequency Analysis of Some NSW Coastal
    Rivers, Unpublished M.Eng.Sc. Report, School of Civil Engineering, UNSW,
    Kensington, NSW, Australia.</para>

    <para>Costa, JE, (1978), Holocene stratigraphy in flood frequency
    analysis, <emphasis role="italic">Water Resources Research</emphasis>,
    14:626-632.</para>

    <para>Costa, JE, (1983), Palaeohydraulic reconstruction or flashflood
    peaks from boulder deposits in the Colorado Front Range, Geol. Soc.
    America Bulletin, 94:AP986-1004.</para>

    <para>Costa, JE, (1986), A History of Paleoflood Hydrology in the United
    States, 1800-1970. EOS Trans. AGU, 67(17):425-430.</para>

    <para>Cunnane, C, (1973), A Particular Comparison of Annual Maxima and
    Partial Duration Series Methods for Flood Frequency Prediction, Journal of
    Hydrology, 18:257-271.</para>

    <para>Cunnane, C, (1978), Unbiased Plotting Positions - A Review,
    <emphasis role="italic">Journal of Hydrology</emphasis>,
    37:205-222.</para>

    <para>Cunnane, C, (1985), Factors Affecting Choice of Distribution for
    Flood Series, <emphasis role="italic">Hydrological Sciences
    Journal</emphasis>, 30:25-36.</para>

    <para>Dalrymple, T, (1960), Flood-Frequency Analyses, Manual of Hydrology:
    Section 3 - Flood-Flow Techniques, <emphasis role="italic">U.S. Geological
    Survey Water Supply Paper 1543-A</emphasis>, <emphasis
    role="bold">?????</emphasis>, 79 p.</para>

    <para>DeGroot, MH, (1970), <emphasis role="italic">Optimal Statistcal
    Decisions</emphasis>, McGraw-Hill, <emphasis
    role="bold">?????</emphasis></para>

    <para>Doran, DG and Irish, JL, (1980), on the Nature and Extent of Bias in
    Flood Damage Estimation, <emphasis role="italic">Proc. 1980 Hydrology and
    Water Resources Symposium</emphasis>, IEAust Natl Conf. Publ. No. 80/9, pp
    135-139.</para>

    <para>Duan, Q, Sorooshian, S and Gupta, V, (1992), Effective and Efficient
    Global Optimization for Conceptual Rainfall-runoff Models, <emphasis
    role="italic">Water Resources Research</emphasis>, 28(4):1015-1032.</para>

    <para>Erskine, WD, and Warner, RF, (1988), Geomorphic Effects of
    Alternating Flood and Drought Dominated Regimes on a NSW Coastal River,
    <emphasis role="italic">in Fluvial Geomorphology of Australia</emphasis>,
    edited by R.F. Warner, Academic Press, Sydney, NSW, Australia, pp.
    223-244.</para>

    <para>Fiorentino, M, Versace, P and Rossi, F, (1985), Regional Flood
    Frequency Estimation Using the Two-component Extreme Value Distribution,
    Hydrological Sciences Journal, 30:51-64.</para>

    <para>Flavell, DJ, (1983), the Rational Method Applied to Small Rural
    Catchments in the South West of Western Australia, IEAust Civil
    Engineering Transactions, CE25:121-127.</para>

    <para>Franks, SW and Kuczera, G, (2002), Flood Frequency Analysis:
    Evidence and Implications of Secular Climate Variability, New South Wales,
    <emphasis role="italic">Water Resources Research</emphasis>,
    38(5):<emphasis role="bold">10.1029/2001WR000232</emphasis>.</para>

    <para>Gelman, A, Carlin, JB, Stern, HS and Rubin, DB, (1995), <emphasis
    role="italic">Bayesian Data Analysis</emphasis>, Chapman and Hall,
    p.526.</para>

    <para>Grayson, RB, Argent, RM, Nathan, RJ, McMahon, TA and Mein, RG,
    (1996), <emphasis role="italic">Hydrologic Recipes: Estimation Techniques
    in Australian Hydrology</emphasis>, <emphasis role="bold">CRC for
    Catchment Hydrology, ????.</emphasis></para>

    <para>Grygier, J C, Stedinger, JR and Yin, H-B, (1989), A Generalized
    Maintenance of Variance Extension Procedure for Extending Correlated
    Series, <emphasis role="italic">Water Resources Research</emphasis>,
    25(3):345-349.</para>

    <para>Harvey, F, Doran, DG, Cordery, I, and Pilgrim, DH, (1991), Regional
    Flood Frequency Characteristics in Rural New South Wales, <emphasis
    role="italic">Proc. 1991 International Hydrology and Water Resources
    Symposium</emphasis>, Perth, Australia, IEAust NCP 91/19, pp
    775-780.</para>

    <para>Hatfield, ER and Muir, GL, (1984), Surface Water Resources
    Assessment in New South Wales, <emphasis role="italic">Proc. Workshop on
    Surface Water Resources Data</emphasis>, AWRC Conference Series No. 10, pp
    263-292.</para>

    <para>Hirsch, RM, (1982), A Comparison of Four Record Extension
    Techniques, <emphasis role="italic">Water Resources Research</emphasis>,
    18(4):1081-1088.</para>

    <para>Hosking, JRM, (1990), L-Moments: Analysis and Estimation of
    Distributions Using Linear Combinations of Order Statistics, <emphasis
    role="italic">Journal of Royal Statistical Society, Series B</emphasis>,
    52(2):105-124.</para>

    <para>Hosking, JRM and Wallis, JR, (1986), Paleoflood Hydrology and Flood
    Frequency Analysis, <emphasis role="italic">Water Resources
    Research</emphasis>, 22:543-550.</para>

    <para>Houghton, JC, (1978), Birth of a Parent: the Wakeby Distribution for
    Modeling Flood Flows, <emphasis role="italic">Water Resources
    Research</emphasis>, 14:1105-1109.</para>

    <para>Interagency Advisory Committee on Water Data, (1982), Guidelines for
    Determining Flood Flow Frequency, <emphasis role="italic">Bulletin 17B of
    the Hydrology Sub-Committee</emphasis>, Office of Water Data Coordination,
    Geological Survey, US Dept of the Interior, ????</para>

    <para>Jayasuriya, MDA and Mein, RG, (1985), Frequency Analysis Using the
    Partial Series, <emphasis role="italic">Proc. 1985 Hydrology and Water
    Resources Symposium</emphasis>, IEAust. Natl. Conf. Publ. No. 85/2, pp.
    81-85.</para>

    <para>Jin, M and Stedinger, JR, (1989), Flood Frequency Analysis with
    Regional and Historical Information, <emphasis role="italic">Water
    Resources Research</emphasis>, 25(5):925-936.</para>

    <para>Kiem, A S, and Franks, SW, (2004), Multi-decadal Variability of
    Drought Risk - Eastern Australia, <emphasis role="italic">Hydrological
    Processes</emphasis>, 18:<emphasis
    role="bold">doi:10.1002/hyp.1460.</emphasis></para>

    <para>Kiem, AS, Franks, SW and Kuczera, G, (2003), Multi-decadal
    Variability of Flood Risk, <emphasis role="italic">Geophysical Research
    Letters</emphasis>, 30(2):<emphasis role="bold">1035,
    DOI:10.1029/2002GL015992.</emphasis></para>

    <para>Kochel. RC, Baker, VR and Patton, PC, (1982), Paleohydrology of
    Southwestern Texas, <emphasis role="italic">Water Resources
    Research</emphasis>, 18:1165-1183.</para>

    <para>Kopittke, RA., Stewart, BJ and Tickle, KS, (1976), Frequency
    Analysis of Flood Data in Queensland. <emphasis role="italic">Proc. 1976
    Hydrology Symposium</emphasis>, IEAust. Natl Conf. Publ. No. 76/2, pp.
    20-24.</para>

    <para>Kuczera, G, (1999), Comprehensive At-site Flood Frequency Analysis
    Using Monte Carlo Bayesian Inference, <emphasis role="italic">Water
    Resources Research</emphasis>, 35(5):1551-1558.</para>

    <para>Kuczera, G, Lambert, M, Heneker, T, Jennings, S, Frost, A and
    Coombes, P, (2003), Joint Probability and Design Storms at the Crossroads,
    <emphasis role="italic">Proc 2003 Hydrology and Water Resources
    Symposium</emphasis>, IEAust, Wollongong, pp <emphasis
    role="bold">????</emphasis></para>

    <para>Lee, PM, (1989), <emphasis role="italic">Bayesian Statistics: An
    Introduction</emphasis>, Oxford University Press, New York, NY,
    USA.</para>

    <para>Laurenson, EM, (1987), Back to Basics on Flood Frequency Analysis,
    IEAust Civil Engineering Transactions, CE29:47-53.</para>

    <para>Madsen, H, Pearson, CP, Rasmussen, PF and Rosbjerg, D, (1997),
    Comparison of Annual Maximum Series and Partial Duration Series Methods
    for Modeling Extreme Hydrologic Events 1 - At-Site Modeling, <emphasis
    role="italic">Water Resources Research</emphasis>, 33(4):747-758.</para>

    <para>Mantua, NJ, Hare, SR, Zhang, Y, Wallace, JM and Francis, RC, (1997),
    A Pacific Interdecadal Climate Oscillation with Impacts on Salmon
    Production, Bulletin American Meteorological Society,
    78(6):1069-1079.</para>

    <para>Martins, E S and Stedinger, JR, (2000), Generalized Maximum
    Likelihood GEV Quantile Estimators for Hydrologic Data, <emphasis
    role="italic">Water Resources Research</emphasis>, 36(3):737-744.</para>

    <para>Martins, E S and Stedinger, JR, (2001), Generalized Maximum
    Likelihood Pareto-Poisson Flood Risk Analysis for Partial Duration Series,
    Water Resources Research, 37(10):2559-2567.</para>

    <para>Matalas, NC and Jacobs, B, (1964), A Correlation Procedure for
    Augmenting Hydrologic Data, <emphasis role="italic">US Geological Survey
    Professional Paper 434-E</emphasis>, <emphasis
    role="bold">????</emphasis></para>

    <para>McDermott, GE and Pilgrim, DH, (1982), Design Flood Estimation for
    Small Catchments in New South Wales, <emphasis role="italic">Australian
    Water Resources Council Technical Paper No. 73</emphasis>, Dept of
    National Development and Energy, <emphasis role="bold">???</emphasis>, 233
    p.</para>

    <para>McDermott, GE and Pilgrim, DH, (1983), A Design Flood Method for
    Arid Western New South Wales Based on Bankfull Estimates, <emphasis
    role="italic">IEAust, Civil Engineering Transaction</emphasis>,
    CE25:114-120.</para>

    <para>McIllwraith, JF, (1953), Rainfall Intensity-Frequency Data for New
    South Wales Stations. Journal Institution of Engineers Australia,
    25:133-139.</para>

    <para>McMahon, TA, (1979), Hydrologic Characteristics of Australian
    Streams, <emphasis role="italic">Report No. 3/1979</emphasis>, Civil
    Engineering Research Reports, Monash University, Clayton, Vic,
    Australia.</para>

    <para>McMahon, TA and Srikanthan, R, (1981), Log Pearson III Distribution
    - is it Applicable to Flood Frequency Analysis of Australian Streams?,
    <emphasis role="italic">Journal of Hydrology</emphasis>,
    52:139-147.</para>

    <para>Micevski, T, Kiem, AS, Franks, SW and Kuczera, G, (2003),
    Multidecadal Variability in New South Wales Flood Data, <emphasis
    role="italic">Proc. 2003 Hydrology and Water Resources
    Symposium</emphasis>, Institution of Engineers, Australia, Wollongong, pp
    ?.</para>

    <para>Micevski T, Kuczera, G, Franks, SW, (2005), Flood Frequency
    Censoring Errors Associated with Daily-read Flood Observations, <emphasis
    role="italic">Water Resources Research</emphasis>, 41:?</para>

    <para>Natural Environment Research Council (1975), Flood Studies Report,
    1, Hydrological Studies. London.</para>

    <para>O'Connell, DR, Ostemaa, DA, Levish, DR and Klinger, RE, (2002),
    Bayesian Flood Frequency Analysis with Paleohydrologic Bound Data,
    <emphasis role="italic">Water Resources Research</emphasis>,
    38(5):1058-?</para>

    <para>Pilgrim, DH and Doran, DG, (1987), Flood Frequency
    Analysis,<emphasis role="italic"> in Australian Rainfall and Runoff: A
    Guide to Flood Estimation</emphasis>, Pilgrim, DH (ed), The Institution of
    Engineers, Australia, Canberra.</para>

    <para>Pilgrim, DH and McDermott, GE, (1982), Design Floods for Small Rural
    Catchments in Eastern New South Wales, <emphasis role="italic">IEAust
    Civil Engineering Transactions</emphasis>, CE24:226-234.</para>

    <para>Potter, DJ and Pilgrim, DH, (1971), Flood Estimation Using A
    Regional Flood Frequency Approach, Final Report, Vol 2: Report on Analysis
    Components, Australian Water Resources Council, Research Project 68/1,
    Hydrology of Small Rural Catchments, Snowy Mountains Engineering
    Corporation.</para>

    <para>Potter, KW and Walker, JF, (1981), A Model of Discontinuous
    Measurement Error and its Effects on the Probability Distribution of Flood
    Discharge Measurements, <emphasis role="italic">Water Resources
    Research</emphasis>, 17(5):1505-1509.</para>

    <para>Potter, KW and Walker, JF, (1985), An Empirical Study of Flood
    Measurement Error, <emphasis role="italic">Water Resources.
    Research</emphasis>, 21(3):403-406.</para>

    <para>Power, S, Tseitkin, F, Torok, S, Lavery, B, Dahni, R, and McAvaney,
    B, (1998), Australian Temperature, Australian Rainfall and the Southern
    Oscillation, 1910-1992: Coherent Variability and Recent Changes, <emphasis
    role="italic">Australian Meteorology Magazine</emphasis>,
    47(2):85-101.</para>

    <para>Power, S, Casey, T, Folland, C, Colman, A and Mehta, V, (1999),
    Inter-Decadal Modulation of the Impact of ENSO on Australia, <emphasis
    role="italic">Climate Dynamics</emphasis>, 15(5):319-324.</para>

    <para>Rosbjerg, D, Madsen, H and Rasmussen, PF, (1992), Prediction in
    Partial Duration Series with Generalized Pareto-Distributed Exceedances,
    <emphasis role="italic">Water Resources Research</emphasis>,
    28(11):3001-3010.</para>

    <para>Rossi, F, Fiorentino, M and Versace, P, (1984), Two-Component
    Extreme Value Distribution for Flood Frequency Analysis, <emphasis
    role="italic">Water Resources Research</emphasis>, 20:847-856.</para>

    <para>Slack, JR, Wallis, JR and Matalas, NC, (1975), on the Value of
    Information in Flood Frequency Analysis, <emphasis role="italic">Water
    Resources. Research</emphasis>, 11(5):629-648.</para>

    <para>Stedinger, JR, (1983), Design Events with Specified Flood Risk,
    <emphasis role="italic">Water Resources Research</emphasis>,
    19(2):511-522.</para>

    <para>Stedinger, JR, Vogel, RM and Foufoula-Georgiou, E, (1993), Frequency
    Analysis of Extreme Events in <emphasis role="italic">Handbook of
    Hydrology</emphasis>, Maidment, DR (ed.), McGraw-Hill, New York, NY,
    USA.</para>

    <para>Stedinger, JR and Cohn, TA, (1986), Flood Frequency Analysis with
    Historical and Paleoflood Information, <emphasis role="italic">Water
    Resources Research</emphasis>, 22:785-793.</para>

    <para>Stedinger, JR, (1998), Expected Probability and Annual Damage
    Estimators, <emphasis role="italic">ASCE, Journal of Water Resources
    Planning and Management</emphasis>, 123(2):125-35 (with discussion, LR
    Beard (1998), <emphasis role="italic">ASCE, Journal of Water Resources
    Planning and Management</emphasis>, 124(6):365-366).</para>

    <para>Tavares, LV, and da Silva, JE, (1983), Partial Series Method
    Revisited, <emphasis role="italic">Journal of Hydrology</emphasis>,
    64:1-14.</para>

    <para>Wallis, JR and Wood, EF, (1985), Relative Accuracy Log Pearson III
    Procedures. <emphasis role="italic">ASCE, Journal of Hydraulic
    Engineering</emphasis>, lll(7):1043-10<emphasis
    role="bold">??</emphasis></para>

    <para>Wang, QJ, (1996), Direct Sample Estimators of L-Moments, <emphasis
    role="italic">Water Resources Research</emphasis>,
    32(12):3617-3619.</para>

    <para>Wang, QJ, (1997), LH Moments for Statistical Analysis of Extreme
    Events, <emphasis role="italic">Water Resources Research</emphasis>,
    33(12):2841-2848.</para>

    <para>Wang, QJ, (1998), Approximate Goodness-of-Fit Tests of Fitted
    Generalized Extreme Value Distributions Using LH Moments, <emphasis
    role="italic">Water Resources Research</emphasis>,
    34(12):3497-3502.</para>

    <para>Wang, QJ, (1999), <emphasis role="bold">????</emphasis></para>

    <para>Wang, QJ, (2001), A Bayesian Joint Probability Approach for Flood
    Record Augmentation, <emphasis role="italic">Water Resources
    Research</emphasis>, 37(6):1707-1712.</para>

    <para>Waylen, P and Woo, M-K, (1982), Prediction of Annual Floods
    Generated by Mixed Processes, <emphasis role="italic">Water Resources
    Research</emphasis>, 18(4):1283-1286.</para>
  </section>
</chapter>
