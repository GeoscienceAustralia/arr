<?xml version="1.0" encoding="UTF-8"?>
<section status="In Preparation" version="5.0" xml:id="b3_ch2_s9"
         xmlns="http://docbook.org/ns/docbook"
         xmlns:xlink="http://www.w3.org/1999/xlink"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:svg="http://www.w3.org/2000/svg"
         xmlns:m="http://www.w3.org/1998/Math/MathML"
         xmlns:html="http://www.w3.org/1999/xhtml"
         xmlns:db="http://docbook.org/ns/docbook">
  <title>Supplementary Information</title>

  <section xml:id="b3_ch2_s_lg0c6">
    <title>Example 1: Extrapolation and Process Understanding</title>

    <para>The importance of process understanding when extrapolating beyond
    the observed record is illustrated by a simple Monte Carlo experiment. A
    Poisson rectangular pulse rainfall model is used to generate a long record
    of high resolution rainfall. This is routed through a rainfall-runoff
    model to generate runoff into the stream system. The storage-discharge
    relationship for the stream is depicted by the bilinear relationship shown
    in <xref linkend="b3_ch2_f_hpsm2"/>. A feature of this relationship is the
    activation of significant flood terrace storage once a threshold discharge
    is exceeded.</para>

    <figure xml:id="b3_ch2_f_hpsm2">
      <title>Bilinear channel storage-discharge relationship</title>

      <mediaobject>
        <imageobject>
          <imagedata width="100%" fileref="../../figures/3010.jpg"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>The routing model parameters were selected so that major flood terrace storage is
      activated by floods of less than 1 in 100 AEP. This situation was chosen to represent a river
      with multiple flood terraces with the lowest terraces accommodating the majority of floods and
      the highest terrace only inundated by extreme floods.</para>

    <para><xref linkend="b3_ch2_f_owuiv"/> presents the flood frequency curve based on 30 000
      simulated years – it shows a clear break in slope around the 1 in 100 AEP corresponding to the
      activation of major flood terrace storage. Indeed the flood frequency curve displays downward
      curvature despite that the fact the rainfall frequency curve displays upward curvature in the
      1 in 100 to 1 in 1000 AEP range. In contrast the flood frequency curve based on 100 years of
      “data” shows no evidence of downward curvature. This is because in a 100 year record there is
      little chance of the major flood terrace storage being activated. Indeed without knowledge of
      the underlying hydraulics one would be tempted to extrapolate the 100 year flood record using
      a straight line extrapolation. Such an extrapolation would rapidly diverge from the “true”
      frequency curve.</para>

    <figure xml:id="b3_ch2_f_owuiv">
      <title>Simulated rainfall and flood frequency curves with major
      floodplain storage activated at a threshold discharge of 3500
      m<superscript>3</superscript>/s</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="../../figures/3011.png" scalefit="1"
                     width="100%"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>Although the example idealises the dominant rainfall-runoff dynamics it delivers a very
      strong message. Extrapolation of flood frequency curves fitted to gauged discharges records
      requires the exercise of hydrologic judgment backed up by appropriate modelling. The problem
      of extrapolation is much more general. For example, in this case, if a rainfall-runoff
      approach were used with the rainfall-runoff model calibrated to small events the simulated
      flood frequency curve is likely to be compromised in a similar way.</para>
  </section>

  <section xml:id="b3_ch2_s_cli4f">
    <title>Example 2: Accuracy of Daily Gauged Discharges</title>

    <para>The use of daily discharge readings in Flood Frequency Analysis is
    most problematic for smaller catchments, which can be “flashy” in the
    sense that the hydrograph can rise and subside within a twenty four hour
    period. This effect can be quite significant, even for reasonably large
    catchments.</para>

    <para><xref linkend="b3_ch2_f_097da"/> and <xref linkend="b3_ch2_f_mvr25"/>, taken from
        <citation>b3_c2_r56+1</citation>, compare instantaneous annual maximum discharge against the
      discharge recorded at 9am on the same day for two gauging stations in the Hunter Valley:
      Goulburn River at Coggan with area 3340 km<superscript>2</superscript> and Hunter River at
      Singleton with area 16 400 km<superscript>2</superscript>. The dashed line represents
      equality. <xref linkend="b3_ch2_f_097da"/> demonstrates that the true peak flow can be up to
      10 times the 9:00 am flow. In contrast the estimation error is much smaller for the larger
      catchment shown in <xref linkend="b3_ch2_f_mvr25"/>.</para>

    <figure xml:id="b3_ch2_f_097da">
      <title>Comparison between true peak flow and 9:00 am flow for Goulburn River at Coggan</title>

      <mediaobject>
        <imageobject>
          <imagedata width="100%" fileref="../../figures/3012.png"/>
        </imageobject>
      </mediaobject>
    </figure>

    <figure xml:id="b3_ch2_f_mvr25">
      <title>Comparison between true peak flow and 9:00 am flow for Hunter River at
        Singleton</title>

      <mediaobject>
        <imageobject>
          <imagedata width="100%" fileref="../../figures/3013.png"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>The example demonstrates the need to check the representativeness of
    daily readings by comparing instantaneous peak flows against daily
    readings.</para>
  </section>

  <section xml:id="b3_ch2_s_o1s2n">
    <title>Example 3: Fitting a probability model to gauged data</title>

    <section xml:id="b3_ch2_s_400aj">
      <title>Launch TUFLOW Flike</title>

      <para>This example demonstrates undertaking a flood frequency analysis using the procedures
        described in this book. Specifically, this example covers the fitting of a Log Pearson Type
        III distribution to an annual maximum series for the Hunter River at Singleton. The analysis
        will be undertaken using TUFLOW Flike which has been developed to undertake flood frequency
        analysis as described in this book, that is, it has the ability to fit a range of
        statistical distributions using a Bayesian Inference method.</para>

      <para>Once TUFLOW Flike has been obtained and installed, launch
      <emphasis role="bold">TUFLOW Flike</emphasis> and the screen in <xref
      linkend="b3_ch2_f_77nti"/> will appear.</para>

      <figure xml:id="b3_ch2_f_77nti">
        <title>TUFLOW Flike Splash Screen</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3054.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section xml:id="b3_ch2_s_nv5b5">
      <title>Create the .fld file</title>

      <para>The first step will be to create the <emphasis
      role="bold">.fld</emphasis> file which contains information about the
      project. To create a new <emphasis role="bold">.fld</emphasis> file,
      select <emphasis role="bold">New</emphasis> from the <emphasis
      role="bold">File</emphasis> dropdown menu. This will open a new window
      called <emphasis role="bold">Open</emphasis> as shown in <xref
      linkend="b3_ch2_f_prmm3"/>.</para>

      <figure xml:id="b3_ch2_f_prmm3">
        <title>Create New .fld file</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3055.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Create and save a new .fld file in an appropriate location, such
      as in a folder under the job directory, and give it a logical name, in
      this case <emphasis role="bold">Example_3.fld</emphasis>. A message will
      appear asking if you want to create the file, select <emphasis
      role="bold">Yes</emphasis>. Note that the window is titled <emphasis
      role="bold">Open</emphasis>, but it works for creating new files as
      well. Once the <emphasis role="bold">.fld</emphasis> file has been
      saved, the <emphasis role="bold">Flike Editor</emphasis> window will
      open which will be used in the next step.</para>
    </section>

    <section xml:id="b3_ch2_s_zkskh">
      <title>Configure the Project Details</title>

      <para>The <emphasis role="bold">.fld</emphasis> file is used to store
      the project data and configuration. Once the <emphasis
      role="bold">.fld</emphasis> has been created the <emphasis
      role="bold">Flike Editor</emphasis> window will open automatically (see
      <xref linkend="b3_ch2_f_ha723"/>) and the project will be configured
      here. The first bit of information to be completed is the project a name
      which is filled in the <emphasis role="bold">Title</emphasis> text box.
      The project title can go over two lines.</para>

      <figure xml:id="b3_ch2_f_ha723">
        <title>Flike Editor Screen</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3015.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section xml:id="b3_ch2_s_ylho0">
      <title>Import the Data</title>

      <para>The next step is to import the flood series to analyse. To do this
      select the <emphasis role="bold">Observed values</emphasis> tab in the
      <emphasis role="bold">Flike Editor</emphasis> as shown in <xref
      linkend="b3_ch2_f_axe49"/>. In this tab the flood series to be
      investigated will be imported.</para>

      <figure xml:id="b3_ch2_f_axe49">
        <title>Observed Values Screen</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3014.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>To import the flood series select the <emphasis
      role="bold">Import</emphasis> button and the <emphasis
      role="bold">Import gauged values</emphasis> window opens as shown in
      <xref linkend="b3_ch2_f_7evjt"/>. Now select the <emphasis
      role="bold">Browse</emphasis> button and navigate to the Singleton flood
      series. This example data are included in the TUFLOW Flike download, a
      copy of which was installed in the <emphasis role="bold">data
      </emphasis>folder in the install location of TUFLOW-Flike. By default,
      this location is <emphasis role="italic">C:\TUFLOW
      Flike\data\singletonGaugedFlows.csv</emphasis>. This data also appears
      at the end of this example.</para>

      <figure xml:id="b3_ch2_f_7evjt">
        <title>Import Gauged Values Screen</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3056.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Once the data file has been selected, the program will return to
      the <emphasis role="bold">Import gauged values</emphasis> window. As the
      input data format is flexible TUFLOW Flike needs to be told how to
      interpret the data file. To view the format of the data, select the
      <emphasis role="bold">View</emphasis> button and the data will be open
      in your default text editor (see <xref linkend="b3_ch2_f_xb2ll"/>). In
      the example data the first line contains a header line and the data
      follows this. The flow values are in the first column and the year in
      the fourth column. Having taken note of the data structure close the
      text editor and return to the <emphasis role="bold">Import gauged
      values</emphasis> window. It's a good habit to check the data in the
      text editor to ensure that the format of the data is known and the file
      has not been corrupted or includes a large number of trailing comma or
      whitespace. This last issue commonly occurs when deleting information
      from excel files, but it is easy to fix. Simply delete any trailing
      comma or white space in a text editor.</para>

      <figure xml:id="b3_ch2_f_xb2ll">
        <title>View Gauged Values in Text Editor</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3057.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The next step is to configure the import of the data. As the
      example data has a header, the first line needs to be skipped. Enter
      <emphasis role="bold">1</emphasis> into the <emphasis role="bold">Skip
      first __ records and then text field</emphasis>. This will skip the
      first line. Ensure that the <emphasis role="bold">Read to the
      end-of-file</emphasis> option is selected (this is the default).
      Occasionally, there may be a need to specify how many records to read,
      in which case this can be achieved by selecting the <emphasis
      role="bold">Read next __ records</emphasis> option and entering the
      desired number of records to read. Next, specify which column the flood
      data are in, by filling the <emphasis role="bold">gauged values are in
      column __ </emphasis>text box, in this example data this is column 1.
      Next, select the <emphasis role="bold">Years available in column
      __</emphasis> text box and specify the column that this data is in
      (column 4). Finally, select <emphasis role="bold">OK</emphasis> to
      import the data. The <emphasis role="bold">Import gauged
      values</emphasis> window should look similar to <xref
      linkend="b3_ch2_f_7evjt"/>.</para>

      <para>The <emphasis role="bold">Value</emphasis> and <emphasis
      role="bold">Year</emphasis> columns in the <emphasis
      role="bold">Observed values</emphasis> tab will now be filled with the
      data in the order that they were in the data file as shown in <xref
      linkend="b3_ch2_f_1cjit"/>. The data can be sorted by value and year
      using the <emphasis role="bold">Rank</emphasis> button. Selecting this
      button will open a new window (<xref linkend="b3_ch2_f_yrxtw"/>) where
      there are five choices to rank by, these are:</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">Descending flow: </emphasis>Ranks the
          data in order of values from largest to smallest</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Ascending flow: </emphasis>Ranks the
          data in order of values from smallest to largest</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Descending year:</emphasis> Ranks the
          data in order of year from largest to highest</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Ascending year: </emphasis>Ranks the
          data in order of year from highest to largest</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Leave unchanged </emphasis>: Leaves both
          the values and years unchanged</para>
        </listitem>
      </itemizedlist>

      <para>It is always a good idea to initially rank your data in descending order so you can
        check the largest flows. For this data series the value is 12 525.66
          m<superscript>3</superscript>/s. Leave the data ranked in descending order for this
        example.</para>

      <para>Note that the value name and units can be specified by entering
      values in the <emphasis role="bold">Value</emphasis><emphasis
      role="bold">name</emphasis> and <emphasis role="bold">Unit</emphasis>
      text boxes. These titles do not affect the computations in any way, they
      do, however, assist in reviewing the results, particularly when
      presenting results to external audiences.</para>

      <figure xml:id="b3_ch2_f_1cjit">
        <title>Observed Values screen with Imported Data</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3058.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <figure xml:id="b3_ch2_f_yrxtw">
        <title>Rank Data Screen</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3059.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section xml:id="b3_ch2_s_5inth">
      <title>Configure the distribution and fit method</title>

      <para>Now that the data has been imported the statistical distribution
      can be fitted to the data. To do this, select the <emphasis
      role="bold">General</emphasis> tab. As noted above, for this example the
      Log Pearson Type III distribution will be fitted using the Bayesian
      Inference method.</para>

      <para>Before configuring the model it is worthwhile checking that TUFLOW
      Flike has interpreted the data correctly. The number of observed data is
      reported in the <emphasis role="bold">Number of observed data</emphasis>
      text box. In this case the number of observations or length of the data
      series is 31 as shown in <xref linkend="b3_ch2_f_56hql"/>. Before
      continuing, check that this is the case.</para>

      <para>Next, select the probability model; the Log Pearson Type III. To do this ensure that the
        radio button next to the text <emphasis role="bold">Log Pearson Type III (LP3)</emphasis> is
        selected (this is the default) as in <xref linkend="b3_ch2_f_56hql"/>.</para>

      <para>The final task is to choose the fitting method. In this example
      the Bayesian Inference method will be used. To do this, ensure that the
      radio button next to <emphasis role="bold">Bayesian with</emphasis> is
      selected and the radio button next to <emphasis role="bold">No prior
      information</emphasis> is selected as shown in <xref
      linkend="b3_ch2_f_56hql"/>. Again, both of these are the
      defaults.</para>

      <figure xml:id="b3_ch2_f_56hql">
        <title>General Screen – After Data Import</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3060.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section xml:id="b3_ch2_s_6za5s">
      <title>Running TUFLOW Flike and accessing Results</title>

      <para>TUFLOW Flike presents the results in two ways:</para>

      <itemizedlist>
        <listitem>
          <para>As a visual plot; and</para>
        </listitem>

        <listitem>
          <para>In a text based report file.</para>
        </listitem>
      </itemizedlist>

      <para>Both of these will be explored in this example and both should be consulted when
        undertaking a Flood Frequency Analysis. Before we proceed with this example the length of
        the x-axis in the plot needs to be specified; that is, the lowest probability (rarest event)
        to be displayed. It is recommended to always enter a value greater than the 1 in Y AEP event
        that you are interested in. This is specified in the <emphasis role="bold">Maximum AEP 1 in
          Y in probability plot ___ years</emphasis> text box. In this example, enter the 1 in 200
        year AEP event as shown in <link xlink:href="http://localhost:8889/Fit.model.png">Figure
          9</link>. By default the plot window automatically launches when a distribution is
        fitted.</para>

      <para>In addition to the plot window a report file can also be
      automatically launched in a text editor. This can be quite helpful when
      you are developing a model, as it allows you to more readily compare the
      results. To do this select the appropriate radio button next to
      <emphasis role="bold">Always display report file</emphasis> as shown in
      <xref linkend="b3_ch2_f_56hql"/>.</para>
    </section>

    <section xml:id="b3_ch2_s_kw5dq">
      <title>Run TUFLOW Flike</title>

      <para>Now that the data has been imported, the distribution selected,
      the fit method configured and the output configured TUFLOW Flike is
      ready to run. To fit the model select <emphasis
      role="bold">OK</emphasis> on the <emphasis
      role="bold">General</emphasis> tab and this will return you to the
      <emphasis role="bold">TUFLOW Flike</emphasis> window, which will look
      quite empty as in <xref linkend="b3_ch2_f_xprrq"/>. In this window,
      select the <emphasis role="bold">Option</emphasis> dropdown menu and
      choose <emphasis role="bold">Fit</emphasis><emphasis
      role="bold">model</emphasis>. This will run TUFLOW-Flike and present you
      with a <emphasis role="bold">Probability Plot </emphasis>as well as
      opening the <emphasis role="bold">Report File</emphasis> in a text
      editor.</para>

      <figure xml:id="b3_ch2_f_xprrq">
        <title>Blank TUFLOW-Flike Screen</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3061.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section xml:id="b3_ch2_s_wb72d">
      <title>Reviewing the results</title>

      <para>When TUFLOW-Flike has finished fitting the distribution to the
      input data, a plot screen will appear similar to <xref
      linkend="b3_ch2_f_mrvdm"/> and the results file will be shown in the
      default text editor as in <xref linkend="b3_ch2_f_t2d48"/>.</para>

      <figure xml:id="b3_ch2_f_mrvdm">
        <title>Probability Plot</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3016.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <figure xml:id="b3_ch2_f_t2d48">
        <title>Results File</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3062.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>When fitting a flood series to a probability distribution it is
      essential that the results are viewed and reviewed. This is most easily
      achieved by first viewing the results in the <emphasis
      role="bold">Probability Plot</emphasis>. If the <emphasis
      role="bold">Probability Plot</emphasis> window has been closed, it can
      be reopened by selecting the <emphasis role="bold">Option</emphasis>
      dropdown menu and then <emphasis role="bold">View</emphasis><emphasis
      role="bold">plot</emphasis>. The plot contains information about the fit
      as well as the quantile values and confidence limits. Within the plot
      window the y-axis contains information on discharge (or log discharge
      depending on the Plot scale selected) and x-axis displays the Annual
      Exceedance Probability (AEP) in terms of 1 in Y years. The plot displays
      the:</para>

      <itemizedlist>
        <listitem>
          <para>Log-Normal probability plot of the gauged flows with plotting position determined
            using the Cunnane plotting position, shown as blue triangles;</para>
        </listitem>

        <listitem>
          <para>X% AEP quantile curve (derived using the posterior mean
          parameters), shown as a black line;</para>
        </listitem>

        <listitem>
          <para>90% quantile confidence limits shown as dashed pink lines;
          and</para>
        </listitem>

        <listitem>
          <para>The expected probability quantile, shown as a red line.</para>
        </listitem>
      </itemizedlist>

      <para>For the data contained in this example the resulting plot displays a good fit to the
        gauged data and appears to have tight confidence limits with all gauged data points falling
        within the 90% confidence limits; by default the figure plots the logarithm of the flood
        peaks. The plot can be rescaled to remove the log from the flow values. Select the <emphasis
          role="bold">Plot</emphasis><emphasis role="bold">scale</emphasis> button and choose one of
        the non-log options, that is, either Gumble or Exponential and the uncertainty changes as in
          <xref linkend="b3_ch2_f_nc6jz"/>. This will present a more sobering perspective on the
        model fit with the confidence limit appearing much larger for rarer flood quantiles. This
        can be confirmed by reviewing the results in the <emphasis role="bold">Result
          file</emphasis>. <xref linkend="b3_ch2_t_y11qf"/> presents a subset of the results found
        in the <emphasis role="bold">Result file</emphasis> of selected X% AEP quantiles qY and
        their 90% confidence limits. For example, for the 1% AEP flood, the 5% and 95% confidence
        limits are respectively 37% and 546% of the quantile qY! The 0.2% AEP confidence limits are
        so wide as to render estimation meaningless. Note the expected AEP for the quantile qY
        consistently exceeds the nominal X% AEP. For example, the 1% (1 in 100) AEP quantile of 19
        572 m<superscript>3</superscript>/s has an expected AEP of 1.35% (1 in 74).</para>

      <figure xml:id="b3_ch2_f_nc6jz">
        <title>Probability Plot using Gumbel Scale</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3063.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <table xml:id="b3_ch2_t_y11qf">
        <caption>Selected Results</caption>

        <col width="18%"/>

        <col width="22%"/>

        <col width="22%"/>

        <col width="22%"/>

        <col width="22%"/>

        <thead>
          <tr>
            <th><emphasis role="bold">1 in Y AEP</emphasis></th>

            <th><emphasis role="bold">Quantile Estimate
            q</emphasis><subscript>Y</subscript></th>

            <th><emphasis role="bold">Quantile confidence limits 5%
            limit</emphasis></th>

            <th><emphasis role="bold">Quantile confidence limits 95%
            limit</emphasis></th>

            <th><emphasis role="bold">Expected 1 in Y AEP for
            q</emphasis><subscript>Y</subscript></th>
          </tr>
        </thead>

        <tbody>
          <tr>
            <td>10</td>

            <td>3929</td>

            <td>2229</td>

            <td>8408</td>

            <td>10.1%</td>
          </tr>

          <tr>
            <td>50</td>

            <td>12 786</td>

            <td>5502</td>

            <td>51 010</td>

            <td>2.32%</td>
          </tr>

          <tr>
            <td>100</td>

            <td>19 572</td>

            <td>7188</td>

            <td>107 122</td>

            <td>1.36%</td>
          </tr>

          <tr>
            <td>500</td>

            <td>47 034</td>

            <td>11 507</td>

            <td>570 635</td>

            <td>0.48%</td>
          </tr>
        </tbody>
      </table>

      
      <table>
        <caption>Gauged flows on the Hunter River at
          Singleton</caption>
        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <tbody>
          <tr>
            <td>1938</td>

            <td>76.26</td>

            <td>1946</td>

            <td>1374.42</td>

            <td>1954</td>

            <td>1391.43</td>

            <td>1962</td>

            <td>2125.4</td>
          </tr>

          <tr>
            <td>1939</td>

            <td>171.87</td>

            <td>1947</td>

            <td>280.18</td>

            <td>1955</td>

            <td>12525.66</td>

            <td>1963</td>

            <td>966.35</td>
          </tr>

          <tr>
            <td>1940</td>

            <td>218.21</td>

            <td>1948</td>

            <td>202.62</td>

            <td>1956</td>

            <td>1099.54</td>

            <td>1964</td>

            <td>2751.68</td>
          </tr>

          <tr>
            <td>1941</td>

            <td>668.79</td>

            <td>1949</td>

            <td>4052.42</td>

            <td>1957</td>

            <td>447.75</td>

            <td>1965</td>

            <td>49.03</td>
          </tr>

          <tr>
            <td>1942</td>

            <td>1374.42</td>

            <td>1950</td>

            <td>2323.77</td>

            <td>1958</td>

            <td>478.92</td>

            <td>1966</td>

            <td>76.51</td>
          </tr>

          <tr>
            <td>1943</td>

            <td>124.12</td>

            <td>1951</td>

            <td>2536.31</td>

            <td>1959</td>

            <td>180.52</td>

            <td>1967</td>

            <td>912.5</td>
          </tr>

          <tr>
            <td>1944</td>

            <td>276.3</td>

            <td>1952</td>

            <td>3315.62</td>

            <td>1960</td>

            <td>164.36</td>

            <td>1968</td>

            <td>926.67</td>
          </tr>

          <tr>
            <td>1945</td>

            <td>895.5</td>

            <td>1953</td>

            <td>1232.73</td>

            <td>1961</td>

            <td>229.54</td>

            <td/>

            <td/>
          </tr>
        </tbody>
      </table>
    </section>
  </section>

  <section xml:id="b3_ch2_s_kg64r">
    <title>Example 4: Use of binomial censored historical data</title>

    <para>This example is a continuation of <emphasis role="bold">Example 3</emphasis> and it
      examines the benefit of using historical flood information. In the previous example the gauged
      record spanned the period 1938 to 1968. The biggest flood in that record occurred in 1955 with
      a discharge of 12 526 m<superscript>3</superscript>/s. An examination of historic records
      indicates that during the ungauged period 1820 to 1937 there was only one flood that exceeded
      the 1955 flood and that this flood occurred in 1820. The information for the 1820 flood is not
      from a stream gauge; rather it is from a variety of sources including newspaper articles. This
      information is valuable, perhaps the most valuable, even though the magnitude of the 1820
      flood is not reliably known. This information can be incorporated into a Bayesian approach.
      The way that this is done in TUFLOW Flike is through censoring data.</para>

    <para>From the information about the flood history at Singleton we can
    make the following conclusions:</para>

    <itemizedlist>
      <listitem>
        <para>Over the ungauged period 1820 to 1937 there was:</para>

        <itemizedlist>
          <listitem>
            <para>One flood above the 1955 flood; and</para>
          </listitem>

          <listitem>
            <para>117 floods below the 1955 flood.</para>
          </listitem>
        </itemizedlist>
      </listitem>
    </itemizedlist>

    <para>Note that the ungauged record length is 118 years, that is, all
    years from 1820 to 1937 are included as it is assumed each year has an
    event. Also, note that the ungauged period cannot overlap with the gauged
    period.</para>

    <section xml:id="b3_ch2_s_uksma">
      <title>Launch TUFLOW-Flike</title>

      <para>As in <emphasis role="bold">Example 3</emphasis> launch TUFLOW
      Flike; however, this time open the <emphasis
      role="bold">.</emphasis><emphasis role="bold">fld</emphasis> file
      previously created: <emphasis role="bold">Example_3.fld</emphasis>. This
      file will be used as it contains the data that are needed for this
      example. To do this select the File dropdown menu and then select
      <emphasis role="bold">Open</emphasis>. Navigate to the <emphasis
      role="bold">Example_3.fld</emphasis> in the next dialogue box and open
      the file. The <emphasis role="bold">Flike Editor</emphasis> window will
      then appear containing all the information from Example 3.</para>
    </section>

    <section xml:id="b3_ch2_s_5lcbh">
      <title>Save Example_4.fld</title>

      <para>The next step is to save the <emphasis
      role="bold">Example_4.fld</emphasis> file as a new file. It is best to
      do this immediately to ensure that no data is overwritten. To do this,
      select <emphasis role="bold">OK</emphasis> from the <emphasis
      role="bold">Flike Editor</emphasis> window which will return to the main
      <emphasis role="bold">TUFLOW Flike</emphasis> window. Select <emphasis
      role="bold">File</emphasis> again and then <emphasis
      role="bold">Save</emphasis><emphasis role="bold">as</emphasis>. Save the
      file as <emphasis role="bold">Example_4.fld</emphasis> in a new folder
      called <emphasis role="bold">Example 4.</emphasis></para>
    </section>

    <section xml:id="b3_ch2_s_jym0z">
      <title>Enter Historical Flood Information</title>

      <para>In this step the historical flood information is entered. To edit
      the <emphasis role="bold">Example_4.fld</emphasis> data from the
      <emphasis role="bold">TUFLOW-Flike</emphasis> window select <emphasis
      role="bold">Options</emphasis> and then <emphasis role="bold">Edit
      data</emphasis>. This reopens the Flike Editor window. Now select the
      <emphasis role="bold">Censoring of observed values</emphasis> tab and
      this will open a window similar to <xref linkend="b3_ch2_f_lx319"/> with
      no data.</para>

      <figure xml:id="b3_ch2_f_lx319">
        <title>Censoring observed values tab</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3017.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The historical data needs to be entered into the <emphasis
      role="bold">Censoring of observed values</emphasis> tab, that is, we
      need to let TUFLOW-Flike know that there has been one flood greater than
      the 1955 flood between 1820 and 1937. So:</para>

      <itemizedlist>
        <listitem>
          <para>The <emphasis role="bold">Threshold value</emphasis> is 12
              526m<superscript>3</superscript>/s - the size of the 1955 flood.</para>
        </listitem>

        <listitem>
          <para>The Years greater than the threshold (<emphasis
          role="bold">Yrs &gt;</emphasis><emphasis role="bold">
          threshold</emphasis>) is one (1) – the 1820 flood.</para>
        </listitem>

        <listitem>
          <para>The Years less than or equal to the threshold (<emphasis
          role="bold">Yrs &lt;= threshold</emphasis>) is 117 – there were 117
          years between 1820 and 1937 with flood less than the 1820
          flood.</para>
        </listitem>

        <listitem>
          <para>The <emphasis role="bold">Start Year</emphasis> is 1820;
          and</para>

          <para>The <emphasis role="bold">End Year</emphasis> is 1937.</para>
        </listitem>
      </itemizedlist>

      <para>Once the data has been entered, select OK which will return the
      main <emphasis role="bold">TUFLOW-Flike</emphasis> window. TUFLOW-Flike
      preforms some checks of the data to ensure that it has been entered
      correctly. However, these are only checks and it is up to the user to
      ensure they have correctly configured the historic censoring.</para>

      <para>Return to the <emphasis role="bold">General</emphasis> tab by
      selecting <emphasis role="bold">Options</emphasis> and then <emphasis
      role="bold">Edit data</emphasis> and it should appear as in <xref
      linkend="b3_ch2_f_nkj8a"/>. <emphasis role="bold">Note the Number of
      censoring thresholds</emphasis> text field has been populated with the
      number 1, so TUFLOW-Flike has recognised that there censoring has been
      configured.</para>

      <para>As with the previous example, check that the <emphasis
      role="bold">Always display report file</emphasis> radio button has been
      selected.</para>

      <figure xml:id="b3_ch2_f_nkj8a">
        <title>Configured Flike Editor</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3064.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section xml:id="b3_ch2_s_4e1b5">
      <title>Run TUFLOW-Flike with Historic Censoring Data</title>

      <para>On the general tab select <emphasis role="bold">OK</emphasis> and
      return to the <emphasis role="bold">TUFLOW-Flike</emphasis> window. As
      in the previous exercise select <emphasis role="bold">Option</emphasis>
      and then <emphasis role="bold">Fit model</emphasis>. This will run
      TUFLOW-Flike and when the engine has finished the <emphasis
      role="bold">Probability Plot</emphasis> will open together with the
      <emphasis role="bold">Report File</emphasis>.</para>
    </section>

    <section xml:id="b3_ch2_s_nv0wq">
      <title>Results</title>

      <para><xref linkend="b3_ch2_t_febrt"/> presents the posterior mean, standard deviation and
        correlation for the Log Pearson Type III parameters: <emphasis role="italic">m</emphasis>,
          <emphasis role="italic">loges</emphasis> and <emphasis role="italic">g</emphasis> which
        are respectively the mean, standard deviation and skewness of loge(q) taken from the
          <emphasis role="bold">Report File</emphasis>. Comparison with Example 3 reveals the
        censored data have reduced by almost 17% the uncertainty in the skewness (<emphasis
          role="italic">g</emphasis>) parameter. This parameter controls the shape of the
        distribution, particularly in the tail region where the floods of interest are.</para>

      <table xml:id="b3_ch2_t_febrt">
        <caption>Posterior Mean, Standard Deviation and Correlation for the LP III</caption>

        <col width="16%"/>

        <col width="16%"/>

        <col width="16%"/>

        <col width="16%"/>

        <col width="16%"/>

        <col width="16%"/>

        <tbody>
          <tr>
            <td>LP III Parameter</td>

            <td>Mean</td>

            <td>Std. Deviation</td>

            <td>Correlation</td>

            <td/>

            <td/>
          </tr>

          <tr>
            <td>m</td>

            <td>6.365</td>

            <td>0.237</td>

            <td>1.000</td>

            <td/>

            <td/>
          </tr>

          <tr>
            <td>log<subscript>e</subscript>s</td>

            <td>0.303</td>

            <td>0.120</td>

            <td>-0.236</td>

            <td>1.000</td>

            <td/>
          </tr>

          <tr>
            <td>g</td>

            <td>-0.004</td>

            <td>0.405</td>

            <td>-0.227</td>

            <td>-0.409</td>

            <td>1.000</td>
          </tr>
        </tbody>
      </table>

      <para>The resulting <emphasis role="bold">Probability plot</emphasis> is
      shown in <xref linkend="b3_ch2_f_fbunw"/>. This figure displays on a log
      normal probability plot the gauged flows, the X% AEP quantile curve
      (derived using the posterior mean parameters), the 90% quantile
      confidence limits and the expected probability curve. Compared with
      Example 3 the tightening of the confidence limits is noticeable.</para>

      <figure xml:id="b3_ch2_f_136dw">
        <title>Probability plot of the Singleton data with historic
        information</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3018.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The following table (<xref linkend="b3_ch2_t_a0hsi"/>) of selected
      1 in Y AEP quantiles qY and their 90% confidence limits illustrates the
      benefit of the information contained in the historic data. For example,
      for the 1% AEP flood the 5% and 95% confidence limits are respectively
      58% and 205% of the quantile qY! This represents a major reduction in
      quantile uncertainty compared with Example 3 which yielded limits of 38%
      and 553%. This is illustrated in graphically <xref
      linkend="b3_ch2_f_fbunw"/>.</para>

      <table xml:id="b3_ch2_t_a0hsi">
        <caption>Comparison of Selected Quantiles with 90% Confidence
        Limits</caption>

        <col width="20%"/>

        <col width="20%"/>

        <col width="20%"/>

        <col width="20%"/>

        <col width="20%"/>

        <thead>
          <tr>
            <th><emphasis role="bold">1 in Y AEP</emphasis></th>

            <th><emphasis role="bold">Quantile Estimate
            q</emphasis><subscript>Y</subscript></th>

            <th><emphasis role="bold">Quantile Confidence Limits 5%
            Limit</emphasis></th>

            <th><emphasis role="bold">Quantile Confidence Limits 95%
            Limit</emphasis></th>

            <th><emphasis role="bold">Expected 1 in Y AEP for
            q</emphasis><subscript>Y</subscript></th>
          </tr>
        </thead>

        <tbody>
          <tr>
            <td>10</td>

            <td>3294</td>

            <td>2181</td>

            <td>4947</td>

            <td>10.37%</td>
          </tr>

          <tr>
            <td>50</td>

            <td>9350</td>

            <td>5778</td>

            <td>16 511</td>

            <td>2.09%</td>
          </tr>

          <tr>
            <td>100</td>

            <td>13 511</td>

            <td>7785</td>

            <td>27 687</td>

            <td>1.08%</td>
          </tr>

          <tr>
            <td>500</td>

            <td>28 542</td>

            <td>12 966</td>

            <td>85 583</td>

            <td>0.28%</td>
          </tr>
        </tbody>
      </table>

      <para>Note that <emphasis role="bold">Report File</emphasis> presents
      the Expected AEP in 1 in Y years whereas <xref
      linkend="b3_ch2_t_a0hsi"/> presents as the Expected AEP as a
      percentage.</para>

      <para>This example highlights the significant reductions in uncertainty that historical data
        can offer. However, care must be exercised to ensure the integrity of the historic
        information – see <xref linkend="b3_ch2_s_22h0g"/> for more details.</para>

      <figure xml:id="b3_ch2_f_fbunw">
        <title>Probability plot of the Singleton data with historic
        information</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3065.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>
  </section>

  <section xml:id="b3_ch2_s_0bj8p">
    <title>Example 5: Use of regional information</title>

    <para>In this example the use of regional parameter information is explored, building on Example
      3. As was shown in Example 3, there was significant uncertainty in the skewness parameter. In
      that example, the posterior mean of the skewness was estimated to be 0.131 with a posterior
      standard deviation of 0.479. This led to significant uncertainty in the quantile estimates,
      for instance the 5% and 95% confidence limits for the 1% AEP quantile were 37% and 546%
      respectively of the 1% AEP quantile. This example shows how the use of regional information
      can reduce, sometimes significantly, the uncertainty of quantile estimates. Details on the use
      of regional information can be found in <xref linkend="b3_ch2_s_9gkdb"/> and <xref
        linkend="b3_ch2_s_xjch2"/>.</para>

    <para>In this hypothetical example, a regional analysis of skewness has
    been conducted and the expected regional skew was found to be 0.00 with a
    standard deviation of 0.30. This information can be incorporated into the
    Bayesian analysis undertaken by TUFLOW Flike as shown in this
    example.</para>

    <section xml:id="b3_ch2_s_vyhk9">
      <title>Launch TUFLOW Flike</title>

      <para>The Singleton data from the previous examples will be used in this
      example, so as in Example 4 launch TUFLOW Flike and open the <emphasis
      role="bold">.fld</emphasis> file created in Example 3. Save the opened
      <emphasis role="bold">.fld</emphasis> as, say, <emphasis
      role="bold">Example_5.fld</emphasis>.</para>
    </section>

    <section xml:id="b3_ch2_s_jltet">
      <title>Enter Prior Information</title>

      <para>The next step will be to enter the prior information, that is the regional information
        on skew. To do this, select <emphasis role="bold">Edit data</emphasis>from the <emphasis
          role="bold">Options</emphasis> menu. As before, this opens the <emphasis role="bold">Flike
          Editor</emphasis>. To enter the prior regional information, check the <emphasis
          role="bold">Gaussian prior distributions</emphasis> radio button and then click on the
        Edit button as shown in <xref linkend="b3_ch2_f_mom23"/>. This will open the <emphasis
          role="bold">Prior for Log-Pearson III</emphasis> window as shown in <xref
          linkend="b3_ch2_f_zr5se"/>.</para>

      <para>The regional skewness (0.00) is entered into the <emphasis
      role="bold">Mean Skew of log Q</emphasis> text box and the standard
      deviation of the regional skew (0.300) is entered into the <emphasis
      role="bold">Standard Deviation Skew of log Q</emphasis> as shown in
      <xref linkend="b3_ch2_f_zr5se"/>. Note in practice careful attention to
      the units being used is required.</para>

      <para>Very large prior standard deviations are assigned to the <emphasis role="bold">Mean of
          log Q</emphasis> and <emphasis role="bold">Standard deviation of log</emphasis> Q
        parameters to ensure there is no prior information about theses parameters. If the <emphasis
          role="italic">Log Pearson III</emphasis> distribution has been selected, the option to
        import the prior information from the ARR Regional Flood Frequency Estimation method is
        available (<xref linkend="b3_ch3"/>).</para>

      <para>Select <emphasis role="bold">OK</emphasis> to return to the
      <emphasis role="bold">Flike editor</emphasis> window.</para>

      <figure xml:id="b3_ch2_f_mom23">
        <title>Gaussian prior distributions</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3019.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <figure xml:id="b3_ch2_f_zr5se">
        <title>Prior for Log-Pearson III window</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3020.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section xml:id="b3_ch2_s_hehr0">
      <title>Run TUFLOW Flike with Regional Information</title>

      <para>As in the previous examples select <emphasis
      role="bold">OK</emphasis>from the <emphasis role="bold">Flike
      Editor</emphasis> window to return to the main <emphasis
      role="bold">TUFLOW Flike</emphasis> window and select <emphasis
      role="bold">Fit model</emphasis> from the <emphasis role="bold">Options
      menu</emphasis> to run TUFLOW Flike. This should result in the
      Probability plot as shown in <xref linkend="b3_ch2_f_9a967"/>.</para>

      <figure xml:id="b3_ch2_f_9a967">
        <title>Probability plot of with prior regional information</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3021.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para><xref linkend="b3_ch2_f_9a967"/> presents the probability plot for the LP III model
        fitted to the gauged data with prior information on the skewness. Comparison of the results
        from this example with the results from Example 3 (see <xref linkend="b3_ch2_f_qk0wd"/>)
        reveals substantially reduced uncertainty in the right hand tail.</para>

      <figure xml:id="b3_ch2_f_qk0wd">
        <title>Comparison between the results from Example 3 and Example
        5</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3066.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <table xml:id="b3_ch2_t_ap1ps">
        <caption>Comparison of LP III Parameters with and without prior information</caption>

        <col width="20%"/>

        <col width="20%"/>

        <col width="20%"/>

        <col width="20%"/>

        <col width="20%"/>

        <thead>
          <tr>
            <th><emphasis role="bold">LP III Parameter</emphasis></th>

            <th colspan="2"><emphasis role="bold">No Prior Information</emphasis></th>

            <th colspan="2"><emphasis role="bold">With Prior Information</emphasis></th>
          </tr>
        </thead>

        <tbody>
          <tr>
            <td/>

            <td>Mean</td>

            <td>Std. Deviation</td>

            <td>Mean</td>

            <td>Std. Deviation</td>
          </tr>

          <tr>
            <td>m</td>

            <td>6.433</td>

            <td>0.262</td>

            <td>6.421</td>

            <td>0.251</td>
          </tr>

          <tr>
            <td>log<subscript>e</subscript>s</td>

            <td>0.353</td>

            <td>0.144</td>

            <td>0.320</td>

            <td>0.131</td>
          </tr>

          <tr>
            <td>g</td>

            <td>0.131</td>

            <td>0.479</td>

            <td>0.019</td>

            <td>0.261</td>
          </tr>
        </tbody>
      </table>

      <para><xref linkend="b3_ch2_t_sbwn2"/> presents selected AEP quantiles
      qY and their 90% confidence limits. This table further illustrates the
      benefit of incorporating regional information. For example, for the 1%
      AEP flood the 5% and 95% confidence limits are respectively 37% and 546%
      of the quantile q1% when no prior information is used. These limits are
      reduced to 46% and 292%, respectively using prior regional
      information.</para>

      <table xml:id="b3_ch2_t_sbwn2">
        <caption>Selected Results</caption>

        <col width="14%"/>

        <col width="14%"/>

        <col width="14%"/>

        <col width="14%"/>

        <col width="14%"/>

        <col width="14%"/>

        <col width="14%"/>

        <thead>
          <tr>
            <th><emphasis role="bold">AEP (%)</emphasis></th>

            <th colspan="3"><emphasis role="bold">No Prior Information</emphasis></th>

            <th colspan="3"><emphasis role="bold">With Prior Information</emphasis></th>
          </tr>

          <tr>
            <th/>

            <th><emphasis role="bold">Quantile Estimate
            q</emphasis><subscript>Y</subscript></th>

            <th><emphasis role="bold">Quantile Confidence 5%
            Limit</emphasis></th>

            <th><emphasis role="bold">Quantile Confidence 95%
            Limit</emphasis></th>

            <th><emphasis role="bold">Quantile Estimate
            q</emphasis><subscript>Y</subscript></th>

            <th><emphasis role="bold">Quantile Confidence 5%
            Limit</emphasis></th>

            <th><emphasis role="bold">Quantile Confidence 95%
            Limit</emphasis></th>
          </tr>
        </thead>

        <tbody>
          <tr>
            <td>10%</td>

            <td>3929</td>

            <td>2229</td>

            <td>8408</td>

            <td>3598</td>

            <td>2172</td>

            <td>6702</td>
          </tr>

          <tr>
            <td>2%</td>

            <td>12 786</td>

            <td>5 502</td>

            <td>51 010</td>

            <td>10 535</td>

            <td>5 310</td>

            <td>26 633</td>
          </tr>

          <tr>
            <td>1%</td>

            <td>19 572</td>

            <td>7 188</td>

            <td>107 122</td>

            <td>15 413</td>

            <td>7 093</td>

            <td>45 087</td>
          </tr>

          <tr>
            <td>0.2%</td>

            <td>47 034</td>

            <td>11 507</td>

            <td>570 635</td>

            <td>33 365</td>

            <td>12 244</td>

            <td>134 107</td>
          </tr>
        </tbody>
      </table>
    </section>
  </section>

  <section xml:id="b3_ch2_s_oyr7w">
    <title>Example 6: Censoring PILFs using multiple Grubbs-Beck test</title>

    <para>In many Australian watercourses there are often years in which there are no floods. The
      annual maximum from those years are not representative of the population of floods and can
      unduly influence the fit of the distribution as discussed in <xref linkend="b3_ch2_s_1f6ft"/>.
      The flow values are referred to as Potentially Influential Low Flows (PILFs). It is
      recommended that in all flood frequency analyses the removal of these flows is investigated
      using the multiple Grubbs-Beck test to identify PILFs. The following example is taken from
        <citation>b3_c2_r59+1</citation> using data provided by the Wimmera Catchment Management
      Authority. The table at the end of this example lists 56 years of Annual Maximum discharges
      for the Wimmera River at Glynwylln. This data is included in the TUFLOW Flike download and was
      installed in the <emphasis role="bold">data</emphasis> folder in the install location of
      TUFLOW Flike. This location will be something similar to <emphasis role="italic">C:\TUFLOW
        Flike\data\wimmeraGaugedFlows.csv</emphasis>.</para>

    <para>This example will examine the influence of PILFs and demonstrate how
    to use the multiple Grubbs-Beck test to safely remove them from the flood
    frequency analysis.</para>

    <section xml:id="b3_ch2_s_zda5a">
      <title>Launch TUFLOW Flike and Import Data</title>

      <para>As in Example 3 launch TUFLOW Flike and create a new <emphasis
      role="bold">.fld</emphasis> file. Save the opened .fld as say, <emphasis
      role="bold">Example_6.fld</emphasis>. Import the Wimmera River data in
      the same way that the Singleton data was imported, ensuring that the
      structure of the data has been checked using the <emphasis
      role="bold">View</emphasis> button. The <emphasis role="bold">Records
      </emphasis>start in the second row (skip the first), <emphasis
      role="bold">Years</emphasis> are in column 1 and the <emphasis
      role="bold">Gauged values</emphasis> are in column 2. Configure the
      import options and import the data.</para>

      <para>Once this has been done and the <emphasis role="bold">Gauged
      values</emphasis> have been ranked in descending order the Flike Editor
      window should look like <xref linkend="b3_ch2_f_6lj7c"/>.</para>

      <figure xml:id="b3_ch2_f_6lj7c">
        <title>TUFLOW Flike editor window with Wimmera data</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3067.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section xml:id="b3_ch2_s_22b6v">
      <title>Fit Distribution</title>

      <para>The Wimmera data will be fitted to a Generalised Extreme Value
      (GEV) distribution. To do this, return to the <emphasis
      role="bold">Flike Editor General</emphasis> tab and ensure that the
      following settings have been chosen:</para>

      <itemizedlist>
        <listitem>
          <para>Bayesian inference method with <emphasis role="bold">No prior
          information; </emphasis></para>
        </listitem>

        <listitem>
          <para>The <emphasis role="bold">GEV</emphasis> probability model;
          and</para>
        </listitem>

        <listitem>
          <para>The <emphasis role="bold">Maximum AEP</emphasis> is set to 200
          years</para>
        </listitem>
      </itemizedlist>

      <para>Once these settings have been selected, select <emphasis
      role="bold">OK</emphasis> and run TUFLOW Flike in the usual way.</para>
    </section>

    <section xml:id="b3_ch2_s_njj43">
      <title>Initial Results</title>

      <para>When TUFLOW Flike has run a new probability plot window will open.
      The plot will <emphasis role="bold">not</emphasis> look like <xref
      linkend="b3_ch2_f_z3jbp"/>. To expose a better view of the distributions
      fit, the plot scale should be changed using the <emphasis
      role="bold">Plot Scale</emphasis> button from a <emphasis
      role="bold">Gumbel </emphasis>plot scale to a <emphasis
      role="bold">Gumbel-log</emphasis> plot scale and the y-axis rescaled
      using the <emphasis role="bold">Rescale</emphasis> button to have a
      minimum of 0.0 and a maximum of 4.0.</para>

      <para>In <xref linkend="b3_ch2_f_z3jbp"/>, the fit to the right-hand
      tail is not satisfactory. The expected quantiles are significantly
      greater than the gauged data, further the largest 3 data points fall
      outside of the lower 90% confidence limits.</para>

      <figure xml:id="b3_ch2_f_z3jbp">
        <title>Initial probability plot for Wimmera data with GEV</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3022.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section xml:id="b3_ch2_s_d6qyg">
      <title>Multiple Grubbs-Beck test</title>

      <para>The fit of the distribution can be improved by removing PILFs. In
      TUFLOW Flike this can be done using the multiple Grubbs-Beck test, to do
      this, return to the <emphasis role="bold">Flike Editor</emphasis> window
      and select the <emphasis role="bold">Censor</emphasis> button. TUFLOW
      Flike will run the multiple Grubbs-Beck test on the Wimmera data and
      when finished it will return a window similar to the one shown in <xref
      linkend="b3_ch2_f_iql1y"/>. The multiple Grubbs-Beck test has detected
      27 possible PILFs, select <emphasis role="bold">Yes</emphasis> to censor
      them.</para>

      <figure xml:id="b3_ch2_f_iql1y">
        <title>Results of the multiple Grubbs-Beck test</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3023.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>On agreeing to censor these flows, TUFLOW Flike automatically
      performs two changes to the inference setup:</para>

      <orderedlist>
        <listitem>
          <para>The 27 lowest discharges are excluded from the
          calibration.</para>
        </listitem>

        <listitem>
          <para>A censored threshold is added, with the information that there
          are 27 Annual Maximum discharges that lie below the threshold of
          54.396m<superscript>3</superscript>/s which corresponds the 28th
          ranked discharge.</para>
        </listitem>
      </orderedlist>

      <para>These are further explained below</para>

      <section xml:id="b3_ch2_s_6nq35">
        <title>Excluded Data</title>

        <para>The exclusion of the lowest 27 discharges can be seen in the
        <emphasis role="bold">Observed Flows</emphasis> tab of the <emphasis
        role="bold">Flike Editor</emphasis> as shown in <xref
        linkend="b3_ch2_f_tbyqe"/>. In this tab all the values below the
        threshold have the <emphasis role="bold">Exclude check</emphasis> box
        crossed, this can be seen by scrolling down the window or by
        re-ranking the data and selecting <emphasis
        role="bold">Ascending</emphasis>. If you have re-ranked the data in
        ascending order re-rank it back into <emphasis
        role="bold">Decesending</emphasis> order.</para>

        <figure xml:id="b3_ch2_f_tbyqe">
          <title>Excluded gauged values</title>

          <mediaobject>
            <imageobject>
              <imagedata width="100%" fileref="../../figures/3024.png"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section xml:id="b3_ch2_s_eudes">
        <title>Censoring Threshold</title>

        <para>The addition of the censored threshold appears in the <emphasis
        role="bold">Censoring of observed values</emphasis> tab of the
        <emphasis role="bold">Flike Editor</emphasis> as shown in <xref
        linkend="b3_ch2_f_xdzap"/>. The <emphasis
        role="bold">Threshold</emphasis> value
        (54.396m<superscript>3</superscript>/s) has been automatically
        populated together with the years that are greater than the threshold
        (0). The number of years less than the threshold (27) has also been
        populated. What this is telling TUFLOW Flike is that 27 years of
        discharges are less than the threshold are being censored; that is,
        gauged values are not considered but the frequency is. The <emphasis
        role="bold">Start year</emphasis> and <emphasis role="bold">End
        year</emphasis> are also populated with dummy year ranges beginning
        1000BC. This is done to satisfy an automatic check in TUFLOW Flike
        designed to assist in the entry of historic data.</para>

        <figure xml:id="b3_ch2_f_xdzap">
          <title>Censoring of observed values</title>

          <mediaobject>
            <imageobject>
              <imagedata width="100%" fileref="../../figures/3068.png"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>
    </section>

    <section xml:id="b3_ch2_s_b2jgj">
      <title>Results using multiple Grubbs-Beck test</title>

      <para>Return to the main <emphasis role="bold">TUFLOW Flike</emphasis>
      window and run TUFLOW FLIKE by selecting <emphasis role="bold">Fit
      model</emphasis>. As usual, a <emphasis role="bold">Probability
      plot</emphasis> window will automatically appear, as for the initial
      results change the plot scale to Gumbel-log and rescale the y-axis to
      have a minimum of 0.0 and a maximum of 4.0. The resulting plot will look
      like <xref linkend="b3_ch2_f_cf285"/>.</para>

      <para>A comparison of <xref linkend="b3_ch2_f_z3jbp"/> and <xref
      linkend="b3_ch2_f_cf285"/> shows the improved fit, in <xref
      linkend="b3_ch2_f_cf285"/> all of the gauged data points fall within the
      90% confidence limits. Further, censoring the PILFs using the multiple
      Grubbs-Beck test has significantly altered the quantile estimates and
      reduced the confidence limits as shown in <xref
      linkend="b3_ch2_t_a42ds"/>. For instance the quantile q1% when PILFs are
      excluded is around 21% of the initial estimate. The lower and upper
      confidence limits have been considerable reduced, initially they were
      30% and 500% of the quantile q1% and following the removal of PILFs they
      became 68% and 220% of the quantile q1%.</para>

      <figure xml:id="b3_ch2_f_cf285">
        <title>GEV fit - 56 years AM of gauged discharge - Using multiple
        Grubbs-Beck test</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3025.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <table xml:id="b3_ch2_t_a42ds">
        <caption>Selected Results</caption>

        <col width="14%"/>

        <col width="14%"/>

        <col width="16%"/>

        <col width="16%"/>

        <col width="14%"/>

        <col width="16%"/>

        <col width="16%"/>

        <thead>
          <tr>
            <th><emphasis role="bold">AEP (%)</emphasis></th>

            <th><emphasis role="bold">No Removal of PILFS</emphasis></th>

            <th/>

            <th/>

            <th><emphasis role="bold">Removal of PILFS</emphasis></th>

            <th/>

            <th/>
          </tr>

          <tr>
            <th/>

            <th><emphasis role="bold">Quantile Estimate
            q<subscript>Y</subscript></emphasis></th>

            <th><emphasis role="bold">Quantile Confidence 5%
            Limit</emphasis></th>

            <th><emphasis role="bold">Quantile Confidence 95%
            Limit</emphasis></th>

            <th><emphasis role="bold">Quantile Estimate
            q<subscript>Y</subscript></emphasis></th>

            <th><emphasis role="bold">Quantile Confidence 5%
            Limit</emphasis></th>

            <th><emphasis role="bold">Quantile Confidence 95%
            Limit</emphasis></th>
          </tr>
        </thead>

        <tbody>
          <tr>
            <td>10%</td>

            <td>286</td>

            <td>172</td>

            <td>578</td>

            <td>227</td>

            <td>177</td>

            <td>311</td>
          </tr>

          <tr>
            <td>2%</td>

            <td>1315</td>

            <td>493</td>

            <td>4975</td>

            <td>423</td>

            <td>304</td>

            <td>784</td>
          </tr>

          <tr>
            <td>1%</td>

            <td>2481</td>

            <td>737</td>

            <td>12 398</td>

            <td>521</td>

            <td>354</td>

            <td>145</td>
          </tr>

          <tr>
            <td>0.2%</td>

            <td>10696</td>

            <td>1802</td>

            <td>101 034</td>

            <td>789</td>

            <td>448</td>

            <td>2813</td>
          </tr>
        </tbody>
      </table>

      <para><emphasis role="bold"> Annual Maximum data for the Wimmera River
      at Glynwylln</emphasis></para>

      <informaltable>
        <col width="14%"/>

        <col width="14%"/>

        <col width="14%"/>

        <col width="14%"/>

        <col width="14%"/>

        <col width="14%"/>

        <col width="14%"/>

        <tbody>
          <tr>
            <td>464.35</td>

            <td>167.72</td>

            <td>119.63</td>

            <td>71.4</td>

            <td>32.18</td>

            <td>14.16</td>

            <td>8.52</td>
          </tr>

          <tr>
            <td>395.65</td>

            <td>155.22</td>

            <td>110.56</td>

            <td>69.67</td>

            <td>25.91</td>

            <td>12.64</td>

            <td>3.22</td>
          </tr>

          <tr>
            <td>285.92</td>

            <td>147</td>

            <td>102.62</td>

            <td>67.49</td>

            <td>24.83</td>

            <td>11.9</td>

            <td>2.28</td>
          </tr>

          <tr>
            <td>278.01</td>

            <td>143.99</td>

            <td>97.32</td>

            <td>61.64</td>

            <td>23.95</td>

            <td>11.79</td>

            <td>2.13</td>
          </tr>

          <tr>
            <td>235.22</td>

            <td>143.62</td>

            <td>96.78</td>

            <td>54.4</td>

            <td>22.76</td>

            <td>11.41</td>

            <td>1.9</td>
          </tr>

          <tr>
            <td>211.91</td>

            <td>142.66</td>

            <td>87.98</td>

            <td>38.62</td>

            <td>19.04</td>

            <td>10.8</td>

            <td>1.43</td>
          </tr>

          <tr>
            <td>173.79</td>

            <td>134.36</td>

            <td>79.15</td>

            <td>36.62</td>

            <td>17.37</td>

            <td>10.31</td>

            <td>1.16</td>
          </tr>

          <tr>
            <td>170.13</td>

            <td>123.8</td>

            <td>77.03</td>

            <td>34.07</td>

            <td>14.87</td>

            <td>10.08</td>

            <td>0.01</td>
          </tr>
        </tbody>
      </informaltable>
    </section>
  </section>

  <section xml:id="b3_ch2_s_k1ei7">
    <title>Example 7: Improving poor fits using censoring of low flow
    data</title>

    <para>The standard probability models such as GEV and LP III may not adequately fit flood data
      for a variety of reasons, for example Probable Influential Low Flow Flows (PILFs). In this
      example the censoring of data is used to censor low discharge data and improve the fit of the
      distribution to the data.</para>

    <para>Often the poor fit of a distribution is associated with a sigmoidal
    probability plot as illustrated in <xref linkend="b3_ch2_f_irqh6"/>. In
    such cases a four or five-parameter distributions which have sufficient
    degrees of freedom can be used to track the data in both upper and lower
    tails of the sigmoidal curve. Alternatively a calibration approach that
    gives less weight to smaller floods can be adopted. The second approach is
    adopted in this example.</para>

    <figure xml:id="b3_ch2_f_irqh6">
      <title>Bayesian fit to all gauged data Gumbel probability plot</title>

      <mediaobject>
        <imageobject>
          <imagedata width="100%" fileref="../../figures/3026.png"/>
        </imageobject>
      </mediaobject>
    </figure>

    <section xml:id="b3_ch2_s_vadix">
      <title>Launch TUFLOW Flike and Import Data</title>

      <para>As in previous examples launch TUFLOW Flike, create a new
      <emphasis role="bold">.fld</emphasis> file and import the Albert River
      at Bromfleet data (<emphasis
      role="italic">albertRvGaugedFlows.txt)</emphasis> file which was
      included in the TUFLOW Flike install in the <emphasis
      role="bold">data</emphasis> directory. Note the structure of this file
      and configure the <emphasis role="bold">Import gauged values</emphasis>
      window. The Albert River at Broomfleet data is included at the end of
      this example.</para>
    </section>

    <section xml:id="b3_ch2_s_j7tax">
      <title>Fit GEV Distribution</title>

      <para>To recreate <xref linkend="b3_ch2_f_irqh6"/> fit a GEV distribution to the Albert River
        data and accept the defaults in the <emphasis role="bold">General</emphasis> tab of the
          <emphasis role="bold">FLIKE Editor</emphasis>. The plot in <xref linkend="b3_ch2_f_irqh6"
        /> can be recreated by changing the plot scale to <emphasis role="bold">Gumbel</emphasis>
        and rescaling the y-axis to 0 and 4000.</para>

      <para><xref linkend="b3_ch2_f_irqh6"/> displays the GEV Bayesian fit on
      a Gumbel probability plot. Although the observed floods are largely
      contained within the 90% confidence limits, the fit, nonetheless, is
      poor – the data exhibit a sigmoidal trend with reverse curvature
      developing for floods with an AEP greater than 50%. It appears that the
      confidence limits have been inflated because the GEV fit represents a
      poor compromise.</para>
    </section>

    <section xml:id="b3_ch2_s_qrtgk">
      <title>Use the multiple Grubbs Beck test to improve fit</title>

      <para>The first step in improving the poor fit of this data is to use
      the multiple Grubbs Beck test to remove PILFs. Repeat the procedure
      outline in the previous example. This will result in the censoring of 5
      data points with a threshold of
      36.509m<superscript>3</superscript>/s.</para>

      <para>Now run TUFLOW Flike and fit the model. Changing the plot scale
      and rescale the y-axis as above will result in <xref
      linkend="b3_ch2_f_8wx12"/>.</para>

      <para><xref linkend="b3_ch2_f_8wx12"/> displays the fit after censoring
      the 5 low outliers identified by the multiple Grubbs-Beck test. The
      improvement in fit is marginal at best over <xref
      linkend="b3_ch2_f_irqh6"/>.</para>

      <figure xml:id="b3_ch2_f_8wx12">
        <title>Bayesian fit with 5 low outliers censored after application of
        multiple Grubbs-Beck test</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3027.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section xml:id="b3_ch2_s_2j4e5">
      <title>Trial and error approach</title>

      <para>To deal with this poor fit, a trial-and-error approach to
      selecting the threshold discharge for the censoring low flows can be
      used to obtain a fit that favours the right hand tail of the
      distribution. This involves testing different threshold values until an
      acceptable fit is produced. <xref linkend="b3_ch2_f_ck6ks"/> illustrates
      one such fit. To de-emphasise the left hand tail the floods below the
      threshold of 250 m<superscript>3</superscript>/s were censored. This
      means the GEV distribution was fitted to:</para>

      <itemizedlist>
        <listitem>
          <para>A gauged record consisting of the 27 floods above
          250m<superscript>3</superscript>/s; and</para>
        </listitem>

        <listitem>
          <para>A censored record consisting of 23 floods below the threshold
          of 250m<superscript>3</superscript>/s and 0 floods above this
          threshold.</para>
        </listitem>
      </itemizedlist>

      <para>To do this in TUFLOW Flike there are two steps, as in Example 6,
      these are:</para>

      <itemizedlist>
        <listitem>
          <para>Exclude the flows below
          250m<superscript>3</superscript>/s</para>
        </listitem>

        <listitem>
          <para>Create a censoring threshold</para>
        </listitem>
      </itemizedlist>

      <para>This is essentially the same process that was undertaken to
      exclude flows in Example 6 except it needs to be done manually. This is
      outlined below.</para>

      <section xml:id="b3_ch2_s_akbjo">
        <title>Excluded data</title>

        <para>The flows below 250m<superscript>3</superscript>/s need to be
        excluded from the analysis. To do this select the <emphasis
        role="bold">Observed values</emphasis> tab of the <emphasis
        role="bold">Flike Editor </emphasis>and choose the <emphasis
        role="bold">Block exclude</emphasis> button. Enter 250 into <emphasis
        role="bold">Value below</emphasis><emphasis role="bold">which values
        are to be excluded</emphasis> text box and select <emphasis
        role="bold">OK</emphasis>. This will exclude all values below
        250m<superscript>3</superscript>/s which can be confirmed by scrolling
        down the table in the <emphasis role="bold">Observed values</emphasis>
        tab.</para>
      </section>

      <section xml:id="b3_ch2_s_xyxg7">
        <title>Censoring threshold</title>

        <para>As in the previous example a censoring threshold needs to be
        entered into the Censoring of observed values tab. Populate the tab
        with the following information:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Threshold value</emphasis>: 250</para>
          </listitem>

          <listitem>
            <para>Years greater than threshold (<emphasis role="bold">Yrs &gt;
            threshold</emphasis>): 0</para>
          </listitem>

          <listitem>
            <para>Years less than or equal to threshold (<emphasis
            role="bold">Yrs &lt;= threshold</emphasis>): 23</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Start year</emphasis>: 1000</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">End year</emphasis>: 1022</para>
          </listitem>
        </itemizedlist>
      </section>
    </section>

    <section xml:id="b3_ch2_s_87t31">
      <title>Results of the trial and error approach</title>

      <para>Run TUFLOW Flike in the usual way and a Probability plot similar
      to <xref linkend="b3_ch2_f_ck6ks"/> will be obtained.</para>

      <para>The censored record provides an anchor point for the GEV
      distribution – it ensures that the chance of an Annual Maximum flood
      being less than 250m<superscript>3</superscript>/s is about 23/50
      without forcing the GEV to fit the peaks below the
      250m<superscript>3</superscript>/s threshold. The fit effectively
      disregards floods with a greater than 50% AEP and provides a good fit to
      the upper tail. Another benefit is the substantially reduced 90%
      confidence limits which can be reviewed by examining the results
      files.</para>

      <figure xml:id="b3_ch2_f_ck6ks">
        <title>Bayesian fit with floods below 250
        m<superscript>3</superscript>/s threshold treated as censored
        observations</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3028.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para><emphasis role="bold">Annual Maximum data for the Albert River at
      Bromfleet data</emphasis></para>

      <informaltable>
        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <tbody>
          <tr>
            <td>1765.92</td>

            <td>1689.51</td>

            <td>1652.72</td>

            <td>1468.77</td>

            <td>1364.06</td>

            <td>1341.42</td>

            <td>1327.27</td>

            <td>1273.5</td>
          </tr>

          <tr>
            <td>1214.07</td>

            <td>1185.77</td>

            <td>1177.28</td>

            <td>1086.72</td>

            <td>865.98</td>

            <td>863.15</td>

            <td>860.32</td>

            <td>761.27</td>
          </tr>

          <tr>
            <td>761.27</td>

            <td>752.78</td>

            <td>676.37</td>

            <td>466.95</td>

            <td>461.29</td>

            <td>384.88</td>

            <td>362.24</td>

            <td>305.64</td>
          </tr>

          <tr>
            <td>302.81</td>

            <td>285.83</td>

            <td>271.68</td>

            <td>294.61</td>

            <td>249.61</td>

            <td>220.74</td>

            <td>210.55</td>

            <td>190.74</td>
          </tr>

          <tr>
            <td>156.5</td>

            <td>156.22</td>

            <td>131.03</td>

            <td>124.52</td>

            <td>116.88</td>

            <td>113.77</td>

            <td>99.9</td>

            <td>95.65</td>
          </tr>

          <tr>
            <td>88.3</td>

            <td>87.73</td>

            <td>78.11</td>

            <td>72.73</td>

            <td>36.51</td>

            <td>22.36</td>

            <td>16.7</td>

            <td>15.85</td>
          </tr>

          <tr>
            <td>15.57</td>

            <td>13.02</td>

            <td colspan="6"/>
          </tr>
        </tbody>
      </informaltable>
    </section>
  </section>

  <section xml:id="b3_ch2_s_j695i">
    <title>Example 8: A Non-Homogeneous Flood Probability Model</title>

    <para>The work of <citation>b3_c2_r56+1</citation> illustrates an example of a non-homogeneous
      model. An indicator time series based on the IPO time series (Figure 7) was used to create the
      exogeneous vector x</para>

    <equation xml:id="b3_ch2_e_rqohy">
      <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
          <mi>x</mi>

          <mo>=</mo>

          <mrow>
            <msub>
              <mi>{I</mi>

              <mi>t</mi>
            </msub>

            <mo>, t = 1,...,n}</mo>
          </mrow>
        </mrow>
      </math>
    </equation>

    <para>where the indicator</para>

    <equation xml:id="b3_ch2_e_uynzz">
      <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
          <msub>
            <mi>I</mi>

            <mi>t</mi>
          </msub>

          <mo>=</mo>

          <mrow>
            <mo>{</mo>

            <mtable>
              <mtr>
                <mtd>
                  <mrow>
                    <mi>1 if</mi>

                    <msub>
                      <mi>IPO</mi>

                      <mi>t</mi>
                    </msub>

                    <mrow>
                      <mi>≥</mi>

                      <msub>
                        <mi>IPO</mi>

                        <mi>thresh</mi>
                      </msub>
                    </mrow>
                  </mrow>
                </mtd>
              </mtr>

              <mtr>
                <mtd>
                  <mrow>
                    <mi>0 if</mi>

                    <msub>
                      <mi>IPO</mi>

                      <mi>t</mi>
                    </msub>

                    <msub>
                      <mi>&lt; IPO</mi>

                      <mi>thresh</mi>
                    </msub>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
          </mrow>
        </mrow>
      </math>
    </equation>

    <para>IPO<subscript>t</subscript> is the IPO index for year t and
    IPO<subscript>thresh</subscript> is a threshold value equal to
    -0.125.</para>

    <para>At each of the 33 NSW sites considered by Micevski et al. the AM peak flows were
      stratified according to the indicator It. A 2-parameter log-Normal distribution was fitted to
      the gauged flows with indicator equal to 1 – this is the IPO+ distribution. Likewise, a
      2-parameter log-Normal distribution was fitted to the gauged flows with indicator equal to 0 –
      this is the IPO- distribution. <xref linkend="b3_ch2_f_l7nea"/> presents the histogram for the
      ratio of the IPO- and IPO+ floods for selected 1 in Y AEPs. If the IPO+ and IPO- distributions
      were homogeneous then about half of the sites should have a flood ratio &lt; 1 – <xref
        linkend="b3_ch2_f_l7nea"/> shows otherwise.</para>

    <para>Figures <xref linkend="b3_ch2_f_9ppj8"/> and <xref linkend="b3_ch2_f_f3kfz"/> present log
      normal fits to the IPO+ and IPO- annual maximum flood data for the Clarence river at Lilydale
      respectively. Though the adequacy of the log normal model to fit high floods may be
      questioned, in the AEP range 1 in 2 to 1 in 10 years, the IPO- floods are about 2.6 times the
      IPO+ floods with the same AEP.</para>

    <figure xml:id="b3_ch2_f_l7nea">
      <title>Histogram of IPO- and IPO+ flood ratios</title>

      <mediaobject>
        <imageobject>
          <imagedata width="100%" fileref="../../figures/3029.png"/>
        </imageobject>
      </mediaobject>
    </figure>

    <figure xml:id="b3_ch2_f_9ppj8">
      <title>Log-Normal fit to 43 years of IPO+ data for the Clarence river at Lilydale (units
        ML/day).</title>

      <mediaobject>
        <imageobject>
          <imagedata width="100%" fileref="../../figures/3030.png"/>
        </imageobject>
      </mediaobject>
    </figure>

    <figure xml:id="b3_ch2_f_f3kfz">
      <title>Log-Normal fit to 33 years of IPO- data for the Clarence river at Lilydale (units
        ML/day).</title>

      <mediaobject>
        <imageobject>
          <imagedata width="100%" fileref="../../figures/3031.png"/>
        </imageobject>
      </mediaobject>
    </figure>

    <figure xml:id="b3_ch2_f_l79jq">
      <title>Log-Normal fit to 76 years of data for the Clarence river at Lilydale (units
        ML/day).</title>

      <mediaobject>
        <imageobject>
          <imagedata width="100%" fileref="../../figures/3032.png"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>To avoid bias in estimating long-term flood risk it is essential that the gauged record
      adequately span both IPO+ and IPO- years. In this example, the IPO+ record is 43 years and the
      IPO- record is 33 years in length. With reference to Figure 7 this length of record appears to
      adequately sample both IPO epochs. This suggests that fitting to all the data will yield a
      largely unbiased estimate of the long-term flood risk. <xref linkend="b3_ch2_f_l79jq"/>
      illustrates a log normal fit to all the data.</para>

    <para>A better appreciation of the differences in flood risk can be gleaned by considering <xref
        linkend="b3_ch2_f_81xae"/> which presents the fitted log normal distributions to the IPO+,
      IPO- and total data. During an IPO+ period a flood peak of 100 m<superscript>3</superscript>/s
      has a 1 in 20 AEP while during an IPO- period it has a 1 in 4 AEP. Likewise a flood peak of
      200 m<superscript>3</superscript> /s has 1 in 100 and 1 in 10 AEPs for IPO+ and IPO- periods
      respectively. The differences in flood risk are considerable. If a short gauged record falling
      largely in the IPO+ period was used, a standard flood frequency analysis could seriously
      underestimate the long-term or marginal flood risk.</para>

    <para>The marginal flood risk can be derived by combining the IPO+ and IPO- distribution using
        <xref linkend="b3_ch2_e_p72ly"/> to give</para>

    <equation xml:id="b3_ch2_e_lcql0">
      <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mrow>
          <mrow>
            <mi>P(Q≤</mi>

            <mo>q)</mo>

            <mi>=</mi>

            <mo>P(x</mo>
          </mrow>

          <mo>=</mo>

          <mrow>
            <mi>0)</mi>

            <mrow>
              <munderover>
                <mo>∫</mo>

                <mi>0</mi>

                <mi>q</mi>
              </munderover>

              <mi>p(z|</mi>
            </mrow>

            <mi>θ(x=0))dz + P (x=1)</mi>

            <mrow>
              <munderover>
                <mo>∫</mo>

                <mi>0</mi>

                <mi>q</mi>
              </munderover>

              <mi>p(z|</mi>
            </mrow>

            <mi>θ(x=1))dz</mi>
          </mrow>
        </mrow>
      </math>
    </equation>

    <para>The exogenous variable x can take two values, 0 or 1, depending on
    the IPO epoch. P(x=0), the probability of being in an IPO- epoch, is
    assigned the value 33/76 based on the observation that 33 of the 76 years
    of record were in the IPO- epoch. Likewise P(x=1), the probability of
    being in an IPO+ epoch, is assigned the value 43/76. It follows that
    p(z|<inlineequation>
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
          <mi>θ</mi>
        </math>
      </inlineequation>,x=0) and p(z|<inlineequation>
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
          <mi>θ</mi>
        </math>
      </inlineequation>,x=1) are the log normal pdfs fitted to IOP- and IPO+
    data respectively</para>

    <para>The derived marginal distribution is plotted in <xref linkend="b3_ch2_f_81xae"/>. It
      almost exactly matches the log normal distribution fitted to all the data.</para>

    <figure xml:id="b3_ch2_f_81xae">
      <title>Marginal, IPO+ and IPO+ log-Normal distributions for the Clarence river at
        Lilydale</title>

      <mediaobject>
        <imageobject>
          <imagedata width="100%" fileref="../../figures/3033.png"/>
        </imageobject>
      </mediaobject>
    </figure>
  </section>

  <section xml:id="b3_ch2_s_f243w">
    <title>Example 9: L-moments fit to gauged data</title>

    <para>This example illustrates fitting a GEV distribution to gauged data using L-moments.
      L-moments are a special case of LH-moments where there is no shift (H=0). The procedure to use
      L-moments to fit a distribution is set out in <xref linkend="b3_ch2_s_olf1q"/>. In this
      example Annual Maximum flood data for the Styx River at Jeogla will be fitted using L-moments.
      The flood data are listed at the end of this example.</para>

    <para>The procedure for fitting distributions by L-moments can be completed by hand, and also
      using TUFLOW Flike. Both of these techniques will be outlined in this example.</para>

    <section xml:id="b3_ch2_s_ai5el">
      <title>L-moments by Hand</title>

      <para>The first four L-moments can be estimated by <xref linkend="b3_ch2_e_gf6r2"/> to <xref
          linkend="b3_ch2_e_jkio8"/> and are reported in <xref linkend="b3_ch2_t_kvf1l"/>. The GEV
        parameter estimates can be calculated by substituting the L-moment estimates into the
        equations in <xref linkend="b3_ch2_t_achre"/> to estimate <emphasis role="italic"
          >τ</emphasis>, <emphasis role="italic">Κ</emphasis> and <emphasis role="italic"
          >α</emphasis>. The standard deviation and correlation were derived from 5000 bootstrapped
        samples following the procedure described in <xref linkend="b3_ch2_s_3sslo"/>and <xref
          linkend="b3_ch2_s_wltta"/>. Note standard deviation and correlation cannot be calculated
        by hand.</para>

      <table xml:id="b3_ch2_t_kvf1l">
        <caption>L-moment and GEV Parameter Estimates</caption>

        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <thead>
          <tr>
            <th>L-moment</th>

            <th>L-moment Estimates</th>

            <th>GEV Parameter</th>

            <th>Parameter Estimate</th>

            <th>Standard Deviation</th>

            <th>Correlation</th>

            <th>Correlation</th>

            <th>Correlation</th>
          </tr>
        </thead>

        <tbody>
          <tr>
            <td><inlineequation>
                <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
                  <semantics>
                    <mrow>
                      <msub>
                        <mi>λ</mi>

                        <mn>1</mn>
                      </msub>
                    </mrow>
                  </semantics>
                </math>
              </inlineequation></td>

            <td>189.238</td>

            <td><inlineequation>
                <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
                  <semantics>
                    <mi>τ</mi>
                  </semantics>
                </math>
              </inlineequation></td>

            <td>100.660</td>

            <td>17.657</td>

            <td>1.000</td>

            <td/>

            <td/>
          </tr>

          <tr>
            <td><inlineequation>
                <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
                  <semantics>
                    <mrow>
                      <msub>
                        <mi>λ</mi>

                        <mn>2</mn>
                      </msub>
                    </mrow>
                  </semantics>
                </math>
              </inlineequation></td>

            <td>92.476</td>

            <td><inlineequation>
                <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
                  <semantics>
                    <mi>α</mi>
                  </semantics>
                </math>
              </inlineequation></td>

            <td>104.157</td>

            <td>15.554</td>

            <td>0.597</td>

            <td>1.000</td>

            <td/>
          </tr>

          <tr>
            <td><inlineequation>
                <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
                  <semantics>
                    <mrow>
                      <msub>
                        <mi>λ</mi>

                        <mn>3</mn>
                      </msub>
                    </mrow>
                  </semantics>
                </math>
              </inlineequation></td>

            <td>29.264</td>

            <td><inlineequation>
                <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
                  <semantics>
                    <mi>κ</mi>
                  </semantics>
                </math>
              </inlineequation></td>

            <td>-0.219</td>

            <td>0.130</td>

            <td>0.358</td>

            <td>0.268</td>

            <td>1.000</td>
          </tr>
        </tbody>
      </table>
    </section>

    <section xml:id="b3_ch2_s_5qpnz">
      <title>L-moments using TUFLOW Flike</title>

      <para>L-moments and the distribution parameters can be estimated in TUFLOW Flike. To do this,
        create a new .fld file and import the Styx River at Jeogla data set. Return the <emphasis
          role="bold">Flike Editor General</emphasis> tab. Now set the Inference method to
        LH-moments fit to observed values with and check the H=0 radio box. This last option sets
        the shift to 0 (i.e. L-moments). The <emphasis role="bold">Flike Editor</emphasis> window
        should look like . Run TUFLOW Flike and examine the results file for the L-moments and GEV
        parameters.</para>

      <figure xml:id="b3_ch2_f_mbhq9">
        <title>Flike Editor configured for L-moments</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3034.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The following table lists 47 ranked flows for the Styx River at
      Jeogla.</para>

      <informaltable>
        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <col width="12%"/>

        <tbody>
          <tr>
            <td>878</td>

            <td>541</td>

            <td>521</td>

            <td>513</td>

            <td>436</td>

            <td>411</td>

            <td>405</td>

            <td>315</td>

          </tr>

          <tr>
            <td>309</td>

            <td>300</td>

            <td>294</td>

            <td>258</td>

            <td>255</td>

            <td>235</td>

            <td>221</td>

            <td>220</td>
          </tr>

          <tr>
            <td>206</td>

            <td>196</td>

            <td>194</td>

            <td>190</td>

            <td>186</td>

            <td>177</td>

            <td>164</td>

            <td>126</td>
          </tr>

          <tr>
            <td>117</td>

            <td>111</td>

            <td>108</td>

            <td>105</td>

            <td>92.2</td>

            <td>88.6</td>

            <td>79.9</td>

            <td>74</td>
          </tr>

          <tr>
            <td>71.9</td>

            <td>62.6</td>

            <td>61.2</td>

            <td>60.3</td>

            <td>58</td>

            <td>53.5</td>

            <td>39.1</td>

            <td>26.7</td>
          </tr>

          <tr>
            <td>26.1</td>

            <td>23.8</td>

            <td>22.4</td>

            <td>22.1</td>

            <td>18.6</td>

            <td>13</td>

            <td>8.18</td>
            
            <td></td>

          </tr>
        </tbody>
      </informaltable>
    </section>
  </section>

  <section xml:id="b3_ch2_s_r8uuy">
    <title>Example 10: Improving poor fits using LH-moments</title>

    <para>In Example 5 the fit of the distribution to the Albert River flood series was improved by
      censoring low flows. In this example, LH-moments are used instead of censoring to improve the
      fit of the GEV distribution to the flood data.</para>

    <section xml:id="b3_ch2_s_6ae3u">
      <title>Launch TUFLOW Flike</title>

      <para>This example uses the same data as <emphasis role="bold">Example 7</emphasis> for the
        Albert River at Broomfleet, so the previous <emphasis role="bold">Example 7.fld</emphasis>
        file can be used. To do this, launch TUFLOW Flike and open <emphasis role="bold"
          >Example_7.fld</emphasis> and save the opened <emphasis role="bold">.fld</emphasis> as
          <emphasis role="bold">Example_10.fld</emphasis>. Open the <emphasis role="bold">Flike
          Editor</emphasis> to configure the LH-moments fitting method. Note that the <emphasis
          role="bold">Example_7.fld</emphasis> file was configured with a Bayesian inference
        method.</para>
    </section>

    <section xml:id="b3_ch2_s_tb1vz">
      <title>Configure Inference Method</title>

      <para>In <emphasis role="bold">Example 7</emphasis>, a Bayesian inference method was used with
        censored low flows, so a number of changes are required to <emphasis role="bold"
          >Example_7.fld</emphasis> before the LH-moments inference method can be used. As low flows
        were censored in the previous example, these need to be included back into the analysis
        by:</para>

      <itemizedlist>
        <listitem>
          <para>Removing the censoring threshold; and</para>
        </listitem>

        <listitem>
          <para>Including all the flood data.</para>
        </listitem>
      </itemizedlist>

      <para>Ensure that the <emphasis role="bold">Bayesian with</emphasis> button is still checked.
        If the <emphasis role="bold">LH-moments fit to observed values with</emphasis> radio button
        is checked the <emphasis role="bold">Censoring of observed values</emphasis> tab cannot be
        accessed.</para>

      <para>To remove the censoring threshold, select the <emphasis
      role="bold">Censoring of observed values</emphasis> tab and select the
      <emphasis role="bold">Clear all</emphasis> button.</para>

      <para>To include all the flood data, select the <emphasis
      role="bold">Observed values</emphasis> tab and select the <emphasis
      role="bold">Include all</emphasis> button. Scroll through the data to
      ensure that all the crosses (<emphasis role="bold">x</emphasis>) in the
      <emphasis role="bold">Exclude</emphasis> column have been
      removed.</para>
    </section>

    <section xml:id="b3_ch2_s_ucckw">
      <title>Fit L-moments</title>

      <para>To configure TUFLOW Flike to fit distributions using the LH-moments inference method,
        return to the <emphasis role="bold">General</emphasis> tab and check the <emphasis
          role="bold">LH-moments fit to observed values with</emphasis> radio button. In the first
        instance, select the <emphasis role="bold">H=0</emphasis> radio button. This will fit a
        distribution using L-moments, this is, LH-moments with no shift.</para>

      <para>TUFLOW Flike will only fit LH-moments with H &gt;= 1 for the GEV distribution, however
        it will fit L-moments (H = 0) for all distributions. Ensure that the GEV probability model
        has been selected.</para>

      <para>The configured<emphasis role="bold"> Flike Editor</emphasis>
      should look like <xref linkend="b3_ch2_f_a2f69"/>. Select OK and run
      TUFLOW Flike. As usual, a probability plot will appear together with the
      report file. Rescale the plot so it looks like <xref
      linkend="b3_ch2_f_kinix"/>.</para>

      <figure xml:id="b3_ch2_f_a2f69">
        <title>Configured Flike Editor</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3069.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <figure xml:id="b3_ch2_f_kinix">
        <title>L-moment fit - Albert River at Broomfleet</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3035.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para><xref linkend="b3_ch2_f_kinix"/> displays the GEV L-moment fit on a Gumbel probability
        plot. Although the observed floods are largely contained within the 90% confidence limits,
        the fit, nonetheless, is poor with systematic departures from the data which exhibits
        reverse curvature.</para>
    </section>

    <section xml:id="b3_ch2_s_8iki7">
      <title>Fit LH-moments</title>

      <para>To deal with this poor fit, a LH-moment search was conducted to find the optimal shift
        parameter using the procedure described in <xref linkend="b3_ch2_s_xodr0"/>. To do this in
        TUFLOW Flike check the <emphasis role="bold">Optimized H</emphasis> radio button and run
        TUFLOW Flike. The results file reveals that the optimal shift was found to be 4. <xref
          linkend="b3_ch2_f_mkro8"/> presents the LH-moment fit with shift equal to 4. The fit
        effectively disregards floods more frequent than the 50% AEP (around
          350m<superscript>3</superscript>/s) and provides a very good fit to upper tail.</para>

      <figure xml:id="b3_ch2_f_mkro8">
        <title>LH-moment fit with shift H=4</title>

        <mediaobject>
          <imageobject>
            <imagedata width="100%" fileref="../../figures/3036.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The very significant reduction in the quantile confidence intervals is largely due to
        the shape parameter <emphasis role="italic">Κ</emphasis> changing from –0.17 to 0.50. The
        L-moment fit in Figure 2 was a compromise; most of the small and medium-sized floods
        suggested an upward curvature in the probability plot which resulted in a negative GEV shape
        parameter (to enable upward curvature). In contrast, the LH-moment fit favoured the
        large-sized floods which exhibit a downward curvature resulted in a positive shape
        parameter. For positive <emphasis role="italic">Κ</emphasis> the GEV has an upper bound. In
        this case the upper bound is about 2070 m<superscript>3</superscript>/s which is only 17%
        greater than the largest observed flood.</para>

      <para>A comparison of the quantile derived from the Bayesian inference method with censoring
        of PILFs and those determined using Optimised LH-moments in presented in <xref
          linkend="b3_ch2_t_zses5"/>. The two different inference methods produce similar results in
        terms of the calculated quantiles; however, the confidence limits are smaller using the
        Bayesian framework. This highlights how LH-moment results could be used to inform the
        selection of the censoring threshold for PILFs in the Bayesian framework.</para>
      <table xml:id="b3_ch2_t_zses5">
        <caption>Comparison of Quantiles using a Bayesian and LH-moments Inference Methods</caption>
        <col width="14%"/>
        <col width="14%"/>
        <col width="14%"/>
        <col width="14%"/>
        <col width="14%"/>
        <col width="14%"/>
        <col width="14%"/>
        <thead>
          <tr>
            <th>AEP (%)</th>
            <th>Bayesian with removal of PILFS</th>
            <th colspan="5">Optimised LH-moments</th>
          </tr>

          <tr>
            <td/>
            <td>Quantile Estimate q<subscript>Y</subscript></td>
            <td>Quantile Confidence 5% Limit</td>
            <td>Quantile Confidence 95% Limit</td>
            <td>Quantile Estimate q<subscript>Y</subscript></td>
            <td>Quantile Confidence 5% Limit</td>
            <td>Quantile Confidence 95% Limit</td>
          </tr>
          </thead>
          <tbody>
          <tr>
            <td>10%</td>
            <td>1400</td>
            <td>1249</td>
            <td>1590</td>
            <td>1406</td>
            <td>1133</td>
            <td>1634</td>
          </tr>
          <tr>
            <td>2%</td>
            <td>1720</td>
            <td>1605</td>
            <td>1931</td>
            <td>782</td>
            <td>1492</td>
            <td>2021</td>
          </tr>
          <tr>
            <td>1%</td>
            <td>1782</td>
            <td>1675</td>
            <td>2003</td>
            <td>1868</td>
            <td>1546</td>
            <td>2168</td>
          </tr>
          <tr>
            <td>0.2%</td>
            <td>1854</td>
            <td>1757</td>
            <td>2111</td>
            <td>1982</td>
            <td>1,99</td>
            <td>2482</td>
          </tr>
        </tbody>
      </table>
    </section>
  </section>

  <section xml:id="b3_ch2_s_01343">
    <title>Example 11: Fitting a probability model to POT data</title>

    <para>This example is a continuation of Example 9 which considers the Styx
    River at Jeogla. It illustrates fitting an exponential distribution to POT
    data. The table lists all the independent peak flows recorded over a 47
    year period that exceeded a threshold of 74
    m<superscript>3</superscript>/s – the total number of peaks was 47.
    Comparison with the annual maximum flood peaks in Example 9 reveals that
    in 15 of the 47 years of record the annual maximum peak were below the
    threshold of 74 m<superscript>3</superscript>/s.</para>

    <informaltable>
      <col width="12%"/>

      <col width="12%"/>

      <col width="12%"/>

      <col width="12%"/>

      <col width="12%"/>

      <col width="12%"/>

      <col width="12%"/>

      <col width="12%"/>

      <tbody>
        <tr>
          <td>878</td>

          <td>541</td>

          <td>521</td>

          <td>513</td>

          <td>436</td>

          <td>411</td>

          <td>405</td>

          <td>315</td>
        </tr>

        <tr>
          <td>309</td>

          <td>301</td>

          <td>300</td>

          <td>294</td>

          <td>283</td>

          <td>258</td>

          <td>255</td>

          <td>255</td>
        </tr>

        <tr>
          <td>238</td>

          <td>235</td>

          <td>221</td>

          <td>220</td>

          <td>206</td>

          <td>196</td>

          <td>194</td>

          <td>190</td>
        </tr>

        <tr>
          <td>186</td>

          <td>164</td>

          <td>150</td>

          <td>149</td>

          <td>134</td>

          <td>129</td>

          <td>129</td>

          <td>126</td>
        </tr>

        <tr>
          <td>119</td>

          <td>118</td>

          <td>117</td>

          <td>117</td>

          <td>111</td>

          <td>108</td>

          <td>105</td>

          <td>98</td>
        </tr>

        <tr>
          <td>92.2</td>

          <td>92.2</td>

          <td>91.7</td>

          <td>88.6</td>

          <td>85.2</td>

          <td>79.9</td>

          <td>74</td>

          <td/>
        </tr>
      </tbody>
    </informaltable>

    <para>The first two L-moments were estimated as 226.36 and 79.2. Noting that the exponential
      distribution is a special case of the generalised Pareto when <inlineequation>
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
          <mi>κ</mi>
        </math>
      </inlineequation> = 0, it follows from <xref linkend="b3_ch2_t_achre"/> that the exponential
      parameters are related to the L-moments by</para>

    <equation xml:id="b3_ch2_e_tixbx">
      <math xmlns="http://www.w3.org/1998/Math/MathML">
        <semantics>
          <mrow>
            <msub>
              <mi>&#x03BB;</mi>
              <mn>1</mn>
            </msub>
            <mo>=</mo>
            <msub>
              <mi>q</mi>
              <mo>*</mo>
            </msub>
            <mo>+</mo>
            <mi>&#x03B2;</mi>
            <mtext>&#x2009;</mtext>
            <mtext>&#x2009;</mtext>
            <mtext>&#x2009;</mtext>
            <mtext>&#x2009;</mtext>
            <mtext>&#x2009;</mtext>
            <mtext>&#x2009;</mtext>
            <msub>
              <mi>&#x03BB;</mi>
              <mn>2</mn>
            </msub>
            <mtext>&#x2009;</mtext>
            <mo>=</mo>
            <mtext>&#x2009;</mtext>
            <mfrac>
              <mi>&#x03B2;</mi>
              <mn>2</mn>
            </mfrac>
          </mrow>
        </semantics>
      </math>
    </equation>

    <para>which yields values for q* and β of 68.11 and 158.24 respectively. Therefore the
      probability of the peak flow q exceeding <emphasis role="italic">w</emphasis> in any POT event
      is</para>

    <equation xml:id="b3_ch2_e_kbsh6">
      <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <semantics>
          <mrow>
            <mi>P</mi>
            <mo stretchy="false">(</mo>
            <mi>q</mi>
            <mo>&#x003E;</mo>
            <mi>w</mi>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <msup>
              <mi mathvariant="normal">e</mi>
              <mrow>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mo>&#x2212;</mo>
                    <mfrac>
                      <mrow>
                        <mi>w</mi>
                        <mo>&#x2212;</mo>
                        <msub>
                          <mi>q</mi>
                          <mo>*</mo>
                        </msub>
                      </mrow>
                      <mi>&#x03B2;</mi>
                    </mfrac>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </msup>
            <mo>=</mo>
            <msup>
              <mi mathvariant="normal">e</mi>
              <mrow>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mo>&#x2212;</mo>
                    <mfrac>
                      <mrow>
                        <mi>w</mi>
                        <mo>&#x2212;</mo>
                        <mn>68.11</mn>
                      </mrow>
                      <mrow>
                        <mn>158.24</mn>
                      </mrow>
                    </mfrac>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </msup>
          </mrow>
        </semantics>
      </math>
    </equation>
    <para>The second step obtains the distribution of annual maximum peaks. Using<xref
        linkend="b3_ch2_e_yf64d"/>, the expected number of peaks that exceed w in a year is</para>

    <equation xml:id="b3_ch2_e_wzulq">
      <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <semantics>
          <mrow>
            <mi>E</mi>
            <mi>Y</mi>
            <mo stretchy="false">(</mo>
            <mi>w</mi>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mtext>&#x2009;</mtext>
            <mi>&#x03BD;</mi>
            <mi>P</mi>
            <mo stretchy="false">(</mo>
            <mi>q</mi>
            <mo>&#x003E;</mo>
            <mi>w</mi>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mi>&#x03BD;</mi>
            <msup>
              <mi mathvariant="normal">e</mi>
              <mrow>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mo>&#x2212;</mo>
                    <mfrac>
                      <mrow>
                        <mi>w</mi>
                        <mo>&#x2212;</mo>
                        <msub>
                          <mi>q</mi>
                          <mo>*</mo>
                        </msub>
                      </mrow>
                      <mi>&#x03B2;</mi>
                    </mfrac>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </msup>
          </mrow>
        </semantics>
      </math>
    </equation>

    <para>where v is the average number of flood peaks above the threshold q*
    per year.</para>
    <para>For plotting purposes it is convenient to use a log transformation which yields</para>
    <equation>
      <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <semantics>
          <mrow>
            <msub>
              <mrow>
                <mi>log</mi>
              </mrow>
              <mi>e</mi>
            </msub>
            <mi>E</mi>
            <mi>Y</mi>
            <mo stretchy="false">(</mo>
            <mi>w</mi>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <msub>
              <mrow>
                <mi>log</mi>
              </mrow>
              <mi>e</mi>
            </msub>
            <mi>&#x03BD;</mi>
            <mo>+</mo>
            <mfrac>
              <mrow>
                <msub>
                  <mi>q</mi>
                  <mo>*</mo>
                </msub>
              </mrow>
              <mi>&#x03B2;</mi>
            </mfrac>
            <mo>&#x2212;</mo>
            <mfrac>
              <mi>w</mi>
              <mi>&#x03B2;</mi>
            </mfrac>
          </mrow>
        </semantics>
      </math>
    </equation>
    <para>A plot of <inlineequation>
        <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
          <semantics>
            <mrow>
              <msub>
                <mrow>
                  <mi>log</mi>
                </mrow>
                <mi>e</mi>
              </msub>
              <mi>E</mi>
              <mi>Y</mi>
              <mo stretchy="false">(</mo>
              <mi>w</mi>
              <mo stretchy="false">)</mo>
            </mrow>
          </semantics>
        </math>
      </inlineequation> versus <emphasis role="italic">w</emphasis> should follow a straight line if
      the underlying POT distribution is exponential.</para>

    <para>Given that 47 peaks above the threshold occurred in 47 years, v
    equals 1.0. The following figure presents a plot of the fitted POT
    exponential model against the observed POT series.</para>

    <figure xml:id="b3_ch2_f_ayxhm">
      <title>Plot of the fitted POT
        exponential model against the observed POT series</title>
      <mediaobject>
        <imageobject>
          <imagedata width="100%" fileref="../../figures/3037.png"/>
        </imageobject>
      </mediaobject>
    </figure>
  </section>
</section>
